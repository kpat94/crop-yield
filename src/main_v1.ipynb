{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torch'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m~/Masters Research Project/Code/crop-yield/src/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mConcatDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"]}],"source":["\n","\n","import torch\n","print(torch.__version__)\n","import pandas as pd\n","print(pd.__version__)\n","from torch.utils.data import DataLoader,ConcatDataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n",""]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["1.8.1\n","1.2.4\n"]}],"source":["\n","\n","import torch\n","print(torch.__version__)\n","import pandas as pd\n","print(pd.__version__)\n","from torch.utils.data import DataLoader,ConcatDataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\n","# ## Read pesticides and fertilizer products data\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def load_data():\n","       pesticides_frame = pd.read_csv(\"Inputs_Pesticides_Use_E_All_Data_NOFLAG.csv\", engine='python')\n","       pesticides_frame.head\n","  \n","       # Read fertilizer data in the form of products\n","       fertilizers_frame = pd.read_csv(\"Inputs_FertilizersProduct_E_All_Data_NOFLAG.csv\", engine='python')\n","       fertilizers_frame.head\n","       \n","       # Read the crop yield data\n","       yield_frame = pd.read_csv(\"Production_Crops_E_All_Data_NOFLAG.csv\",engine='python')\n","       yield_frame.head\n","\n","       # %<br>\n","       # Get the fertilizer usage in terms of agriculture\n","\n","       # In[4]:     \n","       print(\"Fertilizer products\",fertilizers_frame.columns)\n","       print(\"\\n\")\n","       print(\"Input pesticides\", pesticides_frame.columns)\n","       \n","       # Filter out the data for Australia\n","       is_Country_Australia_Products = fertilizers_frame['Area']==\"Australia\"\n","       Australia_Fertilizers_Products = fertilizers_frame[is_Country_Australia_Products]\n","       # In[5]:\n","       is_Country_Australia_Pesticides = pesticides_frame['Area']==\"Australia\"\n","       Australia_Pesticides = pesticides_frame[is_Country_Australia_Pesticides]\n","\n","       # Get the fertilizer usage for agricultural use\n","       # In[6]:\n","       is_Agricultural = Australia_Fertilizers_Products['Element']==\"Agricultural Use\"\n","       Australia_Fertilizers_Products_Agricultural = Australia_Fertilizers_Products[is_Agricultural]\n","       print(\"Fertilizers \\n\")\n","       print(Australia_Fertilizers_Products_Agricultural.columns)\n","       print(Australia_Fertilizers_Products_Agricultural['Item'])\n","       print(\"\\n\")\n","       print(\"Pesticides \\n\")\n","       print(Australia_Pesticides.columns)\n","       print(Australia_Pesticides['Item'])\n","       \n","       # %<br>\n","       # 7 years for training - 2007 to 2013<br>\n","       # 3 years fro validation - 2014 to 2016<br>\n","       # 2017 for testing\n","       # In[7]:\n","       X_ten_years_fertlizers = Australia_Fertilizers_Products_Agricultural.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = Australia_Fertilizers_Products_Agricultural['Y2017']\n","       print(\"2017 \\n\",fertilizer_2017)\n","\n","       # In[8]:\n","       X_ten_years_pesticides = Australia_Pesticides.drop(['Area Code', 'Area', 'Item Code', 'Element Code', 'Element',\n","              'Unit', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_pesticides.columns)\n","       pesticides_2017 = Australia_Pesticides['Y2017']\n","       print(\"2017 \\n\",pesticides_2017)\n","\n","       # %<br>\n","       # Drop Ammonia anyhydrous and Ammonia nitrate from 2007 to 2013. It has not been used in these years.\n","       # In[9]:\n","       X_ten_years_fertlizers = X_ten_years_fertlizers.drop([602, 607], axis=0)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = fertilizer_2017.drop([602, 607], axis=0)\n","       print(fertilizer_2017)\n","       \n","       # %<br>\n","       # 2009 to 2013\n","       # ## Handle missing values from 2009 to 2013\n","       # In[10]:\n","       interpolate_2009_to_2016 = X_ten_years_fertlizers.interpolate()\n","       interpolate_2009_to_2016\n","       interpolate_2017 = fertilizer_2017.interpolate()\n","       print(\"interpolated 2017 fertilizers \\n\",interpolate_2017)\n","       # TODO: Don't just interpolate. Use the means of the previous 2  years\n","       # interpolate_pesticides_2017 = pesticides_2017.interpolate()\n","       # print(\"interpolated 2017 pesticides \\n\",interpolate_pesticides_2017)\n","\n","       # %<br>\n","       #  Interpolate missing fertlizers data<br>\n","       #  Replace na with mean values of 2004,2005,2009,2010\n","       # ##  Replace 2007 nan values with the mean of 2004 and 2005\n","\n","       # In[11]:\n","\n","       interpolate_2004_2005 = Australia_Fertilizers_Products_Agricultural[['Y2004','Y2005']].copy().interpolate()\n","       index_names = interpolate_2004_2005.index\n","       index_names\n","       interpolate_2004_2005=interpolate_2004_2005.drop([602,607], axis=0)\n","       interpolate_2004_2005['mean'] = interpolate_2004_2005.mean(axis=1)\n","       print(\"Mean of 2004 and 2005 ==== \\n\")\n","       print(interpolate_2004_2005)\n","       pesticides_2015_2016 = Australia_Pesticides[['Y2015','Y2016']]\n","       pesticides_2015_2016['mean'] = pesticides_2015_2016.mean(axis=1)\n","       print(\"Mean of 2015 and 2016 === \\n\")\n","       print(pesticides_2015_2016)\n","\n","       # Use this for 2007\n","\n","       # In[ ]:\n","       # ##  Replace 2008 nan values with the mean of 2009 and 2010\n","\n","       # In[12]:\n","       interpolate_2009_2010 = Australia_Fertilizers_Products_Agricultural[['Y2009','Y2010']].copy().interpolate()\n","       index_names = interpolate_2009_2010.index\n","       index_names\n","\n","       # In[13]:\n","       interpolate_2009_2010['mean'] = interpolate_2009_2010.mean(axis=1)\n","       print(\"Mean of 2009 and 2010 ==== \\n\")\n","       # Use this for 2008\n","       interpolate_2009_2010=interpolate_2009_2010.drop([602,607], axis=0)\n","       print(interpolate_2009_2010)\n","\n","\n","       # ## Populate 2007 nan values with computed mean\n","       # In[14]:\n","       # Fertilizers\n","       i=0\n","       for i in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:i,1:2] = interpolate_2004_2005['mean'].iloc[0:i]\n","\n","       #Pesticides\n","       j=0\n","       for j in range(1,7):\n","              pesticides_2017.iloc[0:j,] = pesticides_2015_2016['mean'].iloc[0:j]\n","\n","       # ## Populate 2008 nan values with computed mean\n","       # In[15]:\n","       j=0\n","       for j in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:j,2:3] = interpolate_2009_2010['mean'].iloc[0:j]\n","                \n","       interpolate_2009_to_2016.iloc[15, 1] = 1437658.50\n","       print(interpolate_2009_to_2016.iloc[15, 1])\n","       interpolate_2009_to_2016.iloc[15, 2] = 171351.00\n","\n","       # In[16]:\n","       X_ten_years_fertilizers_interpolated = interpolate_2009_to_2016\n","       X_ten_years_fertilizers_interpolated\n","       fertilizer_2017_interpolated = interpolate_2017\n","       print(fertilizer_2017_interpolated)\n","\n","       # #### TODO: Combine pesticdes and fertilizers data\n","       # In[17]:\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_fertilizers_interpolated)\n","\n","       pesticides_without_Item = X_ten_years_pesticides.copy().drop(['Item'], axis=1)\n","       pesticides_2017_wo_item = pesticides_2017\n","       fertilizers_without_Item = X_ten_years_fertilizers_interpolated.copy().drop(['Item'], axis=1)\n","       fertilizer_2017_wo_item = fertilizer_2017_interpolated\n","       # torch.tensor(X_ten_years_pesticides.values.astype(np.float64))\n","       print(X_ten_years_pesticides.values)\n","       print(X_ten_years_fertilizers_interpolated.values)\n","       print(pesticides_2017_wo_item.values)\n","       print(fertilizer_2017_wo_item.values)\n","\n","       print(pesticides_without_Item)\n","       pesticides_tensor = torch.from_numpy(pesticides_without_Item.values)\n","       print(\"\\t Pesticides tensor\", pesticides_tensor)\n","       pesticides_2017_tensor = torch.from_numpy(pesticides_2017_wo_item.values)\n","\n","       print(fertilizers_without_Item)\n","       fertilizers_tensor = torch.from_numpy(fertilizers_without_Item.values)\n","       fertilizer_2017_tensor = torch.from_numpy(fertilizer_2017_wo_item.values)\n","       print(\"\\t Fertilizers tensor\", fertilizers_tensor)\n","\n","       # In[18]:\n","       print(\"Train : \\n\",pesticides_tensor.shape)\n","       print(\"Train: \\n\",fertilizers_tensor.shape)\n","       print(\"Test: \\n\",pesticides_2017_tensor.shape)\n","       print(\"Test: \\n\",fertilizer_2017_tensor.shape)\n","\n","       # TODO: Normalize inputs before concatenation\n","       pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n","       print(pesticides_and_fertilizers.shape)\n","       print(\"\\n\")\n","       print(\"Training X data for 10 years: \",pesticides_and_fertilizers)\n","       print(\"\\n\")\n","       pesticides_and_fertilizers_2017 = torch.cat((pesticides_2017_tensor, fertilizer_2017_tensor), dim=0)\n","       print(pesticides_and_fertilizers_2017.shape)\n","       print(\"\\n\")\n","       print(\"Testing X data for 1 year: \",pesticides_and_fertilizers_2017)\n","       print(\"\\n\")\n","\n","       # Training + Validation data : Input\n","       X_train_ten = pesticides_and_fertilizers;\n","       X_test = pesticides_and_fertilizers_2017\n","\n","       # #### TODO: Create output vector\n","\n","       # In[19]:\n","       print(yield_frame.columns)\n","       #%%\n","       yield_frame.head\n","       #%%\n","       Australia = yield_frame['Area']==\"Australia\"\n","       Production = yield_frame['Element']==\"Production\"\n","       Australia_yield = yield_frame[Australia]\n","       Australia_yield_production = Australia_yield[Production]\n","       print(yield_frame[Australia])\n","       #%%\n","       # Get the yield data for the year 2017 only\n","       Australia_yield_2017 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015', \n","              'Y2016', 'Y2018'], axis=1)\n","       print(\"\\n\")\n","       print(\"Test Y data: \",Australia_yield_2017)\n","       print(\"\\n\")\n","       print(Australia_yield_2017.columns)\n","       print(Australia_yield_2017['Item'].unique())\n","       print(\"Number of unique crops in 2017: \",len(Australia_yield_2017['Item'].unique()))\n","\n","       Australia_yield_2007_to_2016 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017','Y2018'], axis=1)\n","       print(Australia_yield_2007_to_2016.columns)\n","       print(Australia_yield_2007_to_2016['Item'].unique())\n","       print(\"\\n\")\n","       print(\"Train Y data: \",Australia_yield_2007_to_2016)\n","       print(\"\\n\")\n","       print(\"Number of unique crops from 2007 to 2016\",len(Australia_yield_2007_to_2016['Item'].unique()))\n","\n","       # In[20]:\n","       yield_without_Item = Australia_yield_2007_to_2016.copy().drop(['Item'],axis=1)\n","       yield_without_Item_2017 = Australia_yield_2017.copy().drop(['Item'],axis=1)\n","\n","       print(yield_without_Item)\n","       yield_tensor = torch.from_numpy(yield_without_Item.values)\n","       print(\"Shape: \", yield_tensor.shape)\n","       print(\"\\t Yield tensor\", yield_tensor)\n","       print(yield_without_Item_2017)\n","       yield_tensor_2017 = torch.from_numpy(yield_without_Item_2017.values)\n","       print(\"Shape: \", yield_tensor_2017.shape)\n","       print(\"\\t Yield tensor\", yield_tensor_2017)\n","\n","       # Training + Validation: Output\n","       Y_train_ten = yield_tensor\n","       # Test Y\n","       Y_test = yield_tensor_2017\n","\n","       print(\"Train X: \\n\",X_train_ten.shape)\n","       print(\"Train Y: \\n\",Y_train_ten.shape)\n","       print(\"Test X: \\n\",X_test.shape)\n","       print(\"Test Y: \\n\",Y_test.shape)\n","       \n","       return [X_train_ten, Y_train_ten, X_test, Y_test]"]},{"cell_type":"code","execution_count":4,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["04, 5.0922e+04, 6.3416e+04],\n","        [7.2630e+03, 9.6109e+03, 8.5209e+03, 9.4395e+03, 1.0662e+04, 1.0899e+04,\n","         1.0113e+04, 1.1160e+04, 1.1399e+04, 1.4196e+04],\n","        [2.2404e+04, 2.9647e+04, 2.6284e+04, 2.9118e+04, 3.2890e+04, 3.3619e+04,\n","         3.1195e+04, 3.4426e+04, 3.5161e+04, 4.3789e+04],\n","        [2.3249e+03, 3.0765e+03, 2.7276e+03, 3.0216e+03, 3.4131e+03, 3.4887e+03,\n","         3.2371e+03, 3.5725e+03, 3.6487e+03, 4.5441e+03],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [3.0365e+05, 1.6086e+05,        nan, 1.6086e+05, 4.7329e+04, 1.5680e+05,\n","         1.3463e+05, 1.3892e+05, 1.9200e+05, 1.7258e+05],\n","        [2.0392e+04, 2.9583e+05, 7.4000e+01, 5.9159e+05, 2.6952e+05, 6.4043e+05,\n","         1.4296e+05, 3.7954e+05, 4.9362e+05, 4.8665e+05],\n","        [7.3190e+04, 6.1536e+05, 2.0840e+05, 1.0223e+06, 4.9172e+05, 1.1241e+06,\n","         1.5129e+05, 6.2017e+05, 7.9523e+05, 8.0072e+05],\n","        [1.2599e+05, 7.7861e+05, 4.1672e+05, 1.1405e+06, 7.1392e+05, 9.8775e+05,\n","         1.5962e+05, 8.6079e+05, 1.0969e+06, 1.1148e+06],\n","        [1.7879e+05, 6.4986e+05, 4.1037e+04, 1.2587e+06, 1.1699e+05, 8.5143e+05,\n","         1.6795e+05, 6.1150e+05, 7.0043e+05, 6.6312e+05],\n","        [2.3158e+05, 3.2902e+05, 2.1622e+04, 6.3643e+05, 7.9886e+04, 7.1511e+05,\n","         1.7628e+05, 3.6221e+05, 3.0400e+05, 2.1145e+05],\n","        [2.8438e+05, 8.1850e+03, 2.2080e+03, 1.4162e+04, 4.2783e+04, 5.7880e+05,\n","         1.8461e+05, 3.5574e+05, 3.5850e+05, 2.3647e+05],\n","        [3.3718e+05, 1.0243e+04, 1.1690e+03, 1.9317e+04, 2.1696e+04, 4.4248e+05,\n","         1.9294e+05, 3.4928e+05, 4.1300e+05, 2.6149e+05],\n","        [3.8998e+05, 1.2301e+04, 1.3000e+02, 2.4472e+04, 6.0900e+02, 3.0616e+05,\n","         2.0127e+05, 2.9040e+05, 3.2642e+05, 2.2314e+05],\n","        [4.4278e+05, 1.6292e+04, 2.9580e+03, 2.9627e+04, 2.2152e+05, 1.6984e+05,\n","         2.0960e+05, 2.3152e+05, 2.3984e+05, 1.8479e+05],\n","        [2.3410e+05, 2.0284e+04, 5.7860e+03, 3.4782e+04, 1.2869e+04, 3.3525e+04,\n","         3.1054e+05, 4.0976e+04, 5.9000e+04, 4.8019e+04],\n","        [2.5426e+04, 9.8828e+04, 2.1154e+04, 1.7650e+05, 5.0525e+04, 1.2845e+05,\n","         4.1147e+05, 3.3659e+05, 4.0550e+05, 3.9277e+05],\n","        [2.8115e+05, 5.2029e+05, 7.8618e+04, 9.6196e+05, 8.9395e+04, 8.9976e+05,\n","         5.1240e+05, 6.3220e+05, 7.5200e+05, 7.3752e+05],\n","        [1.1858e+06, 7.0965e+05, 4.0956e+05, 1.0097e+06, 7.8751e+05, 1.0286e+06,\n","         1.2400e+05, 1.6100e+05, 1.9800e+05, 1.4450e+05],\n","        [1.4377e+06, 8.9901e+05, 7.4050e+05, 1.0575e+06, 1.4856e+06, 1.1575e+06,\n","         1.0925e+06, 1.7532e+06, 1.7720e+06, 1.5482e+06],\n","        [1.4377e+06, 1.7135e+05, 1.1072e+05, 2.3198e+05, 1.4856e+06, 2.7388e+05,\n","         1.4371e+05, 1.2240e+05, 1.0940e+05, 1.4470e+05]], dtype=torch.float64)\n","\n","\n","torch.Size([22])\n","\n","\n","Testing X data for 1 year:  tensor([5.7169e+04, 1.2797e+04, 3.9475e+04, 4.0964e+03, 8.0076e+02, 8.0076e+02,\n","        1.4642e+05, 4.1860e+05, 6.9077e+05, 9.6295e+05, 6.2543e+05, 2.8791e+05,\n","        3.2327e+05, 3.5863e+05, 2.4948e+05, 1.4034e+05, 2.5094e+04, 3.6271e+05,\n","        7.0032e+05, 1.7607e+05, 1.6783e+06, 1.2868e+05], dtype=torch.float64)\n","\n","\n","Index(['Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element',\n","       'Unit', 'Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967',\n","       'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', 'Y1974', 'Y1975',\n","       'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983',\n","       'Y1984', 'Y1985', 'Y1986', 'Y1987', 'Y1988', 'Y1989', 'Y1990', 'Y1991',\n","       'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999',\n","       'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007',\n","       'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015',\n","       'Y2016', 'Y2017', 'Y2018'],\n","      dtype='object')\n","      Area Code       Area  Item Code                              Item  \\\n","1317         10  Australia        221               Almonds, with shell   \n","1318         10  Australia        221               Almonds, with shell   \n","1319         10  Australia        221               Almonds, with shell   \n","1320         10  Australia        711  Anise, badian, fennel, coriander   \n","1321         10  Australia        711  Anise, badian, fennel, coriander   \n","...         ...        ...        ...                               ...   \n","1606         10  Australia       1729                   Treenuts, Total   \n","1607         10  Australia       1729                   Treenuts, Total   \n","1608         10  Australia       1735                Vegetables Primary   \n","1609         10  Australia       1735                Vegetables Primary   \n","1610         10  Australia       1735                Vegetables Primary   \n","\n","      Element Code         Element    Unit     Y1961     Y1962     Y1963  ...  \\\n","1317          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1318          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","1319          5510      Production  tonnes       NaN       NaN       NaN  ...   \n","1320          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1321          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","...            ...             ...     ...       ...       ...       ...  ...   \n","1606          5419           Yield   hg/ha   17925.0   15042.0   15856.0  ...   \n","1607          5510      Production  tonnes     190.0     179.0     176.0  ...   \n","1608          5312  Area harvested      ha   63571.0   63397.0   65291.0  ...   \n","1609          5419           Yield   hg/ha  100846.0  103588.0  101085.0  ...   \n","1610          5510      Production  tonnes  641089.0  656717.0  659995.0  ...   \n","\n","          Y2009      Y2010      Y2011      Y2012      Y2013      Y2014  \\\n","1317    27981.0    29340.0    30390.0    28472.0    28586.0    28967.0   \n","1318     6775.0    30675.0    11377.0    10925.0    19863.0    19325.0   \n","1319    18957.0    90000.0    34576.0    31105.0    56779.0    55978.0   \n","1320      710.0      684.0      914.0     1015.0     1040.0     1000.0   \n","1321    13901.0    11853.0    11370.0    11488.0    11550.0    11570.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1606    12376.0    24996.0    12309.0    11661.0    17257.0    17692.0   \n","1607    60694.0   130580.0    70926.0    65324.0    92450.0    94930.0   \n","1608    66234.0    69695.0    74100.0    71471.0    66887.0    69262.0   \n","1609   270772.0   259637.0   224916.0   253910.0   277885.0   238561.0   \n","1610  1793425.0  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0   \n","\n","          Y2015      Y2016      Y2017      Y2018  \n","1317    31115.0    37000.0    38000.0    36940.0  \n","1318    20354.0    19703.0    19835.0    18917.0  \n","1319    63331.0    72902.0    75373.0    69880.0  \n","1320     1011.0     1108.0     1130.0     1074.0  \n","1321    11600.0    11673.0    11721.0    11769.0  \n","...         ...        ...        ...        ...  \n","1606    16017.0    17399.0    17430.0    16445.0  \n","1607   106374.0   118977.0   125683.0   118104.0  \n","1608    66917.0    68090.0    71918.0    70457.0  \n","1609   264193.0   257780.0   245961.0   252789.0  \n","1610  1767902.0  1755227.0  1768915.0  1781067.0  \n","\n","[294 rows x 65 columns]/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:96: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # ## Handle missing values from 2009 to 2013\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_single_block(indexer, value, name)\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value, self.name)\n","/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:203: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n","\n","\n","\n","Test Y data:                                    Item      Y2017\n","1319               Almonds, with shell    75373.0\n","1322  Anise, badian, fennel, coriander     1324.0\n","1325                            Apples   313730.0\n","1328                          Apricots     5351.0\n","1331                         Asparagus     7472.0\n","...                                ...        ...\n","1598          Oilcrops, Oil Equivalent  1888936.0\n","1601                     Pulses, Total  4129481.0\n","1604           Roots and Tubers, Total  1176669.0\n","1607                   Treenuts, Total   125683.0\n","1610                Vegetables Primary  1768915.0\n","\n","[101 rows x 2 columns]\n","\n","\n","Index(['Item', 'Y2017'], dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","Number of unique crops in 2017:  101\n","Index(['Item', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013',\n","       'Y2014', 'Y2015', 'Y2016'],\n","      dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","\n","\n","Train Y data:                                    Item      Y2007      Y2008      Y2009  \\\n","1319               Almonds, with shell    18024.0    65000.0    18957.0   \n","1322  Anise, badian, fennel, coriander       51.0     2091.0      987.0   \n","1325                            Apples   377980.0   265481.0   295134.0   \n","1328                          Apricots    17327.0    17000.0    13673.0   \n","1331                         Asparagus     5609.0     9779.0     6981.0   \n","...                                ...        ...        ...        ...   \n","1598          Oilcrops, Oil Equivalent   311986.0   553601.0   836928.0   \n","1601                     Pulses, Total  1168635.0  1681825.0  1817218.0   \n","1604           Roots and Tubers, Total  1261119.0  1447602.0  1220994.0   \n","1607                   Treenuts, Total    60924.0   103690.0    60694.0   \n","1610                Vegetables Primary  1746974.0  1703422.0  1793425.0   \n","\n","          Y2010      Y2011      Y2012      Y2013      Y2014      Y2015  \\\n","1319    90000.0    34576.0    31105.0    56779.0    55978.0    63331.0   \n","1322      810.0     1039.0     1166.0     1201.0     1157.0     1173.0   \n","1325   264401.0   299778.0   289064.0   288878.0   266771.0   295196.0   \n","1328    13175.0    13283.0    12186.0    11551.0     9238.0     9674.0   \n","1331     8835.0    10276.0     9589.0     8396.0     8375.0     8288.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   869470.0  1152688.0  1647064.0  1869619.0  1709984.0  1503124.0   \n","1601  2143709.0  2551800.0  2633541.0  2224400.0  2247300.0  1989200.0   \n","1604  1327922.0  1185085.0  1350721.0  1341112.0  1246546.0  1215195.0   \n","1607   130580.0    70926.0    65324.0    92450.0    94930.0   106374.0   \n","1610  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0  1767902.0   \n","\n","          Y2016  \n","1319    72902.0  \n","1322     1293.0  \n","1325   308298.0  \n","1328     8700.0  \n","1331     7737.0  \n","...         ...  \n","1598  1239283.0  \n","1601  2421811.0  \n","1604  1200748.0  \n","1607   118977.0  \n","1610  1755227.0  \n","\n","[101 rows x 11 columns]\n","\n","\n","Number of unique crops from 2007 to 2016 101\n","          Y2007      Y2008      Y2009      Y2010      Y2011      Y2012  \\\n","1319    18024.0    65000.0    18957.0    90000.0    34576.0    31105.0   \n","1322       51.0     2091.0      987.0      810.0     1039.0     1166.0   \n","1325   377980.0   265481.0   295134.0   264401.0   299778.0   289064.0   \n","1328    17327.0    17000.0    13673.0    13175.0    13283.0    12186.0   \n","1331     5609.0     9779.0     6981.0     8835.0    10276.0     9589.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   311986.0   553601.0   836928.0   869470.0  1152688.0  1647064.0   \n","1601  1168635.0  1681825.0  1817218.0  2143709.0  2551800.0  2633541.0   \n","1604  1261119.0  1447602.0  1220994.0  1327922.0  1185085.0  1350721.0   \n","1607    60924.0   103690.0    60694.0   130580.0    70926.0    65324.0   \n","1610  1746974.0  1703422.0  1793425.0  1809529.0  1666625.0  1814715.0   \n","\n","          Y2013      Y2014      Y2015      Y2016  \n","1319    56779.0    55978.0    63331.0    72902.0  \n","1322     1201.0     1157.0     1173.0     1293.0  \n","1325   288878.0   266771.0   295196.0   308298.0  \n","1328    11551.0     9238.0     9674.0     8700.0  \n","1331     8396.0     8375.0     8288.0     7737.0  \n","...         ...        ...        ...        ...  \n","1598  1869619.0  1709984.0  1503124.0  1239283.0  \n","1601  2224400.0  2247300.0  1989200.0  2421811.0  \n","1604  1341112.0  1246546.0  1215195.0  1200748.0  \n","1607    92450.0    94930.0   106374.0   118977.0  \n","1610  1858687.0  1652335.0  1767902.0  1755227.0  \n","\n","[101 rows x 10 columns]\n","Shape:  torch.Size([101, 10])\n","\t Yield tensor tensor([[1.8024e+04, 6.5000e+04, 1.8957e+04,  ..., 5.5978e+04, 6.3331e+04,\n","         7.2902e+04],\n","        [5.1000e+01, 2.0910e+03, 9.8700e+02,  ..., 1.1570e+03, 1.1730e+03,\n","         1.2930e+03],\n","        [3.7798e+05, 2.6548e+05, 2.9513e+05,  ..., 2.6677e+05, 2.9520e+05,\n","         3.0830e+05],\n","        ...,\n","        [1.2611e+06, 1.4476e+06, 1.2210e+06,  ..., 1.2465e+06, 1.2152e+06,\n","         1.2007e+06],\n","        [6.0924e+04, 1.0369e+05, 6.0694e+04,  ..., 9.4930e+04, 1.0637e+05,\n","         1.1898e+05],\n","        [1.7470e+06, 1.7034e+06, 1.7934e+06,  ..., 1.6523e+06, 1.7679e+06,\n","         1.7552e+06]], dtype=torch.float64)\n","          Y2017\n","1319    75373.0\n","1322     1324.0\n","1325   313730.0\n","1328     5351.0\n","1331     7472.0\n","...         ...\n","1598  1888936.0\n","1601  4129481.0\n","1604  1176669.0\n","1607   125683.0\n","1610  1768915.0\n","\n","[101 rows x 1 columns]\n","Shape:  torch.Size([101, 1])\n","\t Yield tensor tensor([[7.5373e+04],\n","        [1.3240e+03],\n","        [3.1373e+05],\n","        [5.3510e+03],\n","        [7.4720e+03],\n","        [5.6501e+04],\n","        [4.1297e+05],\n","        [1.3506e+07],\n","        [3.5396e+04],\n","        [4.1373e+04],\n","        [4.4250e+03],\n","        [4.7570e+03],\n","        [3.7604e+05],\n","        [9.9102e+04],\n","        [4.8680e+03],\n","        [2.8382e+05],\n","        [1.4066e+05],\n","        [1.1532e+04],\n","        [2.0040e+06],\n","        [3.8579e+04],\n","        [7.7449e+05],\n","        [1.2600e+06],\n","        [       nan],\n","        [1.7609e+04],\n","        [5.7900e+02],\n","        [       nan],\n","        [7.8000e+01],\n","        [       nan],\n","        [1.5270e+03],\n","        [       nan],\n","        [7.5400e+03],\n","        [6.9880e+03],\n","        [1.8244e+06],\n","        [1.6529e+04],\n","        [7.4900e+02],\n","        [2.3740e+03],\n","        [3.7227e+04],\n","        [2.1860e+05],\n","        [1.4626e+05],\n","        [6.0000e+03],\n","        [1.0314e+06],\n","        [4.3619e+05],\n","        [1.0266e+05],\n","        [4.3748e+04],\n","        [2.6353e+05],\n","        [3.5855e+04],\n","        [4.6326e+04],\n","        [       nan],\n","        [4.6165e+04],\n","        [2.2655e+06],\n","        [1.2257e+05],\n","        [2.6324e+05],\n","        [3.3232e+05],\n","        [6.3850e+03],\n","        [8.2659e+04],\n","        [9.6741e+04],\n","        [4.1519e+05],\n","        [2.0574e+04],\n","        [7.1500e+02],\n","        [8.5922e+04],\n","        [1.4950e+03],\n","        [1.7561e+04],\n","        [1.1052e+06],\n","        [3.6880e+04],\n","        [1.0150e+05],\n","        [       nan],\n","        [4.3132e+06],\n","        [6.9700e+02],\n","        [8.0730e+05],\n","        [3.0000e+04],\n","        [4.9570e+03],\n","        [2.1510e+06],\n","        [9.9400e+05],\n","        [3.1000e+04],\n","        [1.0719e+04],\n","        [4.5251e+04],\n","        [3.6561e+07],\n","        [1.7000e+04],\n","        [7.1475e+04],\n","        [1.2727e+05],\n","        [2.3540e+03],\n","        [3.7158e+05],\n","        [1.4995e+05],\n","        [7.7444e+04],\n","        [1.1955e+04],\n","        [2.6500e+03],\n","        [1.8088e+05],\n","        [3.1819e+07],\n","        [4.9780e+07],\n","        [5.0048e+07],\n","        [5.0533e+05],\n","        [1.7422e+07],\n","        [7.7449e+05],\n","        [3.9737e+06],\n","        [5.7713e+06],\n","        [3.2762e+06],\n","        [1.8889e+06],\n","        [4.1295e+06],\n","        [1.1767e+06],\n","        [1.2568e+05],\n","        [1.7689e+06]], dtype=torch.float64)\n","Train X: \n"," torch.Size([22, 10])\n","Train Y: \n"," torch.Size([101, 10])\n","Test X: \n"," torch.Size([22])\n","Test Y: \n"," torch.Size([101, 1])\n"]}],"source":["\n","training_input = load_data()[0]\n","training_ouput = load_data()[1]\n","test_input = load_data()[2]\n","test_output = load_data()[3]\n","\n","x = training_input\n","y = training_ouput"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["\n","# Transforms\n","\n","# Training data\n","x_tensor = torch.Tensor(x.float())\n","x_data = (x - x.mean())/(x.max() - x.min())\n","test = torch.Tensor([[0,4,5]])\n","x_tensor_normalized = x.normal_()\n","y_data = (y.normal_())\n","\n","transformed_x = torch.reshape(x_tensor_normalized,(1,220))\n","transformed_y = y_data\n","y = torch.reshape(y, (1,1010))\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Test data\n","transformed_test_x = torch.Tensor(test_input.float())\n","transformed_test_x = transformed_test_x.normal_()\n","transformed_test_y = torch.Tensor(test_output.float())\n","transformed_test_y = transformed_test_y.normal_()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Define model\n","class NeuralNet(nn.Module):\n","       def __init__(self, D_in, H1, H2, H3, H4, H5, H6, D_out):\n","              super(NeuralNet, self).__init__()\n","              self.linear1 = nn.Linear(D_in, H1)\n","              self.linear2 = nn.Linear(H1, H2)\n","              self.linear3 = nn.Linear(H2, H3)\n","              self.linear4 = nn.Linear(H3, H4)\n","              self.linear5 = nn.Linear(H4, H5)\n","              self.linear6 = nn.Linear(H5, H6)\n","              self.linear7 = nn.Linear(H6, D_out)\n","              self.relu = nn.ReLU()\n","       def forward(self, x):\n","              y_pred = self.linear1(x)\n","              y_pred = torch.tanh(self.linear2(y_pred))\n","              y_pred = self.linear3(y_pred)\n","              y_pred = torch.tanh(self.linear4(y_pred))\n","              y_pred = self.linear5(y_pred)\n","              y_pred = torch.sigmoid(self.linear6(y_pred))\n","              y_pred = self.linear7(y_pred)\n","              return y_pred\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["\n","\n","from functools import partial\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","from random import randint\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n",""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","\n","def get_accuracy(model, predicted, actual, threshold_percentage):\n","    num_items = len(actual)\n","    X = predicted.view(num_items)\n","    Y = actual.view(num_items)\n","    num_correct = torch.sum(torch.abs(X-Y)<torch.abs(threshold_percentage*Y))\n","    accuracy = (num_correct.item()*100.0/num_items)\n","    return (num_correct, accuracy)\n","    \n","\n","def random_split_training(trainset_x, trainset_y):\n","       # TODO:\n","       # Select 2 random numbers between 1 and 10\n","       # Use them to index into x and y.\n","       # That will give us the training and validation set\n","       # Rest of the indices are thus our training set\n","       index1 = randint(0,9)\n","       index2 = randint(0,9)\n","       indices=[]\n","       i=0\n","       for i in range(0,10):\n","              if(i not in [index1,index2]):\n","                 indices.append(i)    \n","       \n","       indices = torch.tensor(indices)\n","       val_indices = torch.tensor([index1, index2])\n","       t_subset_x = torch.index_select(trainset_x, 1, indices)\n","       t_subset_y = torch.index_select(trainset_y, 1, indices)\n","       v_subset_x = torch.index_select(trainset_x, 1,val_indices)\n","       v_subset_y = torch.index_select(trainset_y, 1, val_indices)\n","       return (t_subset_x, t_subset_y, v_subset_x, v_subset_y)\n","# \n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Function to train and validate data. Save the best model based on validation loss\n","# \n","# params:\n","# config: hyperparameter search space\n","# \n","def train_crop_yield(config):\n","       net = NeuralNet(176, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],8)\n","       criterion = torch.nn.MSELoss()\n","       optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","       # train_set = torch.cat((transformed_x, transformed_y), dim=0)\n","       # test_set = torch.cat((test_input, test_output), dim=0)\n","       \n","       x_by_years = transformed_x.reshape(22,10)\n","       y_by_years = transformed_y.reshape(101,10)\n","       \n","       train_subset_x, train_subset_y, val_subset_x, val_subset_y = random_split_training(x_by_years, y_by_years)\n","       \n","       for epoch in range(500):\n","              running_loss = 0.0\n","              epoch_steps = 0\n","              # Zero the accumulated gradients\n","              optimizer.zero_grad()\n","              # forward + backward + optimize\n","              output = net(train_subset_x.float().reshape(1,176))\n","              loss = criterion(output, train_subset_y.float())\n","              loss.backward()\n","              optimizer.step()\n","              # print statistics\n","              running_loss += loss.item()\n","              epoch_steps += 1\n","              if epoch%5==4:\n","                     print(\"[%d] loss: %.3f\"%(epoch+1, running_loss/epoch_steps))\n","                     running_loss=0.0\n","       \n","       # Validation loss\n","       net = NeuralNet(44, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],2)\n","       val_loss = 0.0\n","       val_steps = 0\n","       total = 0\n","       correct = 0\n","       with torch.no_grad():\n","              val_output = net(val_subset_x.float().reshape(1,44))\n","              total = val_subset_y.size(0)\n","              correct = torch.sum(torch.abs(val_output-val_subset_y)<torch.abs(0.10*val_subset_y))\n","              loss = criterion(val_output, val_subset_y.float())\n","              val_loss += loss.cpu().numpy()\n","              torch.save((net.state_dict(), optimizer.state_dict()), './trained_net.dat')\n","              tune.report(loss=(val_loss), accuracy=(correct/total))\n","       print(\"\\n Finished Training \\n\")\n","              "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["2021-05-16 17:39:46,270\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n","2021-05-16 17:39:50,331\tWARNING experiment.py:294 -- No name detected on trainable. Using DEFAULT.\n","2021-05-16 17:39:50,334\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n","2021-05-16 17:39:50,336\tWARNING function_runner.py:545 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-39-50<br>Number of trials: 10/10 (9 PENDING, 1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_16490_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0872086</td></tr>\n<tr><td>DEFAULT_16490_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0109812</td></tr>\n<tr><td>DEFAULT_16490_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0146765</td></tr>\n<tr><td>DEFAULT_16490_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0464081</td></tr>\n<tr><td>DEFAULT_16490_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0194283</td></tr>\n<tr><td>DEFAULT_16490_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0597779</td></tr>\n<tr><td>DEFAULT_16490_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0135337</td></tr>\n<tr><td>DEFAULT_16490_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.035487 </td></tr>\n<tr><td>DEFAULT_16490_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0402187</td></tr>\n<tr><td>DEFAULT_16490_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.010686 </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=79726)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [5] loss: 1.035\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [5] loss: 1.146\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [10] loss: 1.045\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-39-50<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_16490_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0872086</td></tr>\n<tr><td>DEFAULT_16490_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0109812</td></tr>\n<tr><td>DEFAULT_16490_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0146765</td></tr>\n<tr><td>DEFAULT_16490_00003</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0464081</td></tr>\n<tr><td>DEFAULT_16490_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0194283</td></tr>\n<tr><td>DEFAULT_16490_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0597779</td></tr>\n<tr><td>DEFAULT_16490_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0135337</td></tr>\n<tr><td>DEFAULT_16490_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.035487 </td></tr>\n<tr><td>DEFAULT_16490_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0402187</td></tr>\n<tr><td>DEFAULT_16490_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.010686 </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79724)\u001b[0m [5] loss: 1.092\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [10] loss: 1.108\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [15] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [5] loss: 1.068\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [15] loss: 1.076\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [20] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [10] loss: 1.094\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [20] loss: 1.056\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [25] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [30] loss: 1.035\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [25] loss: 1.044\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [10] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [15] loss: 1.067\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [35] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [30] loss: 1.037\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [40] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [35] loss: 1.034\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [20] loss: 1.042\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [15] loss: 1.045\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [45] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [40] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [50] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [45] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [25] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [20] loss: 1.053\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [50] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [30] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [25] loss: 1.044\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [35] loss: 1.039\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [40] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [30] loss: 1.034\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [45] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [35] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [50] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [40] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [45] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [50] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [180] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [185] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [190] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [180] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [185] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [205] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [190] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [205] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [280] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [280] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [180] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [185] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [370] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [375] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [190] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [370] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [375] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [205] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [455] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [460] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [180] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [185] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m [500] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [455] loss: 1.031\n","Result for DEFAULT_16490_00001:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-40-17\n","  done: false\n","  experiment_id: 3a9f5e42d3bd47b8aff5d0b9a839fd7e\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1385416984558105\n","  node_ip: 10.201.135.226\n","  pid: 79725\n","  time_since_restore: 21.42703604698181\n","  time_this_iter_s: 21.42703604698181\n","  time_total_s: 21.42703604698181\n","  timestamp: 1621152617\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00001'\n","  \n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [460] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79725)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.1385416984558105<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-39-50<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_16490_00000</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0872086</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00001</td><td>RUNNING </td><td>10.201.135.226:79725</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0109812</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          21.427</td><td style=\"text-align: right;\">1.13854</td></tr>\n<tr><td>DEFAULT_16490_00002</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0146765</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00003</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0464081</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00004</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0194283</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00005</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0597779</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00006</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0135337</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00007</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.035487 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00008</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0402187</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00009</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.010686 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_16490_00001:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-40-17\n","  done: true\n","  experiment_id: 3a9f5e42d3bd47b8aff5d0b9a839fd7e\n","  experiment_tag: 1_H1=128,H2=256,H3=2048,H4=128,H5=2048,H6=256,lr=0.010981\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1385416984558105\n","  node_ip: 10.201.135.226\n","  pid: 79725\n","  time_since_restore: 21.42703604698181\n","  time_this_iter_s: 21.42703604698181\n","  time_total_s: 21.42703604698181\n","  timestamp: 1621152617\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00001'\n","  \n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [190] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m [500] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79726)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [205] loss: 1.031\n","Result for DEFAULT_16490_00000:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-40-19\n","  done: true\n","  experiment_id: af4df29fe2c946f5abfc216aad176607\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1650141477584839\n","  node_ip: 10.201.135.226\n","  pid: 79726\n","  time_since_restore: 23.387895107269287\n","  time_this_iter_s: 23.387895107269287\n","  time_total_s: 23.387895107269287\n","  timestamp: 1621152619\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00000'\n","  \n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [280] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [370] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [375] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [280] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [5] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [10] loss: 1.112\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [15] loss: 1.091\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [5] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [10] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [20] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [15] loss: 1.047\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [25] loss: 1.046\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [20] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [25] loss: 1.036\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [30] loss: 1.043\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [30] loss: 1.034\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [35] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [35] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [40] loss: 1.034\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [40] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [45] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [45] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [50] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [50] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [55] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [180] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [185] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [190] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [205] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [180] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [185] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [190] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [205] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [455] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [280] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [460] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [280] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [370] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [375] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [370] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m [500] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79724)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [455] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [370] loss: 1.031\n","Result for DEFAULT_16490_00003:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-40-37\n","  done: true\n","  experiment_id: e8bbceff1242470891412c628aa7626d\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.2092418670654297\n","  node_ip: 10.201.135.226\n","  pid: 79724\n","  time_since_restore: 41.75178289413452\n","  time_this_iter_s: 41.75178289413452\n","  time_total_s: 41.75178289413452\n","  timestamp: 1621152637\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00003'\n","  \n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [460] loss: 1.031\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.1650141477584839<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-39-50<br>Number of trials: 10/10 (4 PENDING, 3 RUNNING, 3 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_16490_00002</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0146765</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00004</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0194283</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00005</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0597779</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00006</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0135337</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00007</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.035487 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00008</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0402187</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00009</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.010686 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0872086</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         23.3879</td><td style=\"text-align: right;\">1.16501</td></tr>\n<tr><td>DEFAULT_16490_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0109812</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.427 </td><td style=\"text-align: right;\">1.13854</td></tr>\n<tr><td>DEFAULT_16490_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0464081</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         41.7518</td><td style=\"text-align: right;\">1.20924</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79750)\u001b[0m [375] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [375] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m [500] loss: 1.031\n","Result for DEFAULT_16490_00004:\n","  accuracy: tensor(0.0693)\n","  date: 2021-05-16_17-40-38\n","  done: true\n","  experiment_id: 654d1f20ed924ea9a9998089f5be8abc\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.363644003868103\n","  node_ip: 10.201.135.226\n","  pid: 79755\n","  time_since_restore: 10.034078121185303\n","  time_this_iter_s: 10.034078121185303\n","  time_total_s: 10.034078121185303\n","  timestamp: 1621152638\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00004'\n","  \n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79755)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [455] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [460] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m [500] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","Result for DEFAULT_16490_00005:\n","  accuracy: tensor(0.0198)\n","  date: 2021-05-16_17-40-41\n","  done: false\n","  experiment_id: a1b5e5258f2f4e2dafc7354b2d24cecf\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.099210500717163\n","  node_ip: 10.201.135.226\n","  pid: 79750\n","  time_since_restore: 13.632983922958374\n","  time_this_iter_s: 13.632983922958374\n","  time_total_s: 13.632983922958374\n","  timestamp: 1621152641\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00005'\n","  \n","Result for DEFAULT_16490_00005:\n","  accuracy: tensor(0.0198)\n","  date: 2021-05-16_17-40-41\n","  done: true\n","  experiment_id: a1b5e5258f2f4e2dafc7354b2d24cecf\n","  experiment_tag: 5_H1=128,H2=256,H3=128,H4=512,H5=1024,H6=256,lr=0.059778\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.099210500717163\n","  node_ip: 10.201.135.226\n","  pid: 79750\n","  time_since_restore: 13.632983922958374\n","  time_this_iter_s: 13.632983922958374\n","  time_total_s: 13.632983922958374\n","  timestamp: 1621152641\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00005'\n","  \n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79750)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [455] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [460] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [5] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [5] loss: 1.078\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [10] loss: 1.060\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [15] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [10] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [5] loss: 1.069\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [20] loss: 1.041\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [25] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [30] loss: 1.034\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [15] loss: 1.056\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [20] loss: 1.037\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [10] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [15] loss: 1.035\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [35] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [25] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [20] loss: 1.040\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [30] loss: 1.036\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [40] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [45] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [50] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [35] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [25] loss: 1.034\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [40] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [30] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [45] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [35] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [50] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [40] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [45] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [50] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [180] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [185] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [190] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [205] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [180] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [185] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [190] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [280] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [205] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m [500] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79727)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [180] loss: 1.031\n","Result for DEFAULT_16490_00002:\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [185] loss: 1.031\n","  accuracy: tensor(0.0594)\n","  date: 2021-05-16_17-40-52\n","  done: true\n","  experiment_id: 673f6e3501e54ce5aba9a8b21c829732\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.5561881065368652\n","  node_ip: 10.201.135.226\n","  pid: 79727\n","  time_since_restore: 56.28799390792847\n","  time_this_iter_s: 56.28799390792847\n","  time_total_s: 56.28799390792847\n","  timestamp: 1621152652\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00002'\n","  \n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [190] loss: 1.031\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.1871280074119568<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-39-50<br>Number of trials: 10/10 (1 PENDING, 3 RUNNING, 6 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_16490_00006</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0135337</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00007</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.035487 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00008</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0402187</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00009</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.010686 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_16490_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0872086</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         23.3879</td><td style=\"text-align: right;\">1.16501</td></tr>\n<tr><td>DEFAULT_16490_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0109812</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.427 </td><td style=\"text-align: right;\">1.13854</td></tr>\n<tr><td>DEFAULT_16490_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0146765</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         56.288 </td><td style=\"text-align: right;\">1.55619</td></tr>\n<tr><td>DEFAULT_16490_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0464081</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         41.7518</td><td style=\"text-align: right;\">1.20924</td></tr>\n<tr><td>DEFAULT_16490_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0194283</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.0341</td><td style=\"text-align: right;\">1.36364</td></tr>\n<tr><td>DEFAULT_16490_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0597779</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.633 </td><td style=\"text-align: right;\">1.09921</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79771)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [205] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [370] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [375] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [280] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [455] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [460] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [370] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [280] loss: 1.031\n","Result for DEFAULT_16490_00006:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-40-53\n","  done: false\n","  experiment_id: eea38947cf4a4498a8319eca865e965f\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1076768636703491\n","  node_ip: 10.201.135.226\n","  pid: 79771\n","  time_since_restore: 4.4038779735565186\n","  time_this_iter_s: 4.4038779735565186\n","  time_total_s: 4.4038779735565186\n","  timestamp: 1621152653\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00006'\n","  \n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m [500] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [375] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","Result for DEFAULT_16490_00006:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-40-53\n","  done: true\n","  experiment_id: eea38947cf4a4498a8319eca865e965f\n","  experiment_tag: 6_H1=256,H2=128,H3=256,H4=128,H5=128,H6=128,lr=0.013534\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1076768636703491\n","  node_ip: 10.201.135.226\n","  pid: 79771\n","  time_since_restore: 4.4038779735565186\n","  time_this_iter_s: 4.4038779735565186\n","  time_total_s: 4.4038779735565186\n","  timestamp: 1621152653\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00006'\n","  \n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79771)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [455] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [460] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m [500] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [370] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [375] loss: 1.031\n","Result for DEFAULT_16490_00008:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_17-40-55\n","  done: false\n","  experiment_id: 1e4a14c01f334fb9873093b4e3bab129\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1199358701705933\n","  node_ip: 10.201.135.226\n","  pid: 79765\n","  time_since_restore: 5.902031898498535\n","  time_this_iter_s: 5.902031898498535\n","  time_total_s: 5.902031898498535\n","  timestamp: 1621152655\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00008'\n","  \n","Result for DEFAULT_16490_00008:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_17-40-55\n","  done: true\n","  experiment_id: 1e4a14c01f334fb9873093b4e3bab129\n","  experiment_tag: 8_H1=128,H2=256,H3=256,H4=512,H5=128,H6=256,lr=0.040219\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1199358701705933\n","  node_ip: 10.201.135.226\n","  pid: 79765\n","  time_since_restore: 5.902031898498535\n","  time_this_iter_s: 5.902031898498535\n","  time_total_s: 5.902031898498535\n","  timestamp: 1621152655\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00008'\n","  \n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79765)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [455] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [460] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m [500] loss: 1.031\n","Result for DEFAULT_16490_00007:\n","  accuracy: tensor(0.0099)\n","  date: 2021-05-16_17-40-57\n","  done: false\n","  experiment_id: 770d69ead9194acc927188db2ac2b546\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.073341727256775\n","  node_ip: 10.201.135.226\n","  pid: 79769\n","  time_since_restore: 7.746327877044678\n","  time_this_iter_s: 7.746327877044678\n","  time_total_s: 7.746327877044678\n","  timestamp: 1621152657\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00007'\n","  \n","Result for DEFAULT_16490_00007:\n","  accuracy: tensor(0.0099)\n","  date: 2021-05-16_17-40-57\n","  done: true\n","  experiment_id: 770d69ead9194acc927188db2ac2b546\n","  experiment_tag: 7_H1=256,H2=256,H3=256,H4=1024,H5=128,H6=256,lr=0.035487\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.073341727256775\n","  node_ip: 10.201.135.226\n","  pid: 79769\n","  time_since_restore: 7.746327877044678\n","  time_this_iter_s: 7.746327877044678\n","  time_total_s: 7.746327877044678\n","  timestamp: 1621152657\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00007'\n","  \n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79769)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [5] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [10] loss: 1.043\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [15] loss: 1.045\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [20] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [25] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [30] loss: 1.034\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [35] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [40] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [45] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [50] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [60] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [65] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [70] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [75] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [80] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [85] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [90] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [95] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [100] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [105] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [110] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [115] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [120] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [125] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [130] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [135] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [140] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [145] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [150] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [155] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [160] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [165] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [170] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [175] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [180] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [185] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [190] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [195] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [200] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [205] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [210] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [215] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [220] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [225] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [230] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [235] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [240] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [245] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [250] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [255] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [260] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [265] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [270] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [275] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [280] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [285] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [290] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [295] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [300] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [305] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [310] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [315] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [320] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [325] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [330] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [335] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [340] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [345] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [350] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [355] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [360] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [365] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [370] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [375] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [380] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [385] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [390] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [395] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [400] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [405] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [410] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [415] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [420] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [425] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [430] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [435] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [440] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [445] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [450] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [455] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [460] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [465] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [470] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [475] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [480] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [485] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [490] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [495] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m [500] loss: 1.031\n","Result for DEFAULT_16490_00009:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-41-02\n","  done: false\n","  experiment_id: e8bfa6dd690c438ab65b8380fb9424cb\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1058752536773682\n","  node_ip: 10.201.135.226\n","  pid: 79792\n","  time_since_restore: 3.4056596755981445\n","  time_this_iter_s: 3.4056596755981445\n","  time_total_s: 3.4056596755981445\n","  timestamp: 1621152662\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: '16490_00009'\n","  \n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79792)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.129238784313202<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-39-50<br>Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_16490_00009</td><td>RUNNING   </td><td>10.201.135.226:79792</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.010686 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.40566</td><td style=\"text-align: right;\">1.10588</td></tr>\n<tr><td>DEFAULT_16490_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0872086</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.3879 </td><td style=\"text-align: right;\">1.16501</td></tr>\n<tr><td>DEFAULT_16490_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0109812</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        21.427  </td><td style=\"text-align: right;\">1.13854</td></tr>\n<tr><td>DEFAULT_16490_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0146765</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        56.288  </td><td style=\"text-align: right;\">1.55619</td></tr>\n<tr><td>DEFAULT_16490_00003</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0464081</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        41.7518 </td><td style=\"text-align: right;\">1.20924</td></tr>\n<tr><td>DEFAULT_16490_00004</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0194283</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.0341 </td><td style=\"text-align: right;\">1.36364</td></tr>\n<tr><td>DEFAULT_16490_00005</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0597779</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.633  </td><td style=\"text-align: right;\">1.09921</td></tr>\n<tr><td>DEFAULT_16490_00006</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0135337</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.40388</td><td style=\"text-align: right;\">1.10768</td></tr>\n<tr><td>DEFAULT_16490_00007</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.035487 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.74633</td><td style=\"text-align: right;\">1.07334</td></tr>\n<tr><td>DEFAULT_16490_00008</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0402187</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.90203</td><td style=\"text-align: right;\">1.11994</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_16490_00009:\n  accuracy: tensor(0.0297)\n  date: 2021-05-16_17-41-02\n  done: true\n  experiment_id: e8bfa6dd690c438ab65b8380fb9424cb\n  experiment_tag: 9_H1=128,H2=256,H3=256,H4=256,H5=1024,H6=256,lr=0.010686\n  hostname: tprovpk.local\n  iterations_since_restore: 1\n  loss: 1.1058752536773682\n  node_ip: 10.201.135.226\n  pid: 79792\n  time_since_restore: 3.4056596755981445\n  time_this_iter_s: 3.4056596755981445\n  time_total_s: 3.4056596755981445\n  timestamp: 1621152662\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: '16490_00009'\n  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.129238784313202<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-39-50<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_16490_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0872086</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.3879 </td><td style=\"text-align: right;\">1.16501</td></tr>\n<tr><td>DEFAULT_16490_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0109812</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        21.427  </td><td style=\"text-align: right;\">1.13854</td></tr>\n<tr><td>DEFAULT_16490_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0146765</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        56.288  </td><td style=\"text-align: right;\">1.55619</td></tr>\n<tr><td>DEFAULT_16490_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0464081</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        41.7518 </td><td style=\"text-align: right;\">1.20924</td></tr>\n<tr><td>DEFAULT_16490_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0194283</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.0341 </td><td style=\"text-align: right;\">1.36364</td></tr>\n<tr><td>DEFAULT_16490_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0597779</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.633  </td><td style=\"text-align: right;\">1.09921</td></tr>\n<tr><td>DEFAULT_16490_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0135337</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.40388</td><td style=\"text-align: right;\">1.10768</td></tr>\n<tr><td>DEFAULT_16490_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.035487 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.74633</td><td style=\"text-align: right;\">1.07334</td></tr>\n<tr><td>DEFAULT_16490_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0402187</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.90203</td><td style=\"text-align: right;\">1.11994</td></tr>\n<tr><td>DEFAULT_16490_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.010686 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.40566</td><td style=\"text-align: right;\">1.10588</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2021-05-16 17:41:02,797\tINFO tune.py:549 -- Total run time: 72.47 seconds (71.25 seconds for the tuning loop).\n","Best trial config: {'H1': 256, 'H2': 256, 'H3': 256, 'H4': 1024, 'H5': 128, 'H6': 256, 'lr': 0.03548703659971478}\n","Best trial final validation loss: 1.073341727256775\n","Best trial fnal validation accuracy: 0.009900989942252636\n","Best trial test set accuracy: 12.871287128712872\n"]}],"source":["# Evaluation:\n","def test_accuracy(net, device='cpu'):\n","       net.eval()\n","       with torch.no_grad():\n","              predicted = net(transformed_test_x.float().reshape(1,22))\n","              return get_accuracy(net, predicted.reshape(101,1), transformed_test_y.reshape(101,1), 0.50)\n","       \n","# Define hyperparameters\n","config = {\n","       \"H1\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H2\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H3\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H4\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H5\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H6\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"lr\":tune.loguniform(1e-2, 1e-1)\n","}\n","\n","# Scheduler. Randomly try out combination of hyperparameters\n","scheduler = ASHAScheduler(\n","       metric=\"loss\",\n","       mode=\"min\",\n","       max_t=10,\n","       grace_period=1,\n","       reduction_factor=2\n",")\n","\n","reporter=CLIReporter(metric_columns=[\"loss\",\"accuracy\", \"training_iteration\"])\n","\n","result = tune.run(\n","       partial(train_crop_yield),\n","       config=config,\n","       num_samples=10,\n","       scheduler=scheduler\n",")\n","\n","best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","\n","print(\"Best trial config: {}\".format(best_trial.config))\n","print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n","print(\"Best trial fnal validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n","\n","best_trained_model = NeuralNet(22, best_trial.config[\"H1\"], best_trial.config[\"H2\"], best_trial.config[\"H3\"],best_trial.config[\"H4\"],best_trial.config[\"H5\"],best_trial.config[\"H6\"],101)\n","\n","device=\"cpu\"\n","best_trained_model.to(device)\n","\n","# best_checkpoint_dir = best_trial.checkpoint.value\n","# print(\" \\n\",type(best_trial.evaluated_params))\n","# model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n","# best_trained_model.load_state_dict(model_state)\n","\n","num_correct, test_acc = test_accuracy(best_trained_model, device)\n","print(\"Best trial test set accuracy: {}\".format(test_acc))\n",""]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Define model\n","class NeuralNet(nn.Module):\n","       def __init__(self, D_in, H1, H2, H3, H4, H5, H6, D_out):\n","              super(NeuralNet, self).__init__()\n","              self.linear1 = nn.Linear(D_in, H1)\n","              self.linear2 = nn.Linear(H1, H2)\n","              self.linear3 = nn.Linear(H2, H3)\n","              self.linear4 = nn.Linear(H3, H4)\n","              self.linear5 = nn.Linear(H4, H5)\n","              self.linear6 = nn.Linear(H5, H6)\n","              self.linear7 = nn.Linear(H6, D_out)\n","              self.relu = nn.ReLU()\n","       def forward(self, x):\n","              y_pred = self.linear1(x)\n","              y_pred = self.linear2(y_pred)\n","              y_pred = self.linear3(y_pred)\n","              y_pred = torch.tanh(self.linear4(y_pred))\n","              y_pred = self.linear5(y_pred)\n","              y_pred = torch.sigmoid(self.linear6(y_pred))\n","              y_pred = self.linear7(y_pred)\n","              return y_pred\n",""]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["\n","\n","from functools import partial\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","from random import randint\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n",""]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["\n","\n","def get_accuracy(model, predicted, actual, threshold_percentage):\n","    num_items = len(actual)\n","    X = predicted.view(num_items)\n","    Y = actual.view(num_items)\n","    num_correct = torch.sum(torch.abs(X-Y)<torch.abs(threshold_percentage*Y))\n","    accuracy = (num_correct.item()*100.0/num_items)\n","    return (num_correct, accuracy)\n","    \n","\n","def random_split_training(trainset_x, trainset_y):\n","       # TODO:\n","       # Select 2 random numbers between 1 and 10\n","       # Use them to index into x and y.\n","       # That will give us the training and validation set\n","       # Rest of the indices are thus our training set\n","       index1 = randint(0,9)\n","       index2 = randint(0,9)\n","       indices=[]\n","       i=0\n","       for i in range(0,10):\n","              if(i not in [index1,index2]):\n","                 indices.append(i)    \n","       \n","       indices = torch.tensor(indices)\n","       val_indices = torch.tensor([index1, index2])\n","       t_subset_x = torch.index_select(trainset_x, 1, indices)\n","       t_subset_y = torch.index_select(trainset_y, 1, indices)\n","       v_subset_x = torch.index_select(trainset_x, 1,val_indices)\n","       v_subset_y = torch.index_select(trainset_y, 1, val_indices)\n","       return (t_subset_x, t_subset_y, v_subset_x, v_subset_y)\n","# \n",""]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Function to train and validate data. Save the best model based on validation loss\n","# \n","# params:\n","# config: hyperparameter search space\n","# \n","def train_crop_yield(config):\n","       net = NeuralNet(176, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],8)\n","       criterion = torch.nn.MSELoss()\n","       optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","       # train_set = torch.cat((transformed_x, transformed_y), dim=0)\n","       # test_set = torch.cat((test_input, test_output), dim=0)\n","       \n","       x_by_years = transformed_x.reshape(22,10)\n","       y_by_years = transformed_y.reshape(101,10)\n","       \n","       train_subset_x, train_subset_y, val_subset_x, val_subset_y = random_split_training(x_by_years, y_by_years)\n","       \n","       for epoch in range(500):\n","              running_loss = 0.0\n","              epoch_steps = 0\n","              # Zero the accumulated gradients\n","              optimizer.zero_grad()\n","              # forward + backward + optimize\n","              output = net(train_subset_x.float().reshape(1,176))\n","              loss = criterion(output, train_subset_y.float())\n","              loss.backward()\n","              optimizer.step()\n","              # print statistics\n","              running_loss += loss.item()\n","              epoch_steps += 1\n","              if epoch%5==4:\n","                     print(\"[%d] loss: %.3f\"%(epoch+1, running_loss/epoch_steps))\n","                     running_loss=0.0\n","       \n","       # Validation loss\n","       net = NeuralNet(44, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],2)\n","       val_loss = 0.0\n","       val_steps = 0\n","       total = 0\n","       correct = 0\n","       with torch.no_grad():\n","              val_output = net(val_subset_x.float().reshape(1,44))\n","              total = val_subset_y.size(0)\n","              correct = torch.sum(torch.abs(val_output-val_subset_y)<torch.abs(0.10*val_subset_y))\n","              loss = criterion(val_output, val_subset_y.float())\n","              val_loss += loss.cpu().numpy()\n","              torch.save((net.state_dict(), optimizer.state_dict()), './trained_net.dat')\n","              tune.report(loss=(val_loss), accuracy=(correct/total))\n","       print(\"\\n Finished Training \\n\")\n","              "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["2021-05-16 17:41:44,760\tWARNING experiment.py:294 -- No name detected on trainable. Using DEFAULT.\n","2021-05-16 17:41:44,760\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (9 PENDING, 1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td></tr>\n<tr><td>DEFAULT_5a742_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td></tr>\n<tr><td>DEFAULT_5a742_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td></tr>\n<tr><td>DEFAULT_5a742_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td></tr>\n<tr><td>DEFAULT_5a742_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=79796)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [5] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [5] loss: 1.055\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [5] loss: 1.039\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [10] loss: 1.072\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [10] loss: 1.048\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [10] loss: 1.047\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [15] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [20] loss: 1.044\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [15] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [15] loss: 1.046\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [25] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [30] loss: 1.033\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [20] loss: 1.034\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [20] loss: 1.030\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [35] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [25] loss: 1.030\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [25] loss: 1.037\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td></tr>\n<tr><td>DEFAULT_5a742_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td></tr>\n<tr><td>DEFAULT_5a742_00003</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td></tr>\n<tr><td>DEFAULT_5a742_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td></tr>\n<tr><td>DEFAULT_5a742_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79796)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [40] loss: 1.030\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [30] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [30] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [35] loss: 1.030\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [35] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [40] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [40] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [5] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [10] loss: 1.035\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [15] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [170] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [20] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [190] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [195] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [205] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [25] loss: 1.030\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [210] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [30] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [35] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [40] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [170] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [170] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [190] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [195] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [190] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [315] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [195] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [205] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [210] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [205] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [210] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [495] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m [500] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [315] loss: 1.029\n","Result for DEFAULT_5a742_00000:\n","  accuracy: tensor(0.0792)\n","  date: 2021-05-16_17-41-58\n","  done: false\n","  experiment_id: 9bfde32c73cf405aa1c1c7c87a09414e\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.277754783630371\n","  node_ip: 10.201.135.226\n","  pid: 79803\n","  time_since_restore: 8.73112177848816\n","  time_this_iter_s: 8.73112177848816\n","  time_total_s: 8.73112177848816\n","  timestamp: 1621152718\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00000\n","  \n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.277754783630371<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00000</td><td>RUNNING </td><td>10.201.135.226:79803</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.73112</td><td style=\"text-align: right;\">1.27775</td></tr>\n<tr><td>DEFAULT_5a742_00001</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00003</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00004</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00008</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79795)\u001b[0m [315] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79803)\u001b[0m \n","Result for DEFAULT_5a742_00000:\n","  accuracy: tensor(0.0792)\n","  date: 2021-05-16_17-41-58\n","  done: true\n","  experiment_id: 9bfde32c73cf405aa1c1c7c87a09414e\n","  experiment_tag: 0_H1=256,H2=128,H3=256,H4=256,H5=512,H6=256,lr=0.051464\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.277754783630371\n","  node_ip: 10.201.135.226\n","  pid: 79803\n","  time_since_restore: 8.73112177848816\n","  time_this_iter_s: 8.73112177848816\n","  time_total_s: 8.73112177848816\n","  timestamp: 1621152718\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00000\n","  \n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [495] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m [500] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","Result for DEFAULT_5a742_00002:\n","  accuracy: tensor(0.0099)\n","  date: 2021-05-16_17-42-02\n","  done: false\n","  experiment_id: 692882e01f484df9a1d0e6e066993375\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1170848608016968\n","  node_ip: 10.201.135.226\n","  pid: 79796\n","  time_since_restore: 13.534386157989502\n","  time_this_iter_s: 13.534386157989502\n","  time_total_s: 13.534386157989502\n","  timestamp: 1621152722\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00002\n","  \n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [495] loss: 1.029\n","Result for DEFAULT_5a742_00002:\n","  accuracy: tensor(0.0099)\n","  date: 2021-05-16_17-42-02\n","  done: true\n","  experiment_id: 692882e01f484df9a1d0e6e066993375\n","  experiment_tag: 2_H1=128,H2=128,H3=128,H4=512,H5=1024,H6=256,lr=0.053367\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1170848608016968\n","  node_ip: 10.201.135.226\n","  pid: 79796\n","  time_since_restore: 13.534386157989502\n","  time_this_iter_s: 13.534386157989502\n","  time_total_s: 13.534386157989502\n","  timestamp: 1621152722\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00002\n","  \n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79796)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m [500] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [170] loss: 1.029\n","Result for DEFAULT_5a742_00001:\n","  accuracy: tensor(0.0198)\n","  date: 2021-05-16_17-42-03\n","  done: false\n","  experiment_id: f79732f6f7af49c385107c410df560bc\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.0819658041000366\n","  node_ip: 10.201.135.226\n","  pid: 79795\n","  time_since_restore: 13.771687984466553\n","  time_this_iter_s: 13.771687984466553\n","  time_total_s: 13.771687984466553\n","  timestamp: 1621152723\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00001\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.1170848608016968<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (4 PENDING, 4 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00001</td><td>RUNNING   </td><td>10.201.135.226:79795</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.7717 </td><td style=\"text-align: right;\">1.08197</td></tr>\n<tr><td>DEFAULT_5a742_00003</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00004</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00008</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.73112</td><td style=\"text-align: right;\">1.27775</td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.5344 </td><td style=\"text-align: right;\">1.11708</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79795)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79795)\u001b[0m \n","Result for DEFAULT_5a742_00001:\n","  accuracy: tensor(0.0198)\n","  date: 2021-05-16_17-42-03\n","  done: true\n","  experiment_id: f79732f6f7af49c385107c410df560bc\n","  experiment_tag: 1_H1=128,H2=128,H3=2048,H4=128,H5=1024,H6=256,lr=0.045074\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.0819658041000366\n","  node_ip: 10.201.135.226\n","  pid: 79795\n","  time_since_restore: 13.771687984466553\n","  time_this_iter_s: 13.771687984466553\n","  time_total_s: 13.771687984466553\n","  timestamp: 1621152723\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00001\n","  \n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [190] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [195] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [205] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [210] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [5] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [10] loss: 1.038\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [15] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [20] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [25] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [30] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [35] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [40] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [5] loss: 1.046\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [10] loss: 1.075\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [15] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [20] loss: 1.046\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [25] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [5] loss: 1.066\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [30] loss: 1.035\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [35] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [10] loss: 1.042\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [40] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [15] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [315] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [20] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [25] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [30] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [35] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [40] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [170] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [190] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [195] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [205] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [210] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [315] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [495] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m [500] loss: 1.029\n","Result for DEFAULT_5a742_00006:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_17-42-17\n","  done: true\n","  experiment_id: ead624aa7c674d3994fab34aa3da9612\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.2530475854873657\n","  node_ip: 10.201.135.226\n","  pid: 79837\n","  time_since_restore: 4.621551990509033\n","  time_this_iter_s: 4.621551990509033\n","  time_total_s: 4.621551990509033\n","  timestamp: 1621152737\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00006\n","  \n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79837)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.1850662231445312<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (3 PENDING, 3 RUNNING, 4 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00003</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00004</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00008</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.73112</td><td style=\"text-align: right;\">1.27775</td></tr>\n<tr><td>DEFAULT_5a742_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.7717 </td><td style=\"text-align: right;\">1.08197</td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.5344 </td><td style=\"text-align: right;\">1.11708</td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.62155</td><td style=\"text-align: right;\">1.25305</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79835)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [170] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [190] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [195] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [205] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [210] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [170] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [315] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [190] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [195] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [495] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [205] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m [500] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [210] loss: 1.029\n","Result for DEFAULT_5a742_00003:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_17-42-25\n","  done: false\n","  experiment_id: 1f0d6cf40a81435dbf69ccb9c75f7ab9\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.0927265882492065\n","  node_ip: 10.201.135.226\n","  pid: 79817\n","  time_since_restore: 34.60469198226929\n","  time_this_iter_s: 34.60469198226929\n","  time_total_s: 34.60469198226929\n","  timestamp: 1621152745\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00003\n","  \n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.1170848608016968<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (2 PENDING, 4 RUNNING, 4 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00003</td><td>RUNNING   </td><td>10.201.135.226:79817</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        34.6047 </td><td style=\"text-align: right;\">1.09273</td></tr>\n<tr><td>DEFAULT_5a742_00004</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00008</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.73112</td><td style=\"text-align: right;\">1.27775</td></tr>\n<tr><td>DEFAULT_5a742_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.7717 </td><td style=\"text-align: right;\">1.08197</td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.5344 </td><td style=\"text-align: right;\">1.11708</td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.62155</td><td style=\"text-align: right;\">1.25305</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_5a742_00003:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_17-42-25\n","  done: true\n","  experiment_id: 1f0d6cf40a81435dbf69ccb9c75f7ab9\n","  experiment_tag: 3_H1=128,H2=128,H3=512,H4=1024,H5=2048,H6=128,lr=0.018003\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.0927265882492065\n","  node_ip: 10.201.135.226\n","  pid: 79817\n","  time_since_restore: 34.60469198226929\n","  time_this_iter_s: 34.60469198226929\n","  time_total_s: 34.60469198226929\n","  timestamp: 1621152745\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00003\n","  \n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79817)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [5] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [10] loss: 1.043\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [15] loss: 1.046\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [20] loss: 1.030\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [495] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [25] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [30] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m [500] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [35] loss: 1.029\n","Result for DEFAULT_5a742_00005:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-42-28\n","  done: true\n","  experiment_id: 46b22155c5314ac4920b081c4843af83\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3093301057815552\n","  node_ip: 10.201.135.226\n","  pid: 79835\n","  time_since_restore: 16.001336812973022\n","  time_this_iter_s: 16.001336812973022\n","  time_total_s: 16.001336812973022\n","  timestamp: 1621152748\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00005\n","  \n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79835)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [40] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [170] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [190] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [195] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [205] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [210] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [315] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [315] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [5] loss: 1.100\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [10] loss: 1.042\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [15] loss: 1.075\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [20] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [25] loss: 1.039\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [30] loss: 1.035\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [35] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [40] loss: 1.032\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [5] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [10] loss: 1.066\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [15] loss: 1.043\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [20] loss: 1.031\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [25] loss: 1.038\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [30] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [35] loss: 1.030\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [40] loss: 1.030\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [170] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [495] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [45] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m [500] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [190] loss: 1.029\n","Result for DEFAULT_5a742_00007:\n","  accuracy: tensor(0.0198)\n","  date: 2021-05-16_17-42-40\n","  done: true\n","  experiment_id: 3e5b32b6302d4334978c9be9a1860317\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.2272064685821533\n","  node_ip: 10.201.135.226\n","  pid: 79850\n","  time_since_restore: 13.09539794921875\n","  time_this_iter_s: 13.09539794921875\n","  time_total_s: 13.09539794921875\n","  timestamp: 1621152760\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00007\n","  \n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [195] loss: 1.029\u001b[2m\u001b[36m(pid=79850)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79850)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.2/8.0 GiB<br>Using AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.2272064685821533<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00004</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00008</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.73112</td><td style=\"text-align: right;\">1.27775</td></tr>\n<tr><td>DEFAULT_5a742_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.7717 </td><td style=\"text-align: right;\">1.08197</td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.5344 </td><td style=\"text-align: right;\">1.11708</td></tr>\n<tr><td>DEFAULT_5a742_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        34.6047 </td><td style=\"text-align: right;\">1.09273</td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.0013 </td><td style=\"text-align: right;\">1.30933</td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.62155</td><td style=\"text-align: right;\">1.25305</td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.0954 </td><td style=\"text-align: right;\">1.22721</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [50] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [205] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [210] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [55] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [60] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [65] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [70] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [75] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [80] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [85] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [315] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [90] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [95] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [100] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [105] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [110] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [115] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [120] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [125] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [130] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [135] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [140] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [145] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [495] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m [500] loss: 1.029\n","Result for DEFAULT_5a742_00009:\n","  accuracy: tensor(0.0891)\n","  date: 2021-05-16_17-42-46\n","  done: true\n","  experiment_id: bd8e9252323b4dfcbee2dbab9ec34e51\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.2877801656723022\n","  node_ip: 10.201.135.226\n","  pid: 79859\n","  time_since_restore: 9.872045755386353\n","  time_this_iter_s: 9.872045755386353\n","  time_total_s: 9.872045755386353\n","  timestamp: 1621152766\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00009\n","  \n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [150] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79859)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.2401270270347595<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00004</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00008</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_5a742_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.73112</td><td style=\"text-align: right;\">1.27775</td></tr>\n<tr><td>DEFAULT_5a742_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.7717 </td><td style=\"text-align: right;\">1.08197</td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.5344 </td><td style=\"text-align: right;\">1.11708</td></tr>\n<tr><td>DEFAULT_5a742_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        34.6047 </td><td style=\"text-align: right;\">1.09273</td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.0013 </td><td style=\"text-align: right;\">1.30933</td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.62155</td><td style=\"text-align: right;\">1.25305</td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.0954 </td><td style=\"text-align: right;\">1.22721</td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.87205</td><td style=\"text-align: right;\">1.28778</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79832)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [155] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [160] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [165] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [170] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [175] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [180] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [185] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [190] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [495] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [195] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m [500] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [200] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [205] loss: 1.029\n","Result for DEFAULT_5a742_00004:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-42-48\n","  done: false\n","  experiment_id: 3cb3869bb0e949e598bc776c9a8019a6\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.184485912322998\n","  node_ip: 10.201.135.226\n","  pid: 79832\n","  time_since_restore: 41.42949390411377\n","  time_this_iter_s: 41.42949390411377\n","  time_total_s: 41.42949390411377\n","  timestamp: 1621152768\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00004\n","  \n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79832)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [210] loss: 1.029\n","Result for DEFAULT_5a742_00004:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-42-48\n","  done: true\n","  experiment_id: 3cb3869bb0e949e598bc776c9a8019a6\n","  experiment_tag: 4_H1=128,H2=256,H3=2048,H4=1024,H5=1024,H6=256,lr=0.019596\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.184485912322998\n","  node_ip: 10.201.135.226\n","  pid: 79832\n","  time_since_restore: 41.42949390411377\n","  time_this_iter_s: 41.42949390411377\n","  time_total_s: 41.42949390411377\n","  timestamp: 1621152768\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00004\n","  \n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [215] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [220] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [225] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [230] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [235] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [240] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [245] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [250] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [255] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [260] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [265] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [270] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [275] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [280] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [285] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [290] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [295] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [300] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [305] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [310] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [315] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [320] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [325] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [330] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [335] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [340] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [345] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [350] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [355] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [360] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [365] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [370] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [375] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [380] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [385] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [390] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [395] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [400] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [405] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [410] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [415] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [420] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [425] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [430] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [435] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [440] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [445] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [450] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [455] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [460] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [465] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [470] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [475] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [480] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [485] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [490] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [495] loss: 1.029\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m [500] loss: 1.029\n","Result for DEFAULT_5a742_00008:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-42-55\n","  done: false\n","  experiment_id: aab80c53f6bd4cfea142c0dec5ec8705\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.095700740814209\n","  node_ip: 10.201.135.226\n","  pid: 79863\n","  time_since_restore: 17.69373321533203\n","  time_this_iter_s: 17.69373321533203\n","  time_total_s: 17.69373321533203\n","  timestamp: 1621152775\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 5a742_00008\n","  \n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.2058461904525757<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00008</td><td>RUNNING   </td><td>10.201.135.226:79863</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.6937 </td><td style=\"text-align: right;\">1.0957 </td></tr>\n<tr><td>DEFAULT_5a742_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.73112</td><td style=\"text-align: right;\">1.27775</td></tr>\n<tr><td>DEFAULT_5a742_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.7717 </td><td style=\"text-align: right;\">1.08197</td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.5344 </td><td style=\"text-align: right;\">1.11708</td></tr>\n<tr><td>DEFAULT_5a742_00003</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        34.6047 </td><td style=\"text-align: right;\">1.09273</td></tr>\n<tr><td>DEFAULT_5a742_00004</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        41.4295 </td><td style=\"text-align: right;\">1.18449</td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.0013 </td><td style=\"text-align: right;\">1.30933</td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.62155</td><td style=\"text-align: right;\">1.25305</td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.0954 </td><td style=\"text-align: right;\">1.22721</td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.87205</td><td style=\"text-align: right;\">1.28778</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_5a742_00008:\n  accuracy: tensor(0.0297)\n  date: 2021-05-16_17-42-55\n  done: true\n  experiment_id: aab80c53f6bd4cfea142c0dec5ec8705\n  experiment_tag: 8_H1=256,H2=128,H3=1024,H4=2048,H5=256,H6=128,lr=0.017594\n  hostname: tprovpk.local\n  iterations_since_restore: 1\n  loss: 1.095700740814209\n  node_ip: 10.201.135.226\n  pid: 79863\n  time_since_restore: 17.69373321533203\n  time_this_iter_s: 17.69373321533203\n  time_total_s: 17.69373321533203\n  timestamp: 1621152775\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: 5a742_00008\n  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.2058461904525757<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-41-44<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_5a742_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0514636</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.73112</td><td style=\"text-align: right;\">1.27775</td></tr>\n<tr><td>DEFAULT_5a742_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0450736</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.7717 </td><td style=\"text-align: right;\">1.08197</td></tr>\n<tr><td>DEFAULT_5a742_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0533672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.5344 </td><td style=\"text-align: right;\">1.11708</td></tr>\n<tr><td>DEFAULT_5a742_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0180032</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        34.6047 </td><td style=\"text-align: right;\">1.09273</td></tr>\n<tr><td>DEFAULT_5a742_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0195965</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        41.4295 </td><td style=\"text-align: right;\">1.18449</td></tr>\n<tr><td>DEFAULT_5a742_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0189042</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.0013 </td><td style=\"text-align: right;\">1.30933</td></tr>\n<tr><td>DEFAULT_5a742_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0500462</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.62155</td><td style=\"text-align: right;\">1.25305</td></tr>\n<tr><td>DEFAULT_5a742_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0206985</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.0954 </td><td style=\"text-align: right;\">1.22721</td></tr>\n<tr><td>DEFAULT_5a742_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0175938</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.6937 </td><td style=\"text-align: right;\">1.0957 </td></tr>\n<tr><td>DEFAULT_5a742_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0418781</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.87205</td><td style=\"text-align: right;\">1.28778</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79863)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79863)\u001b[0m \n","2021-05-16 17:42:55,791\tINFO tune.py:549 -- Total run time: 71.03 seconds (70.85 seconds for the tuning loop).\n","Best trial config: {'H1': 128, 'H2': 128, 'H3': 2048, 'H4': 128, 'H5': 1024, 'H6': 256, 'lr': 0.045073629794893315}\n","Best trial final validation loss: 1.0819658041000366\n","Best trial fnal validation accuracy: 0.019801979884505272\n","Best trial test set accuracy: 8.910891089108912\n"]}],"source":["# Evaluation:\n","def test_accuracy(net, device='cpu'):\n","       net.eval()\n","       with torch.no_grad():\n","              predicted = net(transformed_test_x.float().reshape(1,22))\n","              return get_accuracy(net, predicted.reshape(101,1), transformed_test_y.reshape(101,1), 0.50)\n","       \n","# Define hyperparameters\n","config = {\n","       \"H1\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H2\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H3\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H4\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H5\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H6\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"lr\":tune.loguniform(1e-2, 1e-1)\n","}\n","\n","# Scheduler. Randomly try out combination of hyperparameters\n","scheduler = ASHAScheduler(\n","       metric=\"loss\",\n","       mode=\"min\",\n","       max_t=10,\n","       grace_period=1,\n","       reduction_factor=2\n",")\n","\n","reporter=CLIReporter(metric_columns=[\"loss\",\"accuracy\", \"training_iteration\"])\n","\n","result = tune.run(\n","       partial(train_crop_yield),\n","       config=config,\n","       num_samples=10,\n","       scheduler=scheduler\n",")\n","\n","best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","\n","print(\"Best trial config: {}\".format(best_trial.config))\n","print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n","print(\"Best trial fnal validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n","\n","best_trained_model = NeuralNet(22, best_trial.config[\"H1\"], best_trial.config[\"H2\"], best_trial.config[\"H3\"],best_trial.config[\"H4\"],best_trial.config[\"H5\"],best_trial.config[\"H6\"],101)\n","\n","device=\"cpu\"\n","best_trained_model.to(device)\n","\n","# best_checkpoint_dir = best_trial.checkpoint.value\n","# print(\" \\n\",type(best_trial.evaluated_params))\n","# model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n","# best_trained_model.load_state_dict(model_state)\n","\n","num_correct, test_acc = test_accuracy(best_trained_model, device)\n","print(\"Best trial test set accuracy: {}\".format(test_acc))\n",""]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Define model\n","class NeuralNet(nn.Module):\n","       def __init__(self, D_in, H1, H2, H3, H4, H5, H6, D_out):\n","              super(NeuralNet, self).__init__()\n","              self.linear1 = nn.Linear(D_in, H1)\n","              self.linear2 = nn.Linear(H1, H2)\n","              self.linear3 = nn.Linear(H2, H3)\n","              self.linear4 = nn.Linear(H3, H4)\n","              self.linear5 = nn.Linear(H4, H5)\n","              self.linear6 = nn.Linear(H5, H6)\n","              self.linear7 = nn.Linear(H6, D_out)\n","              self.relu = nn.ReLU()\n","       def forward(self, x):\n","              y_pred = torch.tanh(self.linear1(x))\n","              y_pred = torch.tanh(self.linear2(y_pred))\n","              y_pred = torch.tanh(self.linear3(y_pred))\n","              y_pred = torch.tanh(self.linear4(y_pred))\n","              y_pred = torch.tanh(self.linear5(y_pred))\n","              y_pred = torch.sigmoid(self.linear6(y_pred))\n","              y_pred = self.linear7(y_pred)\n","              return y_pred\n",""]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["\n","\n","from functools import partial\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","from random import randint\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n",""]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["\n","\n","def get_accuracy(model, predicted, actual, threshold_percentage):\n","    num_items = len(actual)\n","    X = predicted.view(num_items)\n","    Y = actual.view(num_items)\n","    num_correct = torch.sum(torch.abs(X-Y)<torch.abs(threshold_percentage*Y))\n","    accuracy = (num_correct.item()*100.0/num_items)\n","    return (num_correct, accuracy)\n","    \n","\n","def random_split_training(trainset_x, trainset_y):\n","       # TODO:\n","       # Select 2 random numbers between 1 and 10\n","       # Use them to index into x and y.\n","       # That will give us the training and validation set\n","       # Rest of the indices are thus our training set\n","       index1 = randint(0,9)\n","       index2 = randint(0,9)\n","       indices=[]\n","       i=0\n","       for i in range(0,10):\n","              if(i not in [index1,index2]):\n","                 indices.append(i)    \n","       \n","       indices = torch.tensor(indices)\n","       val_indices = torch.tensor([index1, index2])\n","       t_subset_x = torch.index_select(trainset_x, 1, indices)\n","       t_subset_y = torch.index_select(trainset_y, 1, indices)\n","       v_subset_x = torch.index_select(trainset_x, 1,val_indices)\n","       v_subset_y = torch.index_select(trainset_y, 1, val_indices)\n","       return (t_subset_x, t_subset_y, v_subset_x, v_subset_y)\n","# \n",""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Function to train and validate data. Save the best model based on validation loss\n","# \n","# params:\n","# config: hyperparameter search space\n","# \n","def train_crop_yield(config):\n","       net = NeuralNet(176, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],8)\n","       criterion = torch.nn.MSELoss()\n","       optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","       # train_set = torch.cat((transformed_x, transformed_y), dim=0)\n","       # test_set = torch.cat((test_input, test_output), dim=0)\n","       \n","       x_by_years = transformed_x.reshape(22,10)\n","       y_by_years = transformed_y.reshape(101,10)\n","       \n","       train_subset_x, train_subset_y, val_subset_x, val_subset_y = random_split_training(x_by_years, y_by_years)\n","       \n","       for epoch in range(500):\n","              running_loss = 0.0\n","              epoch_steps = 0\n","              # Zero the accumulated gradients\n","              optimizer.zero_grad()\n","              # forward + backward + optimize\n","              output = net(train_subset_x.float().reshape(1,176))\n","              loss = criterion(output, train_subset_y.float())\n","              loss.backward()\n","              optimizer.step()\n","              # print statistics\n","              running_loss += loss.item()\n","              epoch_steps += 1\n","              if epoch%5==4:\n","                     print(\"[%d] loss: %.3f\"%(epoch+1, running_loss/epoch_steps))\n","                     running_loss=0.0\n","       \n","       # Validation loss\n","       net = NeuralNet(44, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],2)\n","       val_loss = 0.0\n","       val_steps = 0\n","       total = 0\n","       correct = 0\n","       with torch.no_grad():\n","              val_output = net(val_subset_x.float().reshape(1,44))\n","              total = val_subset_y.size(0)\n","              correct = torch.sum(torch.abs(val_output-val_subset_y)<torch.abs(0.10*val_subset_y))\n","              loss = criterion(val_output, val_subset_y.float())\n","              val_loss += loss.cpu().numpy()\n","              torch.save((net.state_dict(), optimizer.state_dict()), './trained_net.dat')\n","              tune.report(loss=(val_loss), accuracy=(correct/total))\n","       print(\"\\n Finished Training \\n\")\n","              "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["2021-05-16 17:45:21,527\tWARNING experiment.py:294 -- No name detected on trainable. Using DEFAULT.\n","2021-05-16 17:45:21,529\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-45-21<br>Number of trials: 10/10 (10 PENDING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_dbaa4_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0256193</td></tr>\n<tr><td>DEFAULT_dbaa4_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0136277</td></tr>\n<tr><td>DEFAULT_dbaa4_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0126557</td></tr>\n<tr><td>DEFAULT_dbaa4_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0131832</td></tr>\n<tr><td>DEFAULT_dbaa4_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0125532</td></tr>\n<tr><td>DEFAULT_dbaa4_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0709722</td></tr>\n<tr><td>DEFAULT_dbaa4_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0725692</td></tr>\n<tr><td>DEFAULT_dbaa4_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0513366</td></tr>\n<tr><td>DEFAULT_dbaa4_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0139405</td></tr>\n<tr><td>DEFAULT_dbaa4_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0728766</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.2/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-45-21<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_dbaa4_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0256193</td></tr>\n<tr><td>DEFAULT_dbaa4_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0136277</td></tr>\n<tr><td>DEFAULT_dbaa4_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0126557</td></tr>\n<tr><td>DEFAULT_dbaa4_00003</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0131832</td></tr>\n<tr><td>DEFAULT_dbaa4_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0125532</td></tr>\n<tr><td>DEFAULT_dbaa4_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0709722</td></tr>\n<tr><td>DEFAULT_dbaa4_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0725692</td></tr>\n<tr><td>DEFAULT_dbaa4_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0513366</td></tr>\n<tr><td>DEFAULT_dbaa4_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0139405</td></tr>\n<tr><td>DEFAULT_dbaa4_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0728766</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=79887)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [5] loss: 1.061\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [5] loss: 1.061\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [10] loss: 1.056\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [10] loss: 1.053\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [15] loss: 1.064\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [5] loss: 1.123\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [5] loss: 1.055\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [15] loss: 1.070\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [20] loss: 1.061\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [20] loss: 1.060\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [25] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [10] loss: 1.094\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [25] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [30] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [10] loss: 1.136\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [30] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [35] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [35] loss: 1.053\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [40] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [45] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [15] loss: 1.074\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [40] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [15] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [45] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [55] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [20] loss: 1.062\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [25] loss: 1.055\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [20] loss: 1.078\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [55] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [30] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [25] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [35] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [30] loss: 1.058\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [40] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [35] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [45] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [40] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [45] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [140] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [55] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [55] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [140] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [230] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [240] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [260] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [230] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [240] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [260] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [140] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [140] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [415] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [435] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [440] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [450] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [415] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m [500] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [435] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [440] loss: 1.050\n","Result for DEFAULT_dbaa4_00003:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-45-36\n","  done: false\n","  experiment_id: 993e8871f67b47ac8d17470d417ac823\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 0.9956220388412476\n","  node_ip: 10.201.135.226\n","  pid: 79888\n","  time_since_restore: 5.922396183013916\n","  time_this_iter_s: 5.922396183013916\n","  time_total_s: 5.922396183013916\n","  timestamp: 1621152936\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00003\n","  \n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79888)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.2/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -0.9956220388412476<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-45-21<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_dbaa4_00000</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0256193</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00001</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0136277</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00002</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0126557</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00003</td><td>RUNNING </td><td>10.201.135.226:79888</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0131832</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          5.9224</td><td style=\"text-align: right;\">0.995622</td></tr>\n<tr><td>DEFAULT_dbaa4_00004</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0125532</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00005</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0709722</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00006</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0725692</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00007</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0513366</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00008</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0139405</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00009</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0728766</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_dbaa4_00003:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-45-36\n","  done: true\n","  experiment_id: 993e8871f67b47ac8d17470d417ac823\n","  experiment_tag: 3_H1=256,H2=256,H3=256,H4=256,H5=128,H6=256,lr=0.013183\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 0.9956220388412476\n","  node_ip: 10.201.135.226\n","  pid: 79888\n","  time_since_restore: 5.922396183013916\n","  time_this_iter_s: 5.922396183013916\n","  time_total_s: 5.922396183013916\n","  timestamp: 1621152936\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00003\n","  \u001b[2m\u001b[36m(pid=79887)\u001b[0m [450] loss: 1.050\n","\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [230] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m [500] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [240] loss: 1.050\n","Result for DEFAULT_dbaa4_00002:\n","  accuracy: tensor(0.0693)\n","  date: 2021-05-16_17-45-37\n","  done: true\n","  experiment_id: 417bcfb0588243afa74a9ea853627fdb\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.2871239185333252\n","  node_ip: 10.201.135.226\n","  pid: 79887\n","  time_since_restore: 6.72712516784668\n","  time_this_iter_s: 6.72712516784668\n","  time_total_s: 6.72712516784668\n","  timestamp: 1621152937\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00002\n","  \n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79887)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [260] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [230] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [240] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [260] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [415] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [435] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [440] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [450] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m [500] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [415] loss: 1.050\n","Result for DEFAULT_dbaa4_00000:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-45-45\n","  done: false\n","  experiment_id: 6be1f2e500c548a7a548458e7cfd8e0e\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.109063744544983\n","  node_ip: 10.201.135.226\n","  pid: 79889\n","  time_since_restore: 14.501002788543701\n","  time_this_iter_s: 14.501002788543701\n","  time_total_s: 14.501002788543701\n","  timestamp: 1621152945\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00000\n","  \n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.2/8.0 GiB<br>Using AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.109063744544983<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-45-21<br>Number of trials: 10/10 (4 PENDING, 4 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_dbaa4_00000</td><td>RUNNING   </td><td>10.201.135.226:79889</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0256193</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.501  </td><td style=\"text-align: right;\">1.10906 </td></tr>\n<tr><td>DEFAULT_dbaa4_00001</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0136277</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00004</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0125532</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00005</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0709722</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00006</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0725692</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00007</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0513366</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00008</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0139405</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00009</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0728766</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0126557</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.72713</td><td style=\"text-align: right;\">1.28712 </td></tr>\n<tr><td>DEFAULT_dbaa4_00003</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0131832</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.9224 </td><td style=\"text-align: right;\">0.995622</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_dbaa4_00000:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-45-45\n","  done: true\n","  experiment_id: 6be1f2e500c548a7a548458e7cfd8e0e\n","  experiment_tag: 0_H1=256,H2=128,H3=128,H4=256,H5=2048,H6=256,lr=0.025619\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.109063744544983\n","  node_ip: 10.201.135.226\n","  pid: 79889\n","  time_since_restore: 14.501002788543701\n","  time_this_iter_s: 14.501002788543701\n","  time_total_s: 14.501002788543701\n","  timestamp: 1621152945\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00000\n","  \n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79889)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [435] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [440] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [450] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [5] loss: 1.054\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [10] loss: 1.089\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [15] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [20] loss: 1.064\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [5] loss: 1.068\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m [500] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [25] loss: 1.050\n","Result for DEFAULT_dbaa4_00001:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-45-48\n","  done: false\n","  experiment_id: 24c4e29506774d6a8c12aa626d27d9d4\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1090521812438965\n","  node_ip: 10.201.135.226\n","  pid: 79890\n","  time_since_restore: 17.60870385169983\n","  time_this_iter_s: 17.60870385169983\n","  time_total_s: 17.60870385169983\n","  timestamp: 1621152948\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00001\n","  \n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [30] loss: 1.055\n","Result for DEFAULT_dbaa4_00001:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-45-48\n","  done: true\n","  experiment_id: 24c4e29506774d6a8c12aa626d27d9d4\n","  experiment_tag: 1_H1=256,H2=128,H3=1024,H4=1024,H5=256,H6=128,lr=0.013628\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.1090521812438965\n","  node_ip: 10.201.135.226\n","  pid: 79890\n","  time_since_restore: 17.60870385169983\n","  time_this_iter_s: 17.60870385169983\n","  time_total_s: 17.60870385169983\n","  timestamp: 1621152948\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00001\n","  \n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79890)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [35] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [10] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [40] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [45] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [50] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [55] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [15] loss: 1.054\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [20] loss: 1.059\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [25] loss: 1.059\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [5] loss: 1.135\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [30] loss: 1.055\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [10] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [15] loss: 1.066\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [20] loss: 1.072\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [35] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [25] loss: 1.055\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [30] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [40] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [35] loss: 1.053\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [140] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [40] loss: 1.053\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [45] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [45] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [55] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [55] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [5] loss: 1.056\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [10] loss: 1.074\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [230] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [15] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [240] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [20] loss: 1.059\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [25] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [260] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [140] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [30] loss: 1.053\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [35] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [40] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [45] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [55] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [230] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [240] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [260] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [415] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [435] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [440] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [450] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [140] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m [500] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [140] loss: 1.050\n","Result for DEFAULT_dbaa4_00004:\n","  accuracy: tensor(0.)\n","  date: 2021-05-16_17-46-06\n","  done: false\n","  experiment_id: 715d534711984fbf87152e4032672b37\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 0.9876677989959717\n","  node_ip: 10.201.135.226\n","  pid: 79925\n","  time_since_restore: 18.598530054092407\n","  time_this_iter_s: 18.598530054092407\n","  time_total_s: 18.598530054092407\n","  timestamp: 1621152966\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00004\n","  \n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.2/8.0 GiB<br>Using AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.1090521812438965<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-45-21<br>Number of trials: 10/10 (2 PENDING, 4 RUNNING, 4 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_dbaa4_00004</td><td>RUNNING   </td><td>10.201.135.226:79925</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0125532</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.5985 </td><td style=\"text-align: right;\">0.987668</td></tr>\n<tr><td>DEFAULT_dbaa4_00005</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0709722</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00006</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0725692</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00007</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0513366</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00008</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0139405</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00009</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0728766</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0256193</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.501  </td><td style=\"text-align: right;\">1.10906 </td></tr>\n<tr><td>DEFAULT_dbaa4_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0136277</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.6087 </td><td style=\"text-align: right;\">1.10905 </td></tr>\n<tr><td>DEFAULT_dbaa4_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0126557</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.72713</td><td style=\"text-align: right;\">1.28712 </td></tr>\n<tr><td>DEFAULT_dbaa4_00003</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0131832</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.9224 </td><td style=\"text-align: right;\">0.995622</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_dbaa4_00004:\n","  accuracy: tensor(0.)\n","  date: 2021-05-16_17-46-06\n","  done: true\n","  experiment_id: 715d534711984fbf87152e4032672b37\n","  experiment_tag: 4_H1=128,H2=256,H3=1024,H4=256,H5=2048,H6=128,lr=0.012553\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 0.9876677989959717\n","  node_ip: 10.201.135.226\n","  pid: 79925\n","  time_since_restore: 18.598530054092407\n","  time_this_iter_s: 18.598530054092407\n","  time_total_s: 18.598530054092407\n","  timestamp: 1621152966\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00004\n","  \u001b[2m\u001b[36m(pid=79925)\u001b[0m \n","\n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79925)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [415] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [435] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [440] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [450] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m [500] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [230] loss: 1.050\n","Result for DEFAULT_dbaa4_00006:\n","  accuracy: tensor(0.0693)\n","  date: 2021-05-16_17-46-14\n","  done: true\n","  experiment_id: cf1e2d437b2f4215bb18c12b87f63126\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3315296173095703\n","  node_ip: 10.201.135.226\n","  pid: 79932\n","  time_since_restore: 23.28683614730835\n","  time_this_iter_s: 23.28683614730835\n","  time_total_s: 23.28683614730835\n","  timestamp: 1621152974\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00006\n","  \n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79932)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.2/8.0 GiB<br>Using AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.1090579628944397<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-45-21<br>Number of trials: 10/10 (1 PENDING, 3 RUNNING, 6 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_dbaa4_00005</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0709722</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00007</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0513366</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00008</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0139405</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00009</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0728766</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0256193</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.501  </td><td style=\"text-align: right;\">1.10906 </td></tr>\n<tr><td>DEFAULT_dbaa4_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0136277</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.6087 </td><td style=\"text-align: right;\">1.10905 </td></tr>\n<tr><td>DEFAULT_dbaa4_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0126557</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.72713</td><td style=\"text-align: right;\">1.28712 </td></tr>\n<tr><td>DEFAULT_dbaa4_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0131832</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.9224 </td><td style=\"text-align: right;\">0.995622</td></tr>\n<tr><td>DEFAULT_dbaa4_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0125532</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.5985 </td><td style=\"text-align: right;\">0.987668</td></tr>\n<tr><td>DEFAULT_dbaa4_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0725692</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.2868 </td><td style=\"text-align: right;\">1.33153 </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79920)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [240] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [230] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [260] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [240] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [5] loss: 1.068\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [10] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [15] loss: 1.061\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [20] loss: 1.063\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [260] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [25] loss: 1.056\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [30] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [35] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [40] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [45] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [55] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [140] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [5] loss: 1.091\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [10] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [15] loss: 1.060\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [20] loss: 1.061\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [25] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [30] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [35] loss: 1.052\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [40] loss: 1.051\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [45] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [230] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [55] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [60] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [240] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [65] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [415] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [70] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [75] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [80] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [260] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [85] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [90] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [95] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [100] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [105] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [110] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [435] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [115] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [120] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [440] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [125] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [130] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [135] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [140] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [145] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [450] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [150] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [155] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [160] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [165] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [170] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [175] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [180] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [185] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [190] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [195] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [200] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [205] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [210] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [215] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [220] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [225] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [230] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [235] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [240] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [415] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [245] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [250] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m [500] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [255] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","Result for DEFAULT_dbaa4_00007:\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [260] loss: 1.050\n","  accuracy: tensor(0.)\n","  date: 2021-05-16_17-46-34\n","  done: false\n","  experiment_id: d244c656f93840b7917d901ad89f876a\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.0177665948867798\n","  node_ip: 10.201.135.226\n","  pid: 79935\n","  time_since_restore: 39.83930587768555\n","  time_this_iter_s: 39.83930587768555\n","  time_total_s: 39.83930587768555\n","  timestamp: 1621152994\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00007\n","  \n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79935)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [435] loss: 1.050\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.2/8.0 GiB<br>Using AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.1090521812438965<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-45-21<br>Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_dbaa4_00005</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0709722</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00007</td><td>RUNNING   </td><td>10.201.135.226:79935</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0513366</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        39.8393 </td><td style=\"text-align: right;\">1.01777 </td></tr>\n<tr><td>DEFAULT_dbaa4_00008</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0139405</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00009</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0728766</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0256193</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.501  </td><td style=\"text-align: right;\">1.10906 </td></tr>\n<tr><td>DEFAULT_dbaa4_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0136277</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.6087 </td><td style=\"text-align: right;\">1.10905 </td></tr>\n<tr><td>DEFAULT_dbaa4_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0126557</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.72713</td><td style=\"text-align: right;\">1.28712 </td></tr>\n<tr><td>DEFAULT_dbaa4_00003</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0131832</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.9224 </td><td style=\"text-align: right;\">0.995622</td></tr>\n<tr><td>DEFAULT_dbaa4_00004</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0125532</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.5985 </td><td style=\"text-align: right;\">0.987668</td></tr>\n<tr><td>DEFAULT_dbaa4_00006</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0725692</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.2868 </td><td style=\"text-align: right;\">1.33153 </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_dbaa4_00007:\n","  accuracy: tensor(0.)\n","  date: 2021-05-16_17-46-34\n","  done: true\n","  experiment_id: d244c656f93840b7917d901ad89f876a\n","  experiment_tag: 7_H1=256,H2=256,H3=2048,H4=1024,H5=512,H6=256,lr=0.051337\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.0177665948867798\n","  node_ip: 10.201.135.226\n","  pid: 79935\n","  time_since_restore: 39.83930587768555\n","  time_this_iter_s: 39.83930587768555\n","  time_total_s: 39.83930587768555\n","  timestamp: 1621152994\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00007\n","  \n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [265] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [440] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [270] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [275] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [450] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [280] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [285] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [290] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [295] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [300] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [305] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [310] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [315] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [320] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [325] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [415] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m [500] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [330] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","Result for DEFAULT_dbaa4_00008:\n","  accuracy: tensor(0.)\n","  date: 2021-05-16_17-46-36\n","  done: false\n","  experiment_id: 2b628507ac4a4b728a74f84f90778859\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 0.9842002987861633\n","  node_ip: 10.201.135.226\n","  pid: 79945\n","  time_since_restore: 18.70971918106079\n","  time_this_iter_s: 18.70971918106079\n","  time_total_s: 18.70971918106079\n","  timestamp: 1621152996\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00008\n","  \n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [335] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m \n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=79945)\u001b[0m \n","Result for DEFAULT_dbaa4_00008:\n","  accuracy: tensor(0.)\n","  date: 2021-05-16_17-46-36\n","  done: true\n","  experiment_id: 2b628507ac4a4b728a74f84f90778859\n","  experiment_tag: 8_H1=256,H2=128,H3=512,H4=2048,H5=128,H6=256,lr=0.013941\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 0.9842002987861633\n","  node_ip: 10.201.135.226\n","  pid: 79945\n","  time_since_restore: 18.70971918106079\n","  time_this_iter_s: 18.70971918106079\n","  time_total_s: 18.70971918106079\n","  timestamp: 1621152996\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00008\n","  \n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [340] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [345] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [350] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [355] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [360] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [365] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [370] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [375] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [380] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [385] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [435] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [390] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [395] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [400] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [440] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [405] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [410] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [415] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [420] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [425] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [430] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [435] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [450] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [440] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [445] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [450] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [455] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [460] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [465] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [470] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m [500] loss: 1.050\n","Result for DEFAULT_dbaa4_00009:\n","  accuracy: tensor(0.0099)\n","  date: 2021-05-16_17-46-40\n","  done: true\n","  experiment_id: 344e5464895441eeb3faf3b9e27cdbe1\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.0663061141967773\n","  node_ip: 10.201.135.226\n","  pid: 79949\n","  time_since_restore: 15.584203720092773\n","  time_this_iter_s: 15.584203720092773\n","  time_total_s: 15.584203720092773\n","  timestamp: 1621153000\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00009\n","  \n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79949)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.0663061141967773<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-45-21<br>Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_dbaa4_00005</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0709722</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n<tr><td>DEFAULT_dbaa4_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0256193</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.501  </td><td style=\"text-align: right;\">1.10906 </td></tr>\n<tr><td>DEFAULT_dbaa4_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0136277</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.6087 </td><td style=\"text-align: right;\">1.10905 </td></tr>\n<tr><td>DEFAULT_dbaa4_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0126557</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.72713</td><td style=\"text-align: right;\">1.28712 </td></tr>\n<tr><td>DEFAULT_dbaa4_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0131832</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.9224 </td><td style=\"text-align: right;\">0.995622</td></tr>\n<tr><td>DEFAULT_dbaa4_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0125532</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.5985 </td><td style=\"text-align: right;\">0.987668</td></tr>\n<tr><td>DEFAULT_dbaa4_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0725692</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.2868 </td><td style=\"text-align: right;\">1.33153 </td></tr>\n<tr><td>DEFAULT_dbaa4_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0513366</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        39.8393 </td><td style=\"text-align: right;\">1.01777 </td></tr>\n<tr><td>DEFAULT_dbaa4_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0139405</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.7097 </td><td style=\"text-align: right;\">0.9842  </td></tr>\n<tr><td>DEFAULT_dbaa4_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0728766</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.5842 </td><td style=\"text-align: right;\">1.06631 </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=79920)\u001b[0m [475] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [480] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [485] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [490] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [495] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m [500] loss: 1.050\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=79920)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","Result for DEFAULT_dbaa4_00005:\n","  accuracy: tensor(0.0198)\n","  date: 2021-05-16_17-46-41\n","  done: true\n","  experiment_id: acc87d3e1ce34223ae93d14d3245f74c\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.112239956855774\n","  node_ip: 10.201.135.226\n","  pid: 79920\n","  time_since_restore: 54.510109186172485\n","  time_this_iter_s: 54.510109186172485\n","  time_total_s: 54.510109186172485\n","  timestamp: 1621153001\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: dbaa4_00005\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.087679147720337<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-45-21<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_dbaa4_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0256193</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.501  </td><td style=\"text-align: right;\">1.10906 </td></tr>\n<tr><td>DEFAULT_dbaa4_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0136277</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.6087 </td><td style=\"text-align: right;\">1.10905 </td></tr>\n<tr><td>DEFAULT_dbaa4_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0126557</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.72713</td><td style=\"text-align: right;\">1.28712 </td></tr>\n<tr><td>DEFAULT_dbaa4_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0131832</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.9224 </td><td style=\"text-align: right;\">0.995622</td></tr>\n<tr><td>DEFAULT_dbaa4_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0125532</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.5985 </td><td style=\"text-align: right;\">0.987668</td></tr>\n<tr><td>DEFAULT_dbaa4_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0709722</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        54.5101 </td><td style=\"text-align: right;\">1.11224 </td></tr>\n<tr><td>DEFAULT_dbaa4_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0725692</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.2868 </td><td style=\"text-align: right;\">1.33153 </td></tr>\n<tr><td>DEFAULT_dbaa4_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0513366</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        39.8393 </td><td style=\"text-align: right;\">1.01777 </td></tr>\n<tr><td>DEFAULT_dbaa4_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0139405</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.7097 </td><td style=\"text-align: right;\">0.9842  </td></tr>\n<tr><td>DEFAULT_dbaa4_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0728766</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.5842 </td><td style=\"text-align: right;\">1.06631 </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2021-05-16 17:46:42,165\tINFO tune.py:549 -- Total run time: 80.64 seconds (80.42 seconds for the tuning loop).\n","Best trial config: {'H1': 256, 'H2': 128, 'H3': 512, 'H4': 2048, 'H5': 128, 'H6': 256, 'lr': 0.013940532761118583}\n","Best trial final validation loss: 0.9842002987861633\n","Best trial fnal validation accuracy: 0.0\n","Best trial test set accuracy: 6.930693069306931\n"]}],"source":["# Evaluation:\n","def test_accuracy(net, device='cpu'):\n","       net.eval()\n","       with torch.no_grad():\n","              predicted = net(transformed_test_x.float().reshape(1,22))\n","              return get_accuracy(net, predicted.reshape(101,1), transformed_test_y.reshape(101,1), 0.50)\n","       \n","# Define hyperparameters\n","config = {\n","       \"H1\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H2\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H3\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H4\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H5\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H6\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"lr\":tune.loguniform(1e-2, 1e-1)\n","}\n","\n","# Scheduler. Randomly try out combination of hyperparameters\n","scheduler = ASHAScheduler(\n","       metric=\"loss\",\n","       mode=\"min\",\n","       max_t=10,\n","       grace_period=1,\n","       reduction_factor=2\n",")\n","\n","reporter=CLIReporter(metric_columns=[\"loss\",\"accuracy\", \"training_iteration\"])\n","\n","result = tune.run(\n","       partial(train_crop_yield),\n","       config=config,\n","       num_samples=10,\n","       scheduler=scheduler\n",")\n","\n","best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","\n","print(\"Best trial config: {}\".format(best_trial.config))\n","print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n","print(\"Best trial fnal validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n","\n","best_trained_model = NeuralNet(22, best_trial.config[\"H1\"], best_trial.config[\"H2\"], best_trial.config[\"H3\"],best_trial.config[\"H4\"],best_trial.config[\"H5\"],best_trial.config[\"H6\"],101)\n","\n","device=\"cpu\"\n","best_trained_model.to(device)\n","\n","# best_checkpoint_dir = best_trial.checkpoint.value\n","# print(\" \\n\",type(best_trial.evaluated_params))\n","# model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n","# best_trained_model.load_state_dict(model_state)\n","\n","num_correct, test_acc = test_accuracy(best_trained_model, device)\n","print(\"Best trial test set accuracy: {}\".format(test_acc))\n",""]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["1.8.1\n1.2.4\n"]}],"source":["\n","\n","import torch\n","print(torch.__version__)\n","import pandas as pd\n","print(pd.__version__)\n","from torch.utils.data import DataLoader,ConcatDataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n",""]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["\n","# ## Read pesticides and fertilizer products data\n",""]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def load_data():\n","       pesticides_frame = pd.read_csv(\"Inputs_Pesticides_Use_E_All_Data_NOFLAG.csv\", engine='python')\n","       pesticides_frame.head\n","  \n","       # Read fertilizer data in the form of products\n","       fertilizers_frame = pd.read_csv(\"Inputs_FertilizersProduct_E_All_Data_NOFLAG.csv\", engine='python')\n","       fertilizers_frame.head\n","       \n","       # Read the crop yield data\n","       yield_frame = pd.read_csv(\"Production_Crops_E_All_Data_NOFLAG.csv\",engine='python')\n","       yield_frame.head\n","\n","       # %<br>\n","       # Get the fertilizer usage in terms of agriculture\n","\n","       # In[4]:     \n","       print(\"Fertilizer products\",fertilizers_frame.columns)\n","       print(\"\\n\")\n","       print(\"Input pesticides\", pesticides_frame.columns)\n","       \n","       # Filter out the data for Australia\n","       is_Country_Australia_Products = fertilizers_frame['Area']==\"Australia\"\n","       Australia_Fertilizers_Products = fertilizers_frame[is_Country_Australia_Products]\n","       # In[5]:\n","       is_Country_Australia_Pesticides = pesticides_frame['Area']==\"Australia\"\n","       Australia_Pesticides = pesticides_frame[is_Country_Australia_Pesticides]\n","\n","       # Get the fertilizer usage for agricultural use\n","       # In[6]:\n","       is_Agricultural = Australia_Fertilizers_Products['Element']==\"Agricultural Use\"\n","       Australia_Fertilizers_Products_Agricultural = Australia_Fertilizers_Products[is_Agricultural]\n","       print(\"Fertilizers \\n\")\n","       print(Australia_Fertilizers_Products_Agricultural.columns)\n","       print(Australia_Fertilizers_Products_Agricultural['Item'])\n","       print(\"\\n\")\n","       print(\"Pesticides \\n\")\n","       print(Australia_Pesticides.columns)\n","       print(Australia_Pesticides['Item'])\n","       \n","       # %<br>\n","       # 7 years for training - 2007 to 2013<br>\n","       # 3 years fro validation - 2014 to 2016<br>\n","       # 2017 for testing\n","       # In[7]:\n","       X_ten_years_fertlizers = Australia_Fertilizers_Products_Agricultural.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = Australia_Fertilizers_Products_Agricultural['Y2017']\n","       print(\"2017 \\n\",fertilizer_2017)\n","\n","       # In[8]:\n","       X_ten_years_pesticides = Australia_Pesticides.drop(['Area Code', 'Area', 'Item Code', 'Element Code', 'Element',\n","              'Unit', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_pesticides.columns)\n","       pesticides_2017 = Australia_Pesticides['Y2017']\n","       print(\"2017 \\n\",pesticides_2017)\n","\n","       # %<br>\n","       # Drop Ammonia anyhydrous and Ammonia nitrate from 2007 to 2013. It has not been used in these years.\n","       # In[9]:\n","       X_ten_years_fertlizers = X_ten_years_fertlizers.drop([602, 607], axis=0)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = fertilizer_2017.drop([602, 607], axis=0)\n","       print(fertilizer_2017)\n","       \n","       # %<br>\n","       # 2009 to 2013\n","       # ## Handle missing values from 2009 to 2013\n","       # In[10]:\n","       interpolate_2009_to_2016 = X_ten_years_fertlizers.interpolate()\n","       interpolate_2009_to_2016\n","       interpolate_2017 = fertilizer_2017.interpolate()\n","       print(\"interpolated 2017 fertilizers \\n\",interpolate_2017)\n","       # TODO: Don't just interpolate. Use the means of the previous 2  years\n","       # interpolate_pesticides_2017 = pesticides_2017.interpolate()\n","       # print(\"interpolated 2017 pesticides \\n\",interpolate_pesticides_2017)\n","\n","       # %<br>\n","       #  Interpolate missing fertlizers data<br>\n","       #  Replace na with mean values of 2004,2005,2009,2010\n","       # ##  Replace 2007 nan values with the mean of 2004 and 2005\n","\n","       # In[11]:\n","\n","       interpolate_2004_2005 = Australia_Fertilizers_Products_Agricultural[['Y2004','Y2005']].copy().interpolate()\n","       index_names = interpolate_2004_2005.index\n","       index_names\n","       interpolate_2004_2005=interpolate_2004_2005.drop([602,607], axis=0)\n","       interpolate_2004_2005['mean'] = interpolate_2004_2005.mean(axis=1)\n","       print(\"Mean of 2004 and 2005 ==== \\n\")\n","       print(interpolate_2004_2005)\n","       pesticides_2015_2016 = Australia_Pesticides[['Y2015','Y2016']]\n","       pesticides_2015_2016['mean'] = pesticides_2015_2016.mean(axis=1)\n","       print(\"Mean of 2015 and 2016 === \\n\")\n","       print(pesticides_2015_2016)\n","\n","       # Use this for 2007\n","\n","       # In[ ]:\n","       # ##  Replace 2008 nan values with the mean of 2009 and 2010\n","\n","       # In[12]:\n","       interpolate_2009_2010 = Australia_Fertilizers_Products_Agricultural[['Y2009','Y2010']].copy().interpolate()\n","       index_names = interpolate_2009_2010.index\n","       index_names\n","\n","       # In[13]:\n","       interpolate_2009_2010['mean'] = interpolate_2009_2010.mean(axis=1)\n","       print(\"Mean of 2009 and 2010 ==== \\n\")\n","       # Use this for 2008\n","       interpolate_2009_2010=interpolate_2009_2010.drop([602,607], axis=0)\n","       print(interpolate_2009_2010)\n","\n","\n","       # ## Populate 2007 nan values with computed mean\n","       # In[14]:\n","       # Fertilizers\n","       i=0\n","       for i in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:i,1:2] = interpolate_2004_2005['mean'].iloc[0:i]\n","\n","       #Pesticides\n","       j=0\n","       for j in range(1,7):\n","              pesticides_2017.iloc[0:j,] = pesticides_2015_2016['mean'].iloc[0:j]\n","\n","       # ## Populate 2008 nan values with computed mean\n","       # In[15]:\n","       j=0\n","       for j in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:j,2:3] = interpolate_2009_2010['mean'].iloc[0:j]\n","                \n","       interpolate_2009_to_2016.iloc[15, 1] = 1437658.50\n","       print(interpolate_2009_to_2016.iloc[15, 1])\n","       interpolate_2009_to_2016.iloc[15, 2] = 171351.00\n","\n","       # In[16]:\n","       X_ten_years_fertilizers_interpolated = interpolate_2009_to_2016\n","       X_ten_years_fertilizers_interpolated\n","       fertilizer_2017_interpolated = interpolate_2017\n","       print(fertilizer_2017_interpolated)\n","\n","       # #### TODO: Combine pesticdes and fertilizers data\n","       # In[17]:\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_fertilizers_interpolated)\n","\n","       pesticides_without_Item = X_ten_years_pesticides.copy().drop(['Item'], axis=1)\n","       pesticides_2017_wo_item = pesticides_2017\n","       fertilizers_without_Item = X_ten_years_fertilizers_interpolated.copy().drop(['Item'], axis=1)\n","       fertilizer_2017_wo_item = fertilizer_2017_interpolated\n","       # torch.tensor(X_ten_years_pesticides.values.astype(np.float64))\n","       print(X_ten_years_pesticides.values)\n","       print(X_ten_years_fertilizers_interpolated.values)\n","       print(pesticides_2017_wo_item.values)\n","       print(fertilizer_2017_wo_item.values)\n","\n","       print(pesticides_without_Item)\n","       pesticides_tensor = torch.from_numpy(pesticides_without_Item.values)\n","       print(\"\\t Pesticides tensor\", pesticides_tensor)\n","       pesticides_2017_tensor = torch.from_numpy(pesticides_2017_wo_item.values)\n","\n","       print(fertilizers_without_Item)\n","       fertilizers_tensor = torch.from_numpy(fertilizers_without_Item.values)\n","       fertilizer_2017_tensor = torch.from_numpy(fertilizer_2017_wo_item.values)\n","       print(\"\\t Fertilizers tensor\", fertilizers_tensor)\n","\n","       # In[18]:\n","       print(\"Train : \\n\",pesticides_tensor.shape)\n","       print(\"Train: \\n\",fertilizers_tensor.shape)\n","       print(\"Test: \\n\",pesticides_2017_tensor.shape)\n","       print(\"Test: \\n\",fertilizer_2017_tensor.shape)\n","\n","       # TODO: Normalize inputs before concatenation\n","       pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n","       print(pesticides_and_fertilizers.shape)\n","       print(\"\\n\")\n","       print(\"Training X data for 10 years: \",pesticides_and_fertilizers)\n","       print(\"\\n\")\n","       pesticides_and_fertilizers_2017 = torch.cat((pesticides_2017_tensor, fertilizer_2017_tensor), dim=0)\n","       print(pesticides_and_fertilizers_2017.shape)\n","       print(\"\\n\")\n","       print(\"Testing X data for 1 year: \",pesticides_and_fertilizers_2017)\n","       print(\"\\n\")\n","\n","       # Training + Validation data : Input\n","       X_train_ten = pesticides_and_fertilizers;\n","       X_test = pesticides_and_fertilizers_2017\n","\n","       # #### TODO: Create output vector\n","\n","       # In[19]:\n","       print(yield_frame.columns)\n","       #%%\n","       yield_frame.head\n","       #%%\n","       Australia = yield_frame['Area']==\"Australia\"\n","       Production = yield_frame['Element']==\"Production\"\n","       Australia_yield = yield_frame[Australia]\n","       Australia_yield_production = Australia_yield[Production]\n","       print(yield_frame[Australia])\n","       #%%\n","       # Get the yield data for the year 2017 only\n","       Australia_yield_2017 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015', \n","              'Y2016', 'Y2018'], axis=1)\n","       print(\"\\n\")\n","       print(\"Test Y data: \",Australia_yield_2017)\n","       print(\"\\n\")\n","       print(Australia_yield_2017.columns)\n","       print(Australia_yield_2017['Item'].unique())\n","       print(\"Number of unique crops in 2017: \",len(Australia_yield_2017['Item'].unique()))\n","\n","       Australia_yield_2007_to_2016 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017','Y2018'], axis=1)\n","       print(Australia_yield_2007_to_2016.columns)\n","       print(Australia_yield_2007_to_2016['Item'].unique())\n","       print(\"\\n\")\n","       print(\"Train Y data: \",Australia_yield_2007_to_2016)\n","       print(\"\\n\")\n","       print(\"Number of unique crops from 2007 to 2016\",len(Australia_yield_2007_to_2016['Item'].unique()))\n","\n","       # In[20]:\n","       yield_without_Item = Australia_yield_2007_to_2016.copy().drop(['Item'],axis=1)\n","       yield_without_Item_2017 = Australia_yield_2017.copy().drop(['Item'],axis=1)\n","\n","       print(yield_without_Item)\n","       yield_tensor = torch.from_numpy(yield_without_Item.values)\n","       print(\"Shape: \", yield_tensor.shape)\n","       print(\"\\t Yield tensor\", yield_tensor)\n","       print(yield_without_Item_2017)\n","       yield_tensor_2017 = torch.from_numpy(yield_without_Item_2017.values)\n","       print(\"Shape: \", yield_tensor_2017.shape)\n","       print(\"\\t Yield tensor\", yield_tensor_2017)\n","\n","       # Training + Validation: Output\n","       Y_train_ten = yield_tensor\n","       # Test Y\n","       Y_test = yield_tensor_2017\n","\n","       print(\"Train X: \\n\",X_train_ten.shape)\n","       print(\"Train Y: \\n\",Y_train_ten.shape)\n","       print(\"Test X: \\n\",X_test.shape)\n","       print(\"Test Y: \\n\",Y_test.shape)\n","       \n","       return [X_train_ten, Y_train_ten, X_test, Y_test]"]},{"cell_type":"code","execution_count":25,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["04, 5.0922e+04, 6.3416e+04],\n","        [7.2630e+03, 9.6109e+03, 8.5209e+03, 9.4395e+03, 1.0662e+04, 1.0899e+04,\n","         1.0113e+04, 1.1160e+04, 1.1399e+04, 1.4196e+04],\n","        [2.2404e+04, 2.9647e+04, 2.6284e+04, 2.9118e+04, 3.2890e+04, 3.3619e+04,\n","         3.1195e+04, 3.4426e+04, 3.5161e+04, 4.3789e+04],\n","        [2.3249e+03, 3.0765e+03, 2.7276e+03, 3.0216e+03, 3.4131e+03, 3.4887e+03,\n","         3.2371e+03, 3.5725e+03, 3.6487e+03, 4.5441e+03],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [3.0365e+05, 1.6086e+05,        nan, 1.6086e+05, 4.7329e+04, 1.5680e+05,\n","         1.3463e+05, 1.3892e+05, 1.9200e+05, 1.7258e+05],\n","        [2.0392e+04, 2.9583e+05, 7.4000e+01, 5.9159e+05, 2.6952e+05, 6.4043e+05,\n","         1.4296e+05, 3.7954e+05, 4.9362e+05, 4.8665e+05],\n","        [7.3190e+04, 6.1536e+05, 2.0840e+05, 1.0223e+06, 4.9172e+05, 1.1241e+06,\n","         1.5129e+05, 6.2017e+05, 7.9523e+05, 8.0072e+05],\n","        [1.2599e+05, 7.7861e+05, 4.1672e+05, 1.1405e+06, 7.1392e+05, 9.8775e+05,\n","         1.5962e+05, 8.6079e+05, 1.0969e+06, 1.1148e+06],\n","        [1.7879e+05, 6.4986e+05, 4.1037e+04, 1.2587e+06, 1.1699e+05, 8.5143e+05,\n","         1.6795e+05, 6.1150e+05, 7.0043e+05, 6.6312e+05],\n","        [2.3158e+05, 3.2902e+05, 2.1622e+04, 6.3643e+05, 7.9886e+04, 7.1511e+05,\n","         1.7628e+05, 3.6221e+05, 3.0400e+05, 2.1145e+05],\n","        [2.8438e+05, 8.1850e+03, 2.2080e+03, 1.4162e+04, 4.2783e+04, 5.7880e+05,\n","         1.8461e+05, 3.5574e+05, 3.5850e+05, 2.3647e+05],\n","        [3.3718e+05, 1.0243e+04, 1.1690e+03, 1.9317e+04, 2.1696e+04, 4.4248e+05,\n","         1.9294e+05, 3.4928e+05, 4.1300e+05, 2.6149e+05],\n","        [3.8998e+05, 1.2301e+04, 1.3000e+02, 2.4472e+04, 6.0900e+02, 3.0616e+05,\n","         2.0127e+05, 2.9040e+05, 3.2642e+05, 2.2314e+05],\n","        [4.4278e+05, 1.6292e+04, 2.9580e+03, 2.9627e+04, 2.2152e+05, 1.6984e+05,\n","         2.0960e+05, 2.3152e+05, 2.3984e+05, 1.8479e+05],\n","        [2.3410e+05, 2.0284e+04, 5.7860e+03, 3.4782e+04, 1.2869e+04, 3.3525e+04,\n","         3.1054e+05, 4.0976e+04, 5.9000e+04, 4.8019e+04],\n","        [2.5426e+04, 9.8828e+04, 2.1154e+04, 1.7650e+05, 5.0525e+04, 1.2845e+05,\n","         4.1147e+05, 3.3659e+05, 4.0550e+05, 3.9277e+05],\n","        [2.8115e+05, 5.2029e+05, 7.8618e+04, 9.6196e+05, 8.9395e+04, 8.9976e+05,\n","         5.1240e+05, 6.3220e+05, 7.5200e+05, 7.3752e+05],\n","        [1.1858e+06, 7.0965e+05, 4.0956e+05, 1.0097e+06, 7.8751e+05, 1.0286e+06,\n","         1.2400e+05, 1.6100e+05, 1.9800e+05, 1.4450e+05],\n","        [1.4377e+06, 8.9901e+05, 7.4050e+05, 1.0575e+06, 1.4856e+06, 1.1575e+06,\n","         1.0925e+06, 1.7532e+06, 1.7720e+06, 1.5482e+06],\n","        [1.4377e+06, 1.7135e+05, 1.1072e+05, 2.3198e+05, 1.4856e+06, 2.7388e+05,\n","         1.4371e+05, 1.2240e+05, 1.0940e+05, 1.4470e+05]], dtype=torch.float64)\n","\n","\n","torch.Size([22])\n","\n","\n","Testing X data for 1 year:  tensor([5.7169e+04, 1.2797e+04, 3.9475e+04, 4.0964e+03, 8.0076e+02, 8.0076e+02,\n","        1.4642e+05, 4.1860e+05, 6.9077e+05, 9.6295e+05, 6.2543e+05, 2.8791e+05,\n","        3.2327e+05, 3.5863e+05, 2.4948e+05, 1.4034e+05, 2.5094e+04, 3.6271e+05,\n","        7.0032e+05, 1.7607e+05, 1.6783e+06, 1.2868e+05], dtype=torch.float64)\n","\n","\n","Index(['Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element',\n","       'Unit', 'Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967',\n","       'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', 'Y1974', 'Y1975',\n","       'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983',\n","       'Y1984', 'Y1985', 'Y1986', 'Y1987', 'Y1988', 'Y1989', 'Y1990', 'Y1991',\n","       'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999',\n","       'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007',\n","       'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015',\n","       'Y2016', 'Y2017', 'Y2018'],\n","      dtype='object')\n","      Area Code       Area  Item Code                              Item  \\\n","1317         10  Australia        221               Almonds, with shell   \n","1318         10  Australia        221               Almonds, with shell   \n","1319         10  Australia        221               Almonds, with shell   \n","1320         10  Australia        711  Anise, badian, fennel, coriander   \n","1321         10  Australia        711  Anise, badian, fennel, coriander   \n","...         ...        ...        ...                               ...   \n","1606         10  Australia       1729                   Treenuts, Total   \n","1607         10  Australia       1729                   Treenuts, Total   \n","1608         10  Australia       1735                Vegetables Primary   \n","1609         10  Australia       1735                Vegetables Primary   \n","1610         10  Australia       1735                Vegetables Primary   \n","\n","      Element Code         Element    Unit     Y1961     Y1962     Y1963  ...  \\\n","1317          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1318          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","1319          5510      Production  tonnes       NaN       NaN       NaN  ...   \n","1320          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1321          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","...            ...             ...     ...       ...       ...       ...  ...   \n","1606          5419           Yield   hg/ha   17925.0   15042.0   15856.0  ...   \n","1607          5510      Production  tonnes     190.0     179.0     176.0  ...   \n","1608          5312  Area harvested      ha   63571.0   63397.0   65291.0  ...   \n","1609          5419           Yield   hg/ha  100846.0  103588.0  101085.0  ...   \n","1610          5510      Production  tonnes  641089.0  656717.0  659995.0  ...   \n","\n","          Y2009      Y2010      Y2011      Y2012      Y2013      Y2014  \\\n","1317    27981.0    29340.0    30390.0    28472.0    28586.0    28967.0   \n","1318     6775.0    30675.0    11377.0    10925.0    19863.0    19325.0   \n","1319    18957.0    90000.0    34576.0    31105.0    56779.0    55978.0   \n","1320      710.0      684.0      914.0     1015.0     1040.0     1000.0   \n","1321    13901.0    11853.0    11370.0    11488.0    11550.0    11570.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1606    12376.0    24996.0    12309.0    11661.0    17257.0    17692.0   \n","1607    60694.0   130580.0    70926.0    65324.0    92450.0    94930.0   \n","1608    66234.0    69695.0    74100.0    71471.0    66887.0    69262.0   \n","1609   270772.0   259637.0   224916.0   253910.0   277885.0   238561.0   \n","1610  1793425.0  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0   \n","\n","          Y2015      Y2016      Y2017      Y2018  \n","1317    31115.0    37000.0    38000.0    36940.0  \n","1318    20354.0    19703.0    19835.0    18917.0  \n","1319    63331.0    72902.0    75373.0    69880.0  \n","1320     1011.0     1108.0     1130.0     1074.0  \n","1321    11600.0    11673.0    11721.0    11769.0  \n","...         ...        ...        ...        ...  \n","1606    16017.0    17399.0    17430.0    16445.0  \n","1607   106374.0   118977.0   125683.0   118104.0  \n","1608    66917.0    68090.0    71918.0    70457.0  \n","1609   264193.0   257780.0   245961.0   252789.0  \n","1610  1767902.0  1755227.0  1768915.0  1781067.0  \n","\n","[294 rows x 65 columns]\n","\n","\n","Test Y data:                                    Item      Y2017\n","1319               Almonds, with shell    75373.0\n","1322  Anise, badian, fennel, coriander     1324.0\n","1325                            Apples   313730.0\n","1328                          Apricots     5351.0\n","1331                         Asparagus     7472.0\n","...                                ...        ...\n","1598          Oilcrops, Oil Equivalent  1888936.0\n","1601                     Pulses, Total  4129481.0\n","1604           Roots and Tubers, Total  1176669.0\n","1607                   Treenuts, Total   125683.0\n","1610                Vegetables Primary  1768915.0\n","\n","[101 rows x 2 columns]\n","\n","\n","Index(['Item', 'Y2017'], dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","Number of unique crops in 2017:  101\n","Index(['Item', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013',\n","       'Y2014', 'Y2015', 'Y2016'],\n","      dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","\n","\n","Train Y data:                                    Item      Y2007      Y2008      Y2009  \\\n","1319               Almonds, with shell    18024.0    65000.0    18957.0   \n","1322  Anise, badian, fennel, coriander       51.0     2091.0      987.0   \n","1325                            Apples   377980.0   265481.0   295134.0   \n","1328                          Apricots    17327.0    17000.0    13673.0   \n","1331                         Asparagus     5609.0     9779.0     6981.0   \n","...                                ...        ...        ...        ...   \n","1598          Oilcrops, Oil Equivalent   311986.0   553601.0   836928.0   \n","1601                     Pulses, Total  1168635.0  1681825.0  1817218.0   \n","1604           Roots and Tubers, Total  1261119.0  1447602.0  1220994.0   \n","1607                   Treenuts, Total    60924.0   103690.0    60694.0   \n","1610                Vegetables Primary  1746974.0  1703422.0  1793425.0   \n","\n","          Y2010      Y2011      Y2012      Y2013      Y2014      Y2015  \\\n","1319    90000.0    34576.0    31105.0    56779.0    55978.0    63331.0   \n","1322      810.0     1039.0     1166.0     1201.0     1157.0     1173.0   \n","1325   264401.0   299778.0   289064.0   288878.0   266771.0   295196.0   \n","1328    13175.0    13283.0    12186.0    11551.0     9238.0     9674.0   \n","1331     8835.0    10276.0     9589.0     8396.0     8375.0     8288.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   869470.0  1152688.0  1647064.0  1869619.0  1709984.0  1503124.0   \n","1601  2143709.0  2551800.0  2633541.0  2224400.0  2247300.0  1989200.0   \n","1604  1327922.0  1185085.0  1350721.0  1341112.0  1246546.0  1215195.0   \n","1607   130580.0    70926.0    65324.0    92450.0    94930.0   106374.0   \n","1610  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0  1767902.0   \n","\n","          Y2016  \n","1319    72902.0  \n","1322     1293.0  \n","1325   308298.0  \n","1328     8700.0  \n","1331     7737.0  \n","...         ...  \n","1598  1239283.0  \n","1601  2421811.0  \n","1604  1200748.0  \n","1607   118977.0  \n","1610  1755227.0  \n","\n","[101 rows x 11 columns]\n","\n","\n","Number of unique crops from 2007 to 2016 101\n","          Y2007      Y2008      Y2009      Y2010      Y2011      Y2012  \\\n","1319    18024.0    65000.0    18957.0    90000.0    34576.0    31105.0   \n","1322       51.0     2091.0      987.0      810.0     1039.0     1166.0   \n","1325   377980.0   265481.0   295134.0   264401.0   299778.0   289064.0   \n","1328    17327.0    17000.0    13673.0    13175.0    13283.0    12186.0   \n","1331     5609.0     9779.0     6981.0     8835.0    10276.0     9589.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   311986.0   553601.0   836928.0   869470.0  1152688.0  1647064.0   \n","1601  1168635.0  1681825.0  1817218.0  2143709.0  2551800.0  2633541.0   \n","1604  1261119.0  1447602.0  1220994.0  1327922.0  1185085.0  1350721.0   \n","1607    60924.0   103690.0    60694.0   130580.0    70926.0    65324.0   \n","1610  1746974.0  1703422.0  1793425.0  1809529.0  1666625.0  1814715.0   \n","\n","          Y2013      Y2014      Y2015      Y2016  \n","1319    56779.0    55978.0    63331.0    72902.0  \n","1322     1201.0     1157.0     1173.0     1293.0  \n","1325   288878.0   266771.0   295196.0   308298.0  \n","1328    11551.0     9238.0     9674.0     8700.0  \n","1331     8396.0     8375.0     8288.0     7737.0  \n","...         ...        ...        ...        ...  \n","1598  1869619.0  1709984.0  1503124.0  1239283.0  \n","1601  2224400.0  2247300.0  1989200.0  2421811.0  \n","1604  1341112.0  1246546.0  1215195.0  1200748.0  \n","1607    92450.0    94930.0   106374.0   118977.0  \n","1610  1858687.0  1652335.0  1767902.0  1755227.0  \n","\n","[101 rows x 10 columns]\n","Shape:  torch.Size([101, 10])\n","\t Yield tensor tensor([[1.8024e+04, 6.5000e+04, 1.8957e+04,  ..., 5.5978e+04, 6.3331e+04,\n","         7.2902e+04],\n","        [5.1000e+01, 2.0910e+03, 9.8700e+02,  ..., 1.1570e+03, 1.1730e+03,\n","         1.2930e+03],\n","        [3.7798e+05, 2.6548e+05, 2.9513e+05,  ..., 2.6677e+05, 2.9520e+05,\n","         3.0830e+05],\n","        ...,\n","        [1.2611e+06, 1.4476e+06, 1.2210e+06,  ..., 1.2465e+06, 1.2152e+06,\n","         1.2007e+06],\n","        [6.0924e+04, 1.0369e+05, 6.0694e+04,  ..., 9.4930e+04, 1.0637e+05,\n","         1.1898e+05],\n","        [1.7470e+06, 1.7034e+06, 1.7934e+06,  ..., 1.6523e+06, 1.7679e+06,\n","         1.7552e+06]], dtype=torch.float64)\n","          Y2017\n","1319    75373.0\n","1322     1324.0\n","1325   313730.0\n","1328     5351.0\n","1331     7472.0\n","...         ...\n","1598  1888936.0\n","1601  4129481.0\n","1604  1176669.0\n","1607   125683.0\n","1610  1768915.0\n","\n","[101 rows x 1 columns]\n","Shape:  torch.Size([101, 1])\n","\t Yield tensor tensor([[7.5373e+04],\n","        [1.3240e+03],\n","        [3.1373e+05],\n","        [5.3510e+03],\n","        [7.4720e+03],\n","        [5.6501e+04],\n","        [4.1297e+05],\n","        [1.3506e+07],\n","        [3.5396e+04],\n","        [4.1373e+04],\n","        [4.4250e+03],\n","        [4.7570e+03],\n","        [3.7604e+05],\n","        [9.9102e+04],\n","        [4.8680e+03],\n","        [2.8382e+05],\n","        [1.4066e+05],\n","        [1.1532e+04],\n","        [2.0040e+06],\n","        [3.8579e+04],\n","        [7.7449e+05],\n","        [1.2600e+06],\n","        [       nan],\n","        [1.7609e+04],\n","        [5.7900e+02],\n","        [       nan],\n","        [7.8000e+01],\n","        [       nan],\n","        [1.5270e+03],\n","        [       nan],\n","        [7.5400e+03],\n","        [6.9880e+03],\n","        [1.8244e+06],\n","        [1.6529e+04],\n","        [7.4900e+02],\n","        [2.3740e+03],\n","        [3.7227e+04],\n","        [2.1860e+05],\n","        [1.4626e+05],\n","        [6.0000e+03],\n","        [1.0314e+06],\n","        [4.3619e+05],\n","        [1.0266e+05],\n","        [4.3748e+04],\n","        [2.6353e+05],\n","        [3.5855e+04],\n","        [4.6326e+04],\n","        [       nan],\n","        [4.6165e+04],\n","        [2.2655e+06],\n","        [1.2257e+05],\n","        [2.6324e+05],\n","        [3.3232e+05],\n","        [6.3850e+03],\n","        [8.2659e+04],\n","        [9.6741e+04],\n","        [4.1519e+05],\n","        [2.0574e+04],\n","        [7.1500e+02],\n","        [8.5922e+04],\n","        [1.4950e+03],\n","        [1.7561e+04],\n","        [1.1052e+06],\n","        [3.6880e+04],\n","        [1.0150e+05],\n","        [       nan],\n","        [4.3132e+06],\n","        [6.9700e+02],\n","        [8.0730e+05],\n","        [3.0000e+04],\n","        [4.9570e+03],\n","        [2.1510e+06],\n","        [9.9400e+05],\n","        [3.1000e+04],\n","        [1.0719e+04],\n","        [4.5251e+04],\n","        [3.6561e+07],\n","        [1.7000e+04],\n","        [7.1475e+04],\n","        [1.2727e+05],\n","        [2.3540e+03],\n","        [3.7158e+05],\n","        [1.4995e+05],\n","        [7.7444e+04],\n","        [1.1955e+04],\n","        [2.6500e+03],\n","        [1.8088e+05],\n","        [3.1819e+07],\n","        [4.9780e+07],\n","        [5.0048e+07],\n","        [5.0533e+05],\n","        [1.7422e+07],\n","        [7.7449e+05],\n","        [3.9737e+06],\n","        [5.7713e+06],\n","        [3.2762e+06],\n","        [1.8889e+06],\n","        [4.1295e+06],\n","        [1.1767e+06],\n","        [1.2568e+05],\n","        [1.7689e+06]], dtype=torch.float64)\n","Train X: \n"," torch.Size([22, 10])\n","Train Y: \n"," torch.Size([101, 10])\n","Test X: \n"," torch.Size([22])\n","Test Y: \n"," torch.Size([101, 1])\n","/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:96: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # ## Handle missing values from 2009 to 2013\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_single_block(indexer, value, name)\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value, self.name)\n","/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:203: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n"]}],"source":["\n","training_input = load_data()[0]\n","training_ouput = load_data()[1]\n","test_input = load_data()[2]\n","test_output = load_data()[3]\n","\n","x = training_input\n","y = training_ouput"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["\n","# Transforms\n","\n","# Training data\n","x_tensor = torch.Tensor(x.float())\n","x_data = (x - x.mean())/(x.max() - x.min())\n","test = torch.Tensor([[0,4,5]])\n","x_tensor_normalized = x.normal_()\n","y_data = (y.normal_())\n","\n","transformed_x = torch.reshape(x_tensor_normalized,(1,220))\n","transformed_y = y_data\n","y = torch.reshape(y, (1,1010))\n",""]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# Test data\n","transformed_test_x = torch.Tensor(test_input.float())\n","transformed_test_x = transformed_test_x.normal_()\n","transformed_test_y = torch.Tensor(test_output.float())\n","transformed_test_y = transformed_test_y.normal_()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Define model\n","class NeuralNet(nn.Module):\n","       def __init__(self, D_in, H1, H2, H3, H4, H5, H6, D_out):\n","              super(NeuralNet, self).__init__()\n","              self.linear1 = nn.Linear(D_in, H1)\n","              self.linear2 = nn.Linear(H1, H2)\n","              self.linear3 = nn.Linear(H2, H3)\n","              self.linear4 = nn.Linear(H3, H4)\n","              self.linear5 = nn.Linear(H4, H5)\n","              self.linear6 = nn.Linear(H5, H6)\n","              self.linear7 = nn.Linear(H6, D_out)\n","              self.relu = nn.ReLU()\n","       def forward(self, x):\n","              y_pred = torch.tanh(self.linear1(x))\n","              y_pred = self.linear2(y_pred)\n","              y_pred = torch.tanh(self.linear3(y_pred))\n","              y_pred = self.linear4(y_pred)\n","              y_pred = torch.tanh(self.linear5(y_pred))\n","              y_pred = self.linear6(y_pred)\n","              y_pred = torch.sigmoid(self.linear7(y_pred))\n","              return y_pred\n",""]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["\n","\n","from functools import partial\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","from random import randint\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n",""]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["\n","\n","def get_accuracy(model, predicted, actual, threshold_percentage):\n","    num_items = len(actual)\n","    X = predicted.view(num_items)\n","    Y = actual.view(num_items)\n","    num_correct = torch.sum(torch.abs(X-Y)<torch.abs(threshold_percentage*Y))\n","    accuracy = (num_correct.item()*100.0/num_items)\n","    return (num_correct, accuracy)\n","    \n","\n","def random_split_training(trainset_x, trainset_y):\n","       # TODO:\n","       # Select 2 random numbers between 1 and 10\n","       # Use them to index into x and y.\n","       # That will give us the training and validation set\n","       # Rest of the indices are thus our training set\n","       index1 = randint(0,9)\n","       index2 = randint(0,9)\n","       indices=[]\n","       i=0\n","       for i in range(0,10):\n","              if(i not in [index1,index2]):\n","                 indices.append(i)    \n","       \n","       indices = torch.tensor(indices)\n","       val_indices = torch.tensor([index1, index2])\n","       t_subset_x = torch.index_select(trainset_x, 1, indices)\n","       t_subset_y = torch.index_select(trainset_y, 1, indices)\n","       v_subset_x = torch.index_select(trainset_x, 1,val_indices)\n","       v_subset_y = torch.index_select(trainset_y, 1, val_indices)\n","       return (t_subset_x, t_subset_y, v_subset_x, v_subset_y)\n","# \n",""]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# Function to train and validate data. Save the best model based on validation loss\n","# \n","# params:\n","# config: hyperparameter search space\n","# \n","def train_crop_yield(config):\n","       net = NeuralNet(176, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],8)\n","       criterion = torch.nn.MSELoss()\n","       optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","       # train_set = torch.cat((transformed_x, transformed_y), dim=0)\n","       # test_set = torch.cat((test_input, test_output), dim=0)\n","       \n","       x_by_years = transformed_x.reshape(22,10)\n","       y_by_years = transformed_y.reshape(101,10)\n","       \n","       train_subset_x, train_subset_y, val_subset_x, val_subset_y = random_split_training(x_by_years, y_by_years)\n","       \n","       for epoch in range(500):\n","              running_loss = 0.0\n","              epoch_steps = 0\n","              # Zero the accumulated gradients\n","              optimizer.zero_grad()\n","              # forward + backward + optimize\n","              output = net(train_subset_x.float().reshape(1,176))\n","              loss = criterion(output, train_subset_y.float())\n","              loss.backward()\n","              optimizer.step()\n","              # print statistics\n","              running_loss += loss.item()\n","              epoch_steps += 1\n","              if epoch%5==4:\n","                     print(\"[%d] loss: %.3f\"%(epoch+1, running_loss/epoch_steps))\n","                     running_loss=0.0\n","       \n","       # Validation loss\n","       net = NeuralNet(44, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],2)\n","       val_loss = 0.0\n","       val_steps = 0\n","       total = 0\n","       correct = 0\n","       with torch.no_grad():\n","              val_output = net(val_subset_x.float().reshape(1,44))\n","              total = val_subset_y.size(0)\n","              correct = torch.sum(torch.abs(val_output-val_subset_y)<torch.abs(0.10*val_subset_y))\n","              loss = criterion(val_output, val_subset_y.float())\n","              val_loss += loss.cpu().numpy()\n","              torch.save((net.state_dict(), optimizer.state_dict()), './trained_net.dat')\n","              tune.report(loss=(val_loss), accuracy=(correct/total))\n","       print(\"\\n Finished Training \\n\")\n","              "]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["2021-05-16 17:49:58,497\tWARNING experiment.py:294 -- No name detected on trainable. Using DEFAULT.\n","2021-05-16 17:49:58,501\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58<br>Number of trials: 10/10 (10 PENDING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_80c0d_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0317173</td></tr>\n<tr><td>DEFAULT_80c0d_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0209972</td></tr>\n<tr><td>DEFAULT_80c0d_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.052015 </td></tr>\n<tr><td>DEFAULT_80c0d_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0978931</td></tr>\n<tr><td>DEFAULT_80c0d_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0898699</td></tr>\n<tr><td>DEFAULT_80c0d_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0107664</td></tr>\n<tr><td>DEFAULT_80c0d_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0111338</td></tr>\n<tr><td>DEFAULT_80c0d_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0274834</td></tr>\n<tr><td>DEFAULT_80c0d_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.022764 </td></tr>\n<tr><td>DEFAULT_80c0d_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0117928</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.2/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_80c0d_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0317173</td></tr>\n<tr><td>DEFAULT_80c0d_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0209972</td></tr>\n<tr><td>DEFAULT_80c0d_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.052015 </td></tr>\n<tr><td>DEFAULT_80c0d_00003</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0978931</td></tr>\n<tr><td>DEFAULT_80c0d_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0898699</td></tr>\n<tr><td>DEFAULT_80c0d_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0107664</td></tr>\n<tr><td>DEFAULT_80c0d_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0111338</td></tr>\n<tr><td>DEFAULT_80c0d_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0274834</td></tr>\n<tr><td>DEFAULT_80c0d_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.022764 </td></tr>\n<tr><td>DEFAULT_80c0d_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0117928</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=80005)\u001b[0m 2021-05-16 17:50:09,004\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80005)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m 2021-05-16 17:50:09,004\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80003)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m 2021-05-16 17:50:09,004\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80004)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m 2021-05-16 17:50:09,005\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80002)\u001b[0m \n","2021-05-16 17:50:09,193\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00003: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80003, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80003, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_80c0d_00003:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58<br>Number of trials: 10/10 (1 ERROR, 6 PENDING, 3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_80c0d_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0317173</td></tr>\n<tr><td>DEFAULT_80c0d_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0209972</td></tr>\n<tr><td>DEFAULT_80c0d_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.052015 </td></tr>\n<tr><td>DEFAULT_80c0d_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0898699</td></tr>\n<tr><td>DEFAULT_80c0d_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0107664</td></tr>\n<tr><td>DEFAULT_80c0d_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0111338</td></tr>\n<tr><td>DEFAULT_80c0d_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0274834</td></tr>\n<tr><td>DEFAULT_80c0d_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.022764 </td></tr>\n<tr><td>DEFAULT_80c0d_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0117928</td></tr>\n<tr><td>DEFAULT_80c0d_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0978931</td></tr>\n</tbody>\n</table><br>Number of errored trials: 1<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                       </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_80c0d_00003</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00003_3_H1=128,H2=64,H3=128,H4=2048,H5=64,H6=32,lr=0.097893_2021-05-16_17-50-03/error.txt</td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2021-05-16 17:50:09,312\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00001: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80005, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80005, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:50:09,340\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00000: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80004, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80004, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:50:09,374\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00002: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80002, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80002, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_80c0d_00001:\n","  {}\n","  \n","Result for DEFAULT_80c0d_00000:\n","  {}\n","  \n","Result for DEFAULT_80c0d_00002:\n","  {}\n","  \n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m 2021-05-16 17:50:16,296\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80025)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m 2021-05-16 17:50:16,296\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80024)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m 2021-05-16 17:50:16,296\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80023)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m 2021-05-16 17:50:16,296\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80026)\u001b[0m \n","2021-05-16 17:50:16,480\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00007: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80025, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80025, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_80c0d_00007:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.2/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58<br>Number of trials: 10/10 (5 ERROR, 2 PENDING, 3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_80c0d_00004</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0898699</td></tr>\n<tr><td>DEFAULT_80c0d_00005</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0107664</td></tr>\n<tr><td>DEFAULT_80c0d_00006</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0111338</td></tr>\n<tr><td>DEFAULT_80c0d_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.022764 </td></tr>\n<tr><td>DEFAULT_80c0d_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0117928</td></tr>\n<tr><td>DEFAULT_80c0d_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0317173</td></tr>\n<tr><td>DEFAULT_80c0d_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0209972</td></tr>\n<tr><td>DEFAULT_80c0d_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.052015 </td></tr>\n<tr><td>DEFAULT_80c0d_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0978931</td></tr>\n<tr><td>DEFAULT_80c0d_00007</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0274834</td></tr>\n</tbody>\n</table><br>Number of errored trials: 5<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                        </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_80c0d_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00000_0_H1=256,H2=32,H3=512,H4=256,H5=32,H6=32,lr=0.031717_2021-05-16_17-49-58/error.txt  </td></tr>\n<tr><td>DEFAULT_80c0d_00001</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00001_1_H1=256,H2=32,H3=1024,H4=256,H5=32,H6=256,lr=0.020997_2021-05-16_17-50-03/error.txt</td></tr>\n<tr><td>DEFAULT_80c0d_00002</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00002_2_H1=256,H2=64,H3=2048,H4=512,H5=64,H6=32,lr=0.052015_2021-05-16_17-50-03/error.txt </td></tr>\n<tr><td>DEFAULT_80c0d_00003</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00003_3_H1=128,H2=64,H3=128,H4=2048,H5=64,H6=32,lr=0.097893_2021-05-16_17-50-03/error.txt </td></tr>\n<tr><td>DEFAULT_80c0d_00007</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00007_7_H1=128,H2=32,H3=128,H4=1024,H5=32,H6=64,lr=0.027483_2021-05-16_17-50-09/error.txt </td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2021-05-16 17:50:16,545\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00004: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80026, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80026, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:50:16,594\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00006: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80024, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80024, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:50:16,621\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00005: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80023, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80023, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_80c0d_00004:\n","  {}\n","  \n","Result for DEFAULT_80c0d_00006:\n","  {}\n","  \n","Result for DEFAULT_80c0d_00005:\n","  {}\n","  \n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m 2021-05-16 17:50:19,254\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80052)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m 2021-05-16 17:50:19,247\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m   File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80051)\u001b[0m \n","2021-05-16 17:50:19,446\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00008: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80051, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80051, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:50:19,492\tERROR trial_runner.py:732 -- Trial DEFAULT_80c0d_00009: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80052, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80052, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-31-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_80c0d_00008:\n","  {}\n","  \n","Result for DEFAULT_80c0d_00009:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58<br>Number of trials: 10/10 (10 ERROR)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_80c0d_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0317173</td></tr>\n<tr><td>DEFAULT_80c0d_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0209972</td></tr>\n<tr><td>DEFAULT_80c0d_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.052015 </td></tr>\n<tr><td>DEFAULT_80c0d_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0978931</td></tr>\n<tr><td>DEFAULT_80c0d_00004</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0898699</td></tr>\n<tr><td>DEFAULT_80c0d_00005</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0107664</td></tr>\n<tr><td>DEFAULT_80c0d_00006</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0111338</td></tr>\n<tr><td>DEFAULT_80c0d_00007</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0274834</td></tr>\n<tr><td>DEFAULT_80c0d_00008</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.022764 </td></tr>\n<tr><td>DEFAULT_80c0d_00009</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0117928</td></tr>\n</tbody>\n</table><br>Number of errored trials: 10<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                        </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_80c0d_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00000_0_H1=256,H2=32,H3=512,H4=256,H5=32,H6=32,lr=0.031717_2021-05-16_17-49-58/error.txt  </td></tr>\n<tr><td>DEFAULT_80c0d_00001</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00001_1_H1=256,H2=32,H3=1024,H4=256,H5=32,H6=256,lr=0.020997_2021-05-16_17-50-03/error.txt</td></tr>\n<tr><td>DEFAULT_80c0d_00002</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00002_2_H1=256,H2=64,H3=2048,H4=512,H5=64,H6=32,lr=0.052015_2021-05-16_17-50-03/error.txt </td></tr>\n<tr><td>DEFAULT_80c0d_00003</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00003_3_H1=128,H2=64,H3=128,H4=2048,H5=64,H6=32,lr=0.097893_2021-05-16_17-50-03/error.txt </td></tr>\n<tr><td>DEFAULT_80c0d_00004</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00004_4_H1=256,H2=32,H3=512,H4=128,H5=64,H6=32,lr=0.08987_2021-05-16_17-50-03/error.txt   </td></tr>\n<tr><td>DEFAULT_80c0d_00005</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00005_5_H1=128,H2=32,H3=256,H4=512,H5=32,H6=64,lr=0.010766_2021-05-16_17-50-09/error.txt  </td></tr>\n<tr><td>DEFAULT_80c0d_00006</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00006_6_H1=256,H2=64,H3=128,H4=1024,H5=64,H6=128,lr=0.011134_2021-05-16_17-50-09/error.txt</td></tr>\n<tr><td>DEFAULT_80c0d_00007</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00007_7_H1=128,H2=32,H3=128,H4=1024,H5=32,H6=64,lr=0.027483_2021-05-16_17-50-09/error.txt </td></tr>\n<tr><td>DEFAULT_80c0d_00008</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00008_8_H1=256,H2=32,H3=256,H4=128,H5=32,H6=256,lr=0.022764_2021-05-16_17-50-09/error.txt </td></tr>\n<tr><td>DEFAULT_80c0d_00009</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-49-58/DEFAULT_80c0d_00009_9_H1=128,H2=64,H3=1024,H4=512,H5=32,H6=64,lr=0.011793_2021-05-16_17-50-16/error.txt </td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"error","ename":"TuneError","evalue":"('Trials did not complete', [DEFAULT_80c0d_00000, DEFAULT_80c0d_00001, DEFAULT_80c0d_00002, DEFAULT_80c0d_00003, DEFAULT_80c0d_00004, DEFAULT_80c0d_00005, DEFAULT_80c0d_00006, DEFAULT_80c0d_00007, DEFAULT_80c0d_00008, DEFAULT_80c0d_00009])","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m~/Masters Research Project/Code/crop-yield/src/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m        \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m        \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [DEFAULT_80c0d_00000, DEFAULT_80c0d_00001, DEFAULT_80c0d_00002, DEFAULT_80c0d_00003, DEFAULT_80c0d_00004, DEFAULT_80c0d_00005, DEFAULT_80c0d_00006, DEFAULT_80c0d_00007, DEFAULT_80c0d_00008, DEFAULT_80c0d_00009])"]}],"source":["# Evaluation:\n","def test_accuracy(net, device='cpu'):\n","       net.eval()\n","       with torch.no_grad():\n","              predicted = net(transformed_test_x.float().reshape(1,22))\n","              return get_accuracy(net, predicted.reshape(101,1), transformed_test_y.reshape(101,1), 0.50)\n","       \n","# Define hyperparameters\n","config = {\n","       \"H1\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H2\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H3\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H4\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H5\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H6\":tune.sample_from(lambda _: 2**np.random.randint(5,9)),\n","       \"lr\":tune.loguniform(1e-2, 1e-1)\n","}\n","\n","# Scheduler. Randomly try out combination of hyperparameters\n","scheduler = ASHAScheduler(\n","       metric=\"loss\",\n","       mode=\"min\",\n","       max_t=10,\n","       grace_period=1,\n","       reduction_factor=2\n",")\n","\n","reporter=CLIReporter(metric_columns=[\"loss\",\"accuracy\", \"training_iteration\"])\n","\n","result = tune.run(\n","       partial(train_crop_yield),\n","       config=config,\n","       num_samples=10,\n","       scheduler=scheduler\n",")\n","\n","best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","\n","print(\"Best trial config: {}\".format(best_trial.config))\n","print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n","print(\"Best trial fnal validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n","\n","best_trained_model = NeuralNet(22, best_trial.config[\"H1\"], best_trial.config[\"H2\"], best_trial.config[\"H3\"],best_trial.config[\"H4\"],best_trial.config[\"H5\"],best_trial.config[\"H6\"],101)\n","\n","device=\"cpu\"\n","best_trained_model.to(device)\n","\n","# best_checkpoint_dir = best_trial.checkpoint.value\n","# print(\" \\n\",type(best_trial.evaluated_params))\n","# model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n","# best_trained_model.load_state_dict(model_state)\n","\n","num_correct, test_acc = test_accuracy(best_trained_model, device)\n","print(\"Best trial test set accuracy: {}\".format(test_acc))\n",""]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["1.8.1\n1.2.4\n"]}],"source":["\n","\n","import torch\n","print(torch.__version__)\n","import pandas as pd\n","print(pd.__version__)\n","from torch.utils.data import DataLoader,ConcatDataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n",""]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["\n","# ## Read pesticides and fertilizer products data\n",""]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["def load_data():\n","       pesticides_frame = pd.read_csv(\"Inputs_Pesticides_Use_E_All_Data_NOFLAG.csv\", engine='python')\n","       pesticides_frame.head\n","  \n","       # Read fertilizer data in the form of products\n","       fertilizers_frame = pd.read_csv(\"Inputs_FertilizersProduct_E_All_Data_NOFLAG.csv\", engine='python')\n","       fertilizers_frame.head\n","       \n","       # Read the crop yield data\n","       yield_frame = pd.read_csv(\"Production_Crops_E_All_Data_NOFLAG.csv\",engine='python')\n","       yield_frame.head\n","\n","       # %<br>\n","       # Get the fertilizer usage in terms of agriculture\n","\n","       # In[4]:     \n","       print(\"Fertilizer products\",fertilizers_frame.columns)\n","       print(\"\\n\")\n","       print(\"Input pesticides\", pesticides_frame.columns)\n","       \n","       # Filter out the data for Australia\n","       is_Country_Australia_Products = fertilizers_frame['Area']==\"Australia\"\n","       Australia_Fertilizers_Products = fertilizers_frame[is_Country_Australia_Products]\n","       # In[5]:\n","       is_Country_Australia_Pesticides = pesticides_frame['Area']==\"Australia\"\n","       Australia_Pesticides = pesticides_frame[is_Country_Australia_Pesticides]\n","\n","       # Get the fertilizer usage for agricultural use\n","       # In[6]:\n","       is_Agricultural = Australia_Fertilizers_Products['Element']==\"Agricultural Use\"\n","       Australia_Fertilizers_Products_Agricultural = Australia_Fertilizers_Products[is_Agricultural]\n","       print(\"Fertilizers \\n\")\n","       print(Australia_Fertilizers_Products_Agricultural.columns)\n","       print(Australia_Fertilizers_Products_Agricultural['Item'])\n","       print(\"\\n\")\n","       print(\"Pesticides \\n\")\n","       print(Australia_Pesticides.columns)\n","       print(Australia_Pesticides['Item'])\n","       \n","       # %<br>\n","       # 7 years for training - 2007 to 2013<br>\n","       # 3 years fro validation - 2014 to 2016<br>\n","       # 2017 for testing\n","       # In[7]:\n","       X_ten_years_fertlizers = Australia_Fertilizers_Products_Agricultural.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = Australia_Fertilizers_Products_Agricultural['Y2017']\n","       print(\"2017 \\n\",fertilizer_2017)\n","\n","       # In[8]:\n","       X_ten_years_pesticides = Australia_Pesticides.drop(['Area Code', 'Area', 'Item Code', 'Element Code', 'Element',\n","              'Unit', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_pesticides.columns)\n","       pesticides_2017 = Australia_Pesticides['Y2017']\n","       print(\"2017 \\n\",pesticides_2017)\n","\n","       # %<br>\n","       # Drop Ammonia anyhydrous and Ammonia nitrate from 2007 to 2013. It has not been used in these years.\n","       # In[9]:\n","       X_ten_years_fertlizers = X_ten_years_fertlizers.drop([602, 607], axis=0)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = fertilizer_2017.drop([602, 607], axis=0)\n","       print(fertilizer_2017)\n","       \n","       # %<br>\n","       # 2009 to 2013\n","       # ## Handle missing values from 2009 to 2013\n","       # In[10]:\n","       interpolate_2009_to_2016 = X_ten_years_fertlizers.interpolate()\n","       interpolate_2009_to_2016\n","       interpolate_2017 = fertilizer_2017.interpolate()\n","       print(\"interpolated 2017 fertilizers \\n\",interpolate_2017)\n","       # TODO: Don't just interpolate. Use the means of the previous 2  years\n","       # interpolate_pesticides_2017 = pesticides_2017.interpolate()\n","       # print(\"interpolated 2017 pesticides \\n\",interpolate_pesticides_2017)\n","\n","       # %<br>\n","       #  Interpolate missing fertlizers data<br>\n","       #  Replace na with mean values of 2004,2005,2009,2010\n","       # ##  Replace 2007 nan values with the mean of 2004 and 2005\n","\n","       # In[11]:\n","\n","       interpolate_2004_2005 = Australia_Fertilizers_Products_Agricultural[['Y2004','Y2005']].copy().interpolate()\n","       index_names = interpolate_2004_2005.index\n","       index_names\n","       interpolate_2004_2005=interpolate_2004_2005.drop([602,607], axis=0)\n","       interpolate_2004_2005['mean'] = interpolate_2004_2005.mean(axis=1)\n","       print(\"Mean of 2004 and 2005 ==== \\n\")\n","       print(interpolate_2004_2005)\n","       pesticides_2015_2016 = Australia_Pesticides[['Y2015','Y2016']]\n","       pesticides_2015_2016['mean'] = pesticides_2015_2016.mean(axis=1)\n","       print(\"Mean of 2015 and 2016 === \\n\")\n","       print(pesticides_2015_2016)\n","\n","       # Use this for 2007\n","\n","       # In[ ]:\n","       # ##  Replace 2008 nan values with the mean of 2009 and 2010\n","\n","       # In[12]:\n","       interpolate_2009_2010 = Australia_Fertilizers_Products_Agricultural[['Y2009','Y2010']].copy().interpolate()\n","       index_names = interpolate_2009_2010.index\n","       index_names\n","\n","       # In[13]:\n","       interpolate_2009_2010['mean'] = interpolate_2009_2010.mean(axis=1)\n","       print(\"Mean of 2009 and 2010 ==== \\n\")\n","       # Use this for 2008\n","       interpolate_2009_2010=interpolate_2009_2010.drop([602,607], axis=0)\n","       print(interpolate_2009_2010)\n","\n","\n","       # ## Populate 2007 nan values with computed mean\n","       # In[14]:\n","       # Fertilizers\n","       i=0\n","       for i in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:i,1:2] = interpolate_2004_2005['mean'].iloc[0:i]\n","\n","       #Pesticides\n","       j=0\n","       for j in range(1,7):\n","              pesticides_2017.iloc[0:j,] = pesticides_2015_2016['mean'].iloc[0:j]\n","\n","       # ## Populate 2008 nan values with computed mean\n","       # In[15]:\n","       j=0\n","       for j in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:j,2:3] = interpolate_2009_2010['mean'].iloc[0:j]\n","                \n","       interpolate_2009_to_2016.iloc[15, 1] = 1437658.50\n","       print(interpolate_2009_to_2016.iloc[15, 1])\n","       interpolate_2009_to_2016.iloc[15, 2] = 171351.00\n","\n","       # In[16]:\n","       X_ten_years_fertilizers_interpolated = interpolate_2009_to_2016\n","       X_ten_years_fertilizers_interpolated\n","       fertilizer_2017_interpolated = interpolate_2017\n","       print(fertilizer_2017_interpolated)\n","\n","       # #### TODO: Combine pesticdes and fertilizers data\n","       # In[17]:\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_fertilizers_interpolated)\n","\n","       pesticides_without_Item = X_ten_years_pesticides.copy().drop(['Item'], axis=1)\n","       pesticides_2017_wo_item = pesticides_2017\n","       fertilizers_without_Item = X_ten_years_fertilizers_interpolated.copy().drop(['Item'], axis=1)\n","       fertilizer_2017_wo_item = fertilizer_2017_interpolated\n","       # torch.tensor(X_ten_years_pesticides.values.astype(np.float64))\n","       print(X_ten_years_pesticides.values)\n","       print(X_ten_years_fertilizers_interpolated.values)\n","       print(pesticides_2017_wo_item.values)\n","       print(fertilizer_2017_wo_item.values)\n","\n","       print(pesticides_without_Item)\n","       pesticides_tensor = torch.from_numpy(pesticides_without_Item.values)\n","       print(\"\\t Pesticides tensor\", pesticides_tensor)\n","       pesticides_2017_tensor = torch.from_numpy(pesticides_2017_wo_item.values)\n","\n","       print(fertilizers_without_Item)\n","       fertilizers_tensor = torch.from_numpy(fertilizers_without_Item.values)\n","       fertilizer_2017_tensor = torch.from_numpy(fertilizer_2017_wo_item.values)\n","       print(\"\\t Fertilizers tensor\", fertilizers_tensor)\n","\n","       # In[18]:\n","       print(\"Train : \\n\",pesticides_tensor.shape)\n","       print(\"Train: \\n\",fertilizers_tensor.shape)\n","       print(\"Test: \\n\",pesticides_2017_tensor.shape)\n","       print(\"Test: \\n\",fertilizer_2017_tensor.shape)\n","\n","       # TODO: Normalize inputs before concatenation\n","       pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n","       print(pesticides_and_fertilizers.shape)\n","       print(\"\\n\")\n","       print(\"Training X data for 10 years: \",pesticides_and_fertilizers)\n","       print(\"\\n\")\n","       pesticides_and_fertilizers_2017 = torch.cat((pesticides_2017_tensor, fertilizer_2017_tensor), dim=0)\n","       print(pesticides_and_fertilizers_2017.shape)\n","       print(\"\\n\")\n","       print(\"Testing X data for 1 year: \",pesticides_and_fertilizers_2017)\n","       print(\"\\n\")\n","\n","       # Training + Validation data : Input\n","       X_train_ten = pesticides_and_fertilizers;\n","       X_test = pesticides_and_fertilizers_2017\n","\n","       # #### TODO: Create output vector\n","\n","       # In[19]:\n","       print(yield_frame.columns)\n","       #%%\n","       yield_frame.head\n","       #%%\n","       Australia = yield_frame['Area']==\"Australia\"\n","       Production = yield_frame['Element']==\"Production\"\n","       Australia_yield = yield_frame[Australia]\n","       Australia_yield_production = Australia_yield[Production]\n","       print(yield_frame[Australia])\n","       #%%\n","       # Get the yield data for the year 2017 only\n","       Australia_yield_2017 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015', \n","              'Y2016', 'Y2018'], axis=1)\n","       print(\"\\n\")\n","       print(\"Test Y data: \",Australia_yield_2017)\n","       print(\"\\n\")\n","       print(Australia_yield_2017.columns)\n","       print(Australia_yield_2017['Item'].unique())\n","       print(\"Number of unique crops in 2017: \",len(Australia_yield_2017['Item'].unique()))\n","\n","       Australia_yield_2007_to_2016 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017','Y2018'], axis=1)\n","       print(Australia_yield_2007_to_2016.columns)\n","       print(Australia_yield_2007_to_2016['Item'].unique())\n","       print(\"\\n\")\n","       print(\"Train Y data: \",Australia_yield_2007_to_2016)\n","       print(\"\\n\")\n","       print(\"Number of unique crops from 2007 to 2016\",len(Australia_yield_2007_to_2016['Item'].unique()))\n","\n","       # In[20]:\n","       yield_without_Item = Australia_yield_2007_to_2016.copy().drop(['Item'],axis=1)\n","       yield_without_Item_2017 = Australia_yield_2017.copy().drop(['Item'],axis=1)\n","\n","       print(yield_without_Item)\n","       yield_tensor = torch.from_numpy(yield_without_Item.values)\n","       print(\"Shape: \", yield_tensor.shape)\n","       print(\"\\t Yield tensor\", yield_tensor)\n","       print(yield_without_Item_2017)\n","       yield_tensor_2017 = torch.from_numpy(yield_without_Item_2017.values)\n","       print(\"Shape: \", yield_tensor_2017.shape)\n","       print(\"\\t Yield tensor\", yield_tensor_2017)\n","\n","       # Training + Validation: Output\n","       Y_train_ten = yield_tensor\n","       # Test Y\n","       Y_test = yield_tensor_2017\n","\n","       print(\"Train X: \\n\",X_train_ten.shape)\n","       print(\"Train Y: \\n\",Y_train_ten.shape)\n","       print(\"Test X: \\n\",X_test.shape)\n","       print(\"Test Y: \\n\",Y_test.shape)\n","       \n","       return [X_train_ten, Y_train_ten, X_test, Y_test]"]},{"cell_type":"code","execution_count":36,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["04, 5.0922e+04, 6.3416e+04],\n","        [7.2630e+03, 9.6109e+03, 8.5209e+03, 9.4395e+03, 1.0662e+04, 1.0899e+04,\n","         1.0113e+04, 1.1160e+04, 1.1399e+04, 1.4196e+04],\n","        [2.2404e+04, 2.9647e+04, 2.6284e+04, 2.9118e+04, 3.2890e+04, 3.3619e+04,\n","         3.1195e+04, 3.4426e+04, 3.5161e+04, 4.3789e+04],\n","        [2.3249e+03, 3.0765e+03, 2.7276e+03, 3.0216e+03, 3.4131e+03, 3.4887e+03,\n","         3.2371e+03, 3.5725e+03, 3.6487e+03, 4.5441e+03],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [3.0365e+05, 1.6086e+05,        nan, 1.6086e+05, 4.7329e+04, 1.5680e+05,\n","         1.3463e+05, 1.3892e+05, 1.9200e+05, 1.7258e+05],\n","        [2.0392e+04, 2.9583e+05, 7.4000e+01, 5.9159e+05, 2.6952e+05, 6.4043e+05,\n","         1.4296e+05, 3.7954e+05, 4.9362e+05, 4.8665e+05],\n","        [7.3190e+04, 6.1536e+05, 2.0840e+05, 1.0223e+06, 4.9172e+05, 1.1241e+06,\n","         1.5129e+05, 6.2017e+05, 7.9523e+05, 8.0072e+05],\n","        [1.2599e+05, 7.7861e+05, 4.1672e+05, 1.1405e+06, 7.1392e+05, 9.8775e+05,\n","         1.5962e+05, 8.6079e+05, 1.0969e+06, 1.1148e+06],\n","        [1.7879e+05, 6.4986e+05, 4.1037e+04, 1.2587e+06, 1.1699e+05, 8.5143e+05,\n","         1.6795e+05, 6.1150e+05, 7.0043e+05, 6.6312e+05],\n","        [2.3158e+05, 3.2902e+05, 2.1622e+04, 6.3643e+05, 7.9886e+04, 7.1511e+05,\n","         1.7628e+05, 3.6221e+05, 3.0400e+05, 2.1145e+05],\n","        [2.8438e+05, 8.1850e+03, 2.2080e+03, 1.4162e+04, 4.2783e+04, 5.7880e+05,\n","         1.8461e+05, 3.5574e+05, 3.5850e+05, 2.3647e+05],\n","        [3.3718e+05, 1.0243e+04, 1.1690e+03, 1.9317e+04, 2.1696e+04, 4.4248e+05,\n","         1.9294e+05, 3.4928e+05, 4.1300e+05, 2.6149e+05],\n","        [3.8998e+05, 1.2301e+04, 1.3000e+02, 2.4472e+04, 6.0900e+02, 3.0616e+05,\n","         2.0127e+05, 2.9040e+05, 3.2642e+05, 2.2314e+05],\n","        [4.4278e+05, 1.6292e+04, 2.9580e+03, 2.9627e+04, 2.2152e+05, 1.6984e+05,\n","         2.0960e+05, 2.3152e+05, 2.3984e+05, 1.8479e+05],\n","        [2.3410e+05, 2.0284e+04, 5.7860e+03, 3.4782e+04, 1.2869e+04, 3.3525e+04,\n","         3.1054e+05, 4.0976e+04, 5.9000e+04, 4.8019e+04],\n","        [2.5426e+04, 9.8828e+04, 2.1154e+04, 1.7650e+05, 5.0525e+04, 1.2845e+05,\n","         4.1147e+05, 3.3659e+05, 4.0550e+05, 3.9277e+05],\n","        [2.8115e+05, 5.2029e+05, 7.8618e+04, 9.6196e+05, 8.9395e+04, 8.9976e+05,\n","         5.1240e+05, 6.3220e+05, 7.5200e+05, 7.3752e+05],\n","        [1.1858e+06, 7.0965e+05, 4.0956e+05, 1.0097e+06, 7.8751e+05, 1.0286e+06,\n","         1.2400e+05, 1.6100e+05, 1.9800e+05, 1.4450e+05],\n","        [1.4377e+06, 8.9901e+05, 7.4050e+05, 1.0575e+06, 1.4856e+06, 1.1575e+06,\n","         1.0925e+06, 1.7532e+06, 1.7720e+06, 1.5482e+06],\n","        [1.4377e+06, 1.7135e+05, 1.1072e+05, 2.3198e+05, 1.4856e+06, 2.7388e+05,\n","         1.4371e+05, 1.2240e+05, 1.0940e+05, 1.4470e+05]], dtype=torch.float64)\n","\n","\n","torch.Size([22])\n","\n","\n","Testing X data for 1 year:  tensor([5.7169e+04, 1.2797e+04, 3.9475e+04, 4.0964e+03, 8.0076e+02, 8.0076e+02,\n","        1.4642e+05, 4.1860e+05, 6.9077e+05, 9.6295e+05, 6.2543e+05, 2.8791e+05,\n","        3.2327e+05, 3.5863e+05, 2.4948e+05, 1.4034e+05, 2.5094e+04, 3.6271e+05,\n","        7.0032e+05, 1.7607e+05, 1.6783e+06, 1.2868e+05], dtype=torch.float64)\n","\n","\n","Index(['Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element',\n","       'Unit', 'Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967',\n","       'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', 'Y1974', 'Y1975',\n","       'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983',\n","       'Y1984', 'Y1985', 'Y1986', 'Y1987', 'Y1988', 'Y1989', 'Y1990', 'Y1991',\n","       'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999',\n","       'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007',\n","       'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015',\n","       'Y2016', 'Y2017', 'Y2018'],\n","      dtype='object')\n","/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:96: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # ## Handle missing values from 2009 to 2013\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_single_block(indexer, value, name)\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value, self.name)\n","/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:203: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n","      Area Code       Area  Item Code                              Item  \\\n","1317         10  Australia        221               Almonds, with shell   \n","1318         10  Australia        221               Almonds, with shell   \n","1319         10  Australia        221               Almonds, with shell   \n","1320         10  Australia        711  Anise, badian, fennel, coriander   \n","1321         10  Australia        711  Anise, badian, fennel, coriander   \n","...         ...        ...        ...                               ...   \n","1606         10  Australia       1729                   Treenuts, Total   \n","1607         10  Australia       1729                   Treenuts, Total   \n","1608         10  Australia       1735                Vegetables Primary   \n","1609         10  Australia       1735                Vegetables Primary   \n","1610         10  Australia       1735                Vegetables Primary   \n","\n","      Element Code         Element    Unit     Y1961     Y1962     Y1963  ...  \\\n","1317          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1318          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","1319          5510      Production  tonnes       NaN       NaN       NaN  ...   \n","1320          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1321          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","...            ...             ...     ...       ...       ...       ...  ...   \n","1606          5419           Yield   hg/ha   17925.0   15042.0   15856.0  ...   \n","1607          5510      Production  tonnes     190.0     179.0     176.0  ...   \n","1608          5312  Area harvested      ha   63571.0   63397.0   65291.0  ...   \n","1609          5419           Yield   hg/ha  100846.0  103588.0  101085.0  ...   \n","1610          5510      Production  tonnes  641089.0  656717.0  659995.0  ...   \n","\n","          Y2009      Y2010      Y2011      Y2012      Y2013      Y2014  \\\n","1317    27981.0    29340.0    30390.0    28472.0    28586.0    28967.0   \n","1318     6775.0    30675.0    11377.0    10925.0    19863.0    19325.0   \n","1319    18957.0    90000.0    34576.0    31105.0    56779.0    55978.0   \n","1320      710.0      684.0      914.0     1015.0     1040.0     1000.0   \n","1321    13901.0    11853.0    11370.0    11488.0    11550.0    11570.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1606    12376.0    24996.0    12309.0    11661.0    17257.0    17692.0   \n","1607    60694.0   130580.0    70926.0    65324.0    92450.0    94930.0   \n","1608    66234.0    69695.0    74100.0    71471.0    66887.0    69262.0   \n","1609   270772.0   259637.0   224916.0   253910.0   277885.0   238561.0   \n","1610  1793425.0  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0   \n","\n","          Y2015      Y2016      Y2017      Y2018  \n","1317    31115.0    37000.0    38000.0    36940.0  \n","1318    20354.0    19703.0    19835.0    18917.0  \n","1319    63331.0    72902.0    75373.0    69880.0  \n","1320     1011.0     1108.0     1130.0     1074.0  \n","1321    11600.0    11673.0    11721.0    11769.0  \n","...         ...        ...        ...        ...  \n","1606    16017.0    17399.0    17430.0    16445.0  \n","1607   106374.0   118977.0   125683.0   118104.0  \n","1608    66917.0    68090.0    71918.0    70457.0  \n","1609   264193.0   257780.0   245961.0   252789.0  \n","1610  1767902.0  1755227.0  1768915.0  1781067.0  \n","\n","[294 rows x 65 columns]\n","\n","\n","Test Y data:                                    Item      Y2017\n","1319               Almonds, with shell    75373.0\n","1322  Anise, badian, fennel, coriander     1324.0\n","1325                            Apples   313730.0\n","1328                          Apricots     5351.0\n","1331                         Asparagus     7472.0\n","...                                ...        ...\n","1598          Oilcrops, Oil Equivalent  1888936.0\n","1601                     Pulses, Total  4129481.0\n","1604           Roots and Tubers, Total  1176669.0\n","1607                   Treenuts, Total   125683.0\n","1610                Vegetables Primary  1768915.0\n","\n","[101 rows x 2 columns]\n","\n","\n","Index(['Item', 'Y2017'], dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","Number of unique crops in 2017:  101\n","Index(['Item', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013',\n","       'Y2014', 'Y2015', 'Y2016'],\n","      dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","\n","\n","Train Y data:                                    Item      Y2007      Y2008      Y2009  \\\n","1319               Almonds, with shell    18024.0    65000.0    18957.0   \n","1322  Anise, badian, fennel, coriander       51.0     2091.0      987.0   \n","1325                            Apples   377980.0   265481.0   295134.0   \n","1328                          Apricots    17327.0    17000.0    13673.0   \n","1331                         Asparagus     5609.0     9779.0     6981.0   \n","...                                ...        ...        ...        ...   \n","1598          Oilcrops, Oil Equivalent   311986.0   553601.0   836928.0   \n","1601                     Pulses, Total  1168635.0  1681825.0  1817218.0   \n","1604           Roots and Tubers, Total  1261119.0  1447602.0  1220994.0   \n","1607                   Treenuts, Total    60924.0   103690.0    60694.0   \n","1610                Vegetables Primary  1746974.0  1703422.0  1793425.0   \n","\n","          Y2010      Y2011      Y2012      Y2013      Y2014      Y2015  \\\n","1319    90000.0    34576.0    31105.0    56779.0    55978.0    63331.0   \n","1322      810.0     1039.0     1166.0     1201.0     1157.0     1173.0   \n","1325   264401.0   299778.0   289064.0   288878.0   266771.0   295196.0   \n","1328    13175.0    13283.0    12186.0    11551.0     9238.0     9674.0   \n","1331     8835.0    10276.0     9589.0     8396.0     8375.0     8288.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   869470.0  1152688.0  1647064.0  1869619.0  1709984.0  1503124.0   \n","1601  2143709.0  2551800.0  2633541.0  2224400.0  2247300.0  1989200.0   \n","1604  1327922.0  1185085.0  1350721.0  1341112.0  1246546.0  1215195.0   \n","1607   130580.0    70926.0    65324.0    92450.0    94930.0   106374.0   \n","1610  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0  1767902.0   \n","\n","          Y2016  \n","1319    72902.0  \n","1322     1293.0  \n","1325   308298.0  \n","1328     8700.0  \n","1331     7737.0  \n","...         ...  \n","1598  1239283.0  \n","1601  2421811.0  \n","1604  1200748.0  \n","1607   118977.0  \n","1610  1755227.0  \n","\n","[101 rows x 11 columns]\n","\n","\n","Number of unique crops from 2007 to 2016 101\n","          Y2007      Y2008      Y2009      Y2010      Y2011      Y2012  \\\n","1319    18024.0    65000.0    18957.0    90000.0    34576.0    31105.0   \n","1322       51.0     2091.0      987.0      810.0     1039.0     1166.0   \n","1325   377980.0   265481.0   295134.0   264401.0   299778.0   289064.0   \n","1328    17327.0    17000.0    13673.0    13175.0    13283.0    12186.0   \n","1331     5609.0     9779.0     6981.0     8835.0    10276.0     9589.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   311986.0   553601.0   836928.0   869470.0  1152688.0  1647064.0   \n","1601  1168635.0  1681825.0  1817218.0  2143709.0  2551800.0  2633541.0   \n","1604  1261119.0  1447602.0  1220994.0  1327922.0  1185085.0  1350721.0   \n","1607    60924.0   103690.0    60694.0   130580.0    70926.0    65324.0   \n","1610  1746974.0  1703422.0  1793425.0  1809529.0  1666625.0  1814715.0   \n","\n","          Y2013      Y2014      Y2015      Y2016  \n","1319    56779.0    55978.0    63331.0    72902.0  \n","1322     1201.0     1157.0     1173.0     1293.0  \n","1325   288878.0   266771.0   295196.0   308298.0  \n","1328    11551.0     9238.0     9674.0     8700.0  \n","1331     8396.0     8375.0     8288.0     7737.0  \n","...         ...        ...        ...        ...  \n","1598  1869619.0  1709984.0  1503124.0  1239283.0  \n","1601  2224400.0  2247300.0  1989200.0  2421811.0  \n","1604  1341112.0  1246546.0  1215195.0  1200748.0  \n","1607    92450.0    94930.0   106374.0   118977.0  \n","1610  1858687.0  1652335.0  1767902.0  1755227.0  \n","\n","[101 rows x 10 columns]\n","Shape:  torch.Size([101, 10])\n","\t Yield tensor tensor([[1.8024e+04, 6.5000e+04, 1.8957e+04,  ..., 5.5978e+04, 6.3331e+04,\n","         7.2902e+04],\n","        [5.1000e+01, 2.0910e+03, 9.8700e+02,  ..., 1.1570e+03, 1.1730e+03,\n","         1.2930e+03],\n","        [3.7798e+05, 2.6548e+05, 2.9513e+05,  ..., 2.6677e+05, 2.9520e+05,\n","         3.0830e+05],\n","        ...,\n","        [1.2611e+06, 1.4476e+06, 1.2210e+06,  ..., 1.2465e+06, 1.2152e+06,\n","         1.2007e+06],\n","        [6.0924e+04, 1.0369e+05, 6.0694e+04,  ..., 9.4930e+04, 1.0637e+05,\n","         1.1898e+05],\n","        [1.7470e+06, 1.7034e+06, 1.7934e+06,  ..., 1.6523e+06, 1.7679e+06,\n","         1.7552e+06]], dtype=torch.float64)\n","          Y2017\n","1319    75373.0\n","1322     1324.0\n","1325   313730.0\n","1328     5351.0\n","1331     7472.0\n","...         ...\n","1598  1888936.0\n","1601  4129481.0\n","1604  1176669.0\n","1607   125683.0\n","1610  1768915.0\n","\n","[101 rows x 1 columns]\n","Shape:  torch.Size([101, 1])\n","\t Yield tensor tensor([[7.5373e+04],\n","        [1.3240e+03],\n","        [3.1373e+05],\n","        [5.3510e+03],\n","        [7.4720e+03],\n","        [5.6501e+04],\n","        [4.1297e+05],\n","        [1.3506e+07],\n","        [3.5396e+04],\n","        [4.1373e+04],\n","        [4.4250e+03],\n","        [4.7570e+03],\n","        [3.7604e+05],\n","        [9.9102e+04],\n","        [4.8680e+03],\n","        [2.8382e+05],\n","        [1.4066e+05],\n","        [1.1532e+04],\n","        [2.0040e+06],\n","        [3.8579e+04],\n","        [7.7449e+05],\n","        [1.2600e+06],\n","        [       nan],\n","        [1.7609e+04],\n","        [5.7900e+02],\n","        [       nan],\n","        [7.8000e+01],\n","        [       nan],\n","        [1.5270e+03],\n","        [       nan],\n","        [7.5400e+03],\n","        [6.9880e+03],\n","        [1.8244e+06],\n","        [1.6529e+04],\n","        [7.4900e+02],\n","        [2.3740e+03],\n","        [3.7227e+04],\n","        [2.1860e+05],\n","        [1.4626e+05],\n","        [6.0000e+03],\n","        [1.0314e+06],\n","        [4.3619e+05],\n","        [1.0266e+05],\n","        [4.3748e+04],\n","        [2.6353e+05],\n","        [3.5855e+04],\n","        [4.6326e+04],\n","        [       nan],\n","        [4.6165e+04],\n","        [2.2655e+06],\n","        [1.2257e+05],\n","        [2.6324e+05],\n","        [3.3232e+05],\n","        [6.3850e+03],\n","        [8.2659e+04],\n","        [9.6741e+04],\n","        [4.1519e+05],\n","        [2.0574e+04],\n","        [7.1500e+02],\n","        [8.5922e+04],\n","        [1.4950e+03],\n","        [1.7561e+04],\n","        [1.1052e+06],\n","        [3.6880e+04],\n","        [1.0150e+05],\n","        [       nan],\n","        [4.3132e+06],\n","        [6.9700e+02],\n","        [8.0730e+05],\n","        [3.0000e+04],\n","        [4.9570e+03],\n","        [2.1510e+06],\n","        [9.9400e+05],\n","        [3.1000e+04],\n","        [1.0719e+04],\n","        [4.5251e+04],\n","        [3.6561e+07],\n","        [1.7000e+04],\n","        [7.1475e+04],\n","        [1.2727e+05],\n","        [2.3540e+03],\n","        [3.7158e+05],\n","        [1.4995e+05],\n","        [7.7444e+04],\n","        [1.1955e+04],\n","        [2.6500e+03],\n","        [1.8088e+05],\n","        [3.1819e+07],\n","        [4.9780e+07],\n","        [5.0048e+07],\n","        [5.0533e+05],\n","        [1.7422e+07],\n","        [7.7449e+05],\n","        [3.9737e+06],\n","        [5.7713e+06],\n","        [3.2762e+06],\n","        [1.8889e+06],\n","        [4.1295e+06],\n","        [1.1767e+06],\n","        [1.2568e+05],\n","        [1.7689e+06]], dtype=torch.float64)\n","Train X: \n"," torch.Size([22, 10])\n","Train Y: \n"," torch.Size([101, 10])\n","Test X: \n"," torch.Size([22])\n","Test Y: \n"," torch.Size([101, 1])\n"]}],"source":["\n","training_input = load_data()[0]\n","training_ouput = load_data()[1]\n","test_input = load_data()[2]\n","test_output = load_data()[3]\n","\n","x = training_input\n","y = training_ouput"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["\n","# Transforms\n","\n","# Training data\n","x_tensor = torch.Tensor(x.float())\n","x_data = (x - x.mean())/(x.max() - x.min())\n","test = torch.Tensor([[0,4,5]])\n","x_tensor_normalized = x.normal_()\n","y_data = (y.normal_())\n","\n","transformed_x = torch.reshape(x_tensor_normalized,(1,220))\n","transformed_y = y_data\n","y = torch.reshape(y, (1,1010))\n",""]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["# Test data\n","transformed_test_x = torch.Tensor(test_input.float())\n","transformed_test_x = transformed_test_x.normal_()\n","transformed_test_y = torch.Tensor(test_output.float())\n","transformed_test_y = transformed_test_y.normal_()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# Define model\n","class NeuralNet(nn.Module):\n","       def __init__(self, D_in, H1, H2, H3, H4, H5, H6, D_out):\n","              super(NeuralNet, self).__init__()\n","              self.linear1 = nn.Linear(D_in, H1)\n","              self.linear2 = nn.Linear(H1, H2)\n","              self.linear3 = nn.Linear(H2, H3)\n","              self.linear4 = nn.Linear(H3, H4)\n","              self.linear5 = nn.Linear(H4, H5)\n","              self.linear6 = nn.Linear(H5, H6)\n","              self.linear7 = nn.Linear(H6, D_out)\n","              self.relu = nn.ReLU()\n","       def forward(self, x):\n","              y_pred = torch.tanh(self.linear1(x))\n","              y_pred = self.linear2(y_pred)\n","              y_pred = torch.tanh(self.linear3(y_pred))\n","              y_pred = self.linear4(y_pred)\n","              y_pred = torch.tanh(self.linear5(y_pred))\n","              y_pred = self.linear6(y_pred)\n","              y_pred = torch.sigmoid(self.linear7(y_pred))\n","              return y_pred\n",""]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["\n","\n","from functools import partial\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","from random import randint\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n",""]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["\n","\n","def get_accuracy(model, predicted, actual, threshold_percentage):\n","    num_items = len(actual)\n","    X = predicted.view(num_items)\n","    Y = actual.view(num_items)\n","    num_correct = torch.sum(torch.abs(X-Y)<torch.abs(threshold_percentage*Y))\n","    accuracy = (num_correct.item()*100.0/num_items)\n","    return (num_correct, accuracy)\n","    \n","\n","def random_split_training(trainset_x, trainset_y):\n","       # TODO:\n","       # Select 2 random numbers between 1 and 10\n","       # Use them to index into x and y.\n","       # That will give us the training and validation set\n","       # Rest of the indices are thus our training set\n","       index1 = randint(0,9)\n","       index2 = randint(0,9)\n","       indices=[]\n","       i=0\n","       for i in range(0,10):\n","              if(i not in [index1,index2]):\n","                 indices.append(i)    \n","       \n","       indices = torch.tensor(indices)\n","       val_indices = torch.tensor([index1, index2])\n","       t_subset_x = torch.index_select(trainset_x, 1, indices)\n","       t_subset_y = torch.index_select(trainset_y, 1, indices)\n","       v_subset_x = torch.index_select(trainset_x, 1,val_indices)\n","       v_subset_y = torch.index_select(trainset_y, 1, val_indices)\n","       return (t_subset_x, t_subset_y, v_subset_x, v_subset_y)\n","# \n",""]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["# Function to train and validate data. Save the best model based on validation loss\n","# \n","# params:\n","# config: hyperparameter search space\n","# \n","def train_crop_yield(config):\n","       net = NeuralNet(176, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],8)\n","       criterion = torch.nn.MSELoss()\n","       optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","       # train_set = torch.cat((transformed_x, transformed_y), dim=0)\n","       # test_set = torch.cat((test_input, test_output), dim=0)\n","       \n","       x_by_years = transformed_x.reshape(22,10)\n","       y_by_years = transformed_y.reshape(101,10)\n","       \n","       train_subset_x, train_subset_y, val_subset_x, val_subset_y = random_split_training(x_by_years, y_by_years)\n","       \n","       for epoch in range(500):\n","              running_loss = 0.0\n","              epoch_steps = 0\n","              # Zero the accumulated gradients\n","              optimizer.zero_grad()\n","              # forward + backward + optimize\n","              output = net(train_subset_x.float().reshape(1,176))\n","              loss = criterion(output, train_subset_y.float())\n","              loss.backward()\n","              optimizer.step()\n","              # print statistics\n","              running_loss += loss.item()\n","              epoch_steps += 1\n","              if epoch%5==4:\n","                     print(\"[%d] loss: %.3f\"%(epoch+1, running_loss/epoch_steps))\n","                     running_loss=0.0\n","       \n","       # Validation loss\n","       net = NeuralNet(44, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],2)\n","       val_loss = 0.0\n","       val_steps = 0\n","       total = 0\n","       correct = 0\n","       with torch.no_grad():\n","              val_output = net(val_subset_x.float().reshape(1,44))\n","              total = val_subset_y.size(0)\n","              correct = torch.sum(torch.abs(val_output-val_subset_y)<torch.abs(0.10*val_subset_y))\n","              loss = criterion(val_output, val_subset_y.float())\n","              val_loss += loss.cpu().numpy()\n","              torch.save((net.state_dict(), optimizer.state_dict()), './trained_net.dat')\n","              tune.report(loss=(val_loss), accuracy=(correct/total))\n","       print(\"\\n Finished Training \\n\")\n","              "]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["2021-05-16 17:50:47,092\tWARNING experiment.py:294 -- No name detected on trainable. Using DEFAULT.\n","2021-05-16 17:50:47,097\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47<br>Number of trials: 10/10 (9 PENDING, 1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_9db58_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0139584</td></tr>\n<tr><td>DEFAULT_9db58_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0863757</td></tr>\n<tr><td>DEFAULT_9db58_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0285607</td></tr>\n<tr><td>DEFAULT_9db58_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0213535</td></tr>\n<tr><td>DEFAULT_9db58_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0221542</td></tr>\n<tr><td>DEFAULT_9db58_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0478909</td></tr>\n<tr><td>DEFAULT_9db58_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0777031</td></tr>\n<tr><td>DEFAULT_9db58_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0190126</td></tr>\n<tr><td>DEFAULT_9db58_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0223487</td></tr>\n<tr><td>DEFAULT_9db58_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0579518</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=80050)\u001b[0m 2021-05-16 17:50:51,218\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80050)\u001b[0m \n","2021-05-16 17:50:51,394\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00000: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80050, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80050, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_9db58_00000:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47<br>Number of trials: 10/10 (1 ERROR, 5 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_9db58_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0863757</td></tr>\n<tr><td>DEFAULT_9db58_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0285607</td></tr>\n<tr><td>DEFAULT_9db58_00003</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0213535</td></tr>\n<tr><td>DEFAULT_9db58_00004</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0221542</td></tr>\n<tr><td>DEFAULT_9db58_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0478909</td></tr>\n<tr><td>DEFAULT_9db58_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0777031</td></tr>\n<tr><td>DEFAULT_9db58_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0190126</td></tr>\n<tr><td>DEFAULT_9db58_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0223487</td></tr>\n<tr><td>DEFAULT_9db58_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0579518</td></tr>\n<tr><td>DEFAULT_9db58_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0139584</td></tr>\n</tbody>\n</table><br>Number of errored trials: 1<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                        </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_9db58_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00000_0_H1=256,H2=32,H3=256,H4=2048,H5=32,H6=128,lr=0.013958_2021-05-16_17-50-47/error.txt</td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=80059)\u001b[0m 2021-05-16 17:50:55,025\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80059)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m 2021-05-16 17:50:55,025\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80061)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m 2021-05-16 17:50:55,025\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80060)\u001b[0m \n","2021-05-16 17:50:55,200\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00003: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80061, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80061, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:50:55,310\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00002: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80059, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80059, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:50:55,342\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00001: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80060, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80060, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_9db58_00003:\n","  {}\n","  \n","Result for DEFAULT_9db58_00002:\n","  {}\n","  \n","Result for DEFAULT_9db58_00001:\n","  {}\n","  \n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m 2021-05-16 17:50:57,444\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80069)\u001b[0m \n","2021-05-16 17:50:57,643\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00004: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80069, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80069, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_9db58_00004:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47<br>Number of trials: 10/10 (5 ERROR, 2 PENDING, 3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_9db58_00005</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0478909</td></tr>\n<tr><td>DEFAULT_9db58_00006</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0777031</td></tr>\n<tr><td>DEFAULT_9db58_00007</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0190126</td></tr>\n<tr><td>DEFAULT_9db58_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0223487</td></tr>\n<tr><td>DEFAULT_9db58_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0579518</td></tr>\n<tr><td>DEFAULT_9db58_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0139584</td></tr>\n<tr><td>DEFAULT_9db58_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0863757</td></tr>\n<tr><td>DEFAULT_9db58_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0285607</td></tr>\n<tr><td>DEFAULT_9db58_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0213535</td></tr>\n<tr><td>DEFAULT_9db58_00004</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0221542</td></tr>\n</tbody>\n</table><br>Number of errored trials: 5<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                        </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_9db58_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00000_0_H1=256,H2=32,H3=256,H4=2048,H5=32,H6=128,lr=0.013958_2021-05-16_17-50-47/error.txt</td></tr>\n<tr><td>DEFAULT_9db58_00001</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00001_1_H1=256,H2=32,H3=1024,H4=128,H5=64,H6=256,lr=0.086376_2021-05-16_17-50-47/error.txt</td></tr>\n<tr><td>DEFAULT_9db58_00002</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00002_2_H1=256,H2=64,H3=128,H4=512,H5=64,H6=256,lr=0.028561_2021-05-16_17-50-47/error.txt </td></tr>\n<tr><td>DEFAULT_9db58_00003</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00003_3_H1=128,H2=32,H3=1024,H4=1024,H5=32,H6=32,lr=0.021354_2021-05-16_17-50-47/error.txt</td></tr>\n<tr><td>DEFAULT_9db58_00004</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00004_4_H1=256,H2=32,H3=128,H4=2048,H5=64,H6=32,lr=0.022154_2021-05-16_17-50-47/error.txt </td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=80074)\u001b[0m 2021-05-16 17:50:58,619\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80074)\u001b[0m \n","2021-05-16 17:50:58,825\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00005: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80074, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80074, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_9db58_00005:\n","  {}\n","  \n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m 2021-05-16 17:51:03,477\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80078)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m 2021-05-16 17:51:03,513\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80079)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m 2021-05-16 17:51:03,530\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80077)\u001b[0m \n","2021-05-16 17:51:03,761\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00009: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80078, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80078, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_9db58_00009:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47<br>Number of trials: 10/10 (7 ERROR, 3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_9db58_00006</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0777031</td></tr>\n<tr><td>DEFAULT_9db58_00007</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0190126</td></tr>\n<tr><td>DEFAULT_9db58_00008</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0223487</td></tr>\n<tr><td>DEFAULT_9db58_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0139584</td></tr>\n<tr><td>DEFAULT_9db58_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0863757</td></tr>\n<tr><td>DEFAULT_9db58_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0285607</td></tr>\n<tr><td>DEFAULT_9db58_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0213535</td></tr>\n<tr><td>DEFAULT_9db58_00004</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0221542</td></tr>\n<tr><td>DEFAULT_9db58_00005</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0478909</td></tr>\n<tr><td>DEFAULT_9db58_00009</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0579518</td></tr>\n</tbody>\n</table><br>Number of errored trials: 7<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                        </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_9db58_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00000_0_H1=256,H2=32,H3=256,H4=2048,H5=32,H6=128,lr=0.013958_2021-05-16_17-50-47/error.txt</td></tr>\n<tr><td>DEFAULT_9db58_00001</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00001_1_H1=256,H2=32,H3=1024,H4=128,H5=64,H6=256,lr=0.086376_2021-05-16_17-50-47/error.txt</td></tr>\n<tr><td>DEFAULT_9db58_00002</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00002_2_H1=256,H2=64,H3=128,H4=512,H5=64,H6=256,lr=0.028561_2021-05-16_17-50-47/error.txt </td></tr>\n<tr><td>DEFAULT_9db58_00003</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00003_3_H1=128,H2=32,H3=1024,H4=1024,H5=32,H6=32,lr=0.021354_2021-05-16_17-50-47/error.txt</td></tr>\n<tr><td>DEFAULT_9db58_00004</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00004_4_H1=256,H2=32,H3=128,H4=2048,H5=64,H6=32,lr=0.022154_2021-05-16_17-50-47/error.txt </td></tr>\n<tr><td>DEFAULT_9db58_00005</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00005_5_H1=128,H2=32,H3=256,H4=256,H5=64,H6=64,lr=0.047891_2021-05-16_17-50-51/error.txt  </td></tr>\n<tr><td>DEFAULT_9db58_00009</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00009_9_H1=256,H2=32,H3=1024,H4=128,H5=32,H6=32,lr=0.057952_2021-05-16_17-50-57/error.txt </td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2021-05-16 17:51:03,798\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00006: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80079, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80079, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:51:03,820\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00008: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80077, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80077, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_9db58_00006:\n","  {}\n","  \n","Result for DEFAULT_9db58_00008:\n","  {}\n","  \n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m 2021-05-16 17:51:05,348\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m   File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80086)\u001b[0m \n","2021-05-16 17:51:05,569\tERROR trial_runner.py:732 -- Trial DEFAULT_9db58_00007: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80086, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80086, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-42-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_9db58_00007:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.9/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47<br>Number of trials: 10/10 (10 ERROR)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_9db58_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0139584</td></tr>\n<tr><td>DEFAULT_9db58_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0863757</td></tr>\n<tr><td>DEFAULT_9db58_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0285607</td></tr>\n<tr><td>DEFAULT_9db58_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0213535</td></tr>\n<tr><td>DEFAULT_9db58_00004</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0221542</td></tr>\n<tr><td>DEFAULT_9db58_00005</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0478909</td></tr>\n<tr><td>DEFAULT_9db58_00006</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0777031</td></tr>\n<tr><td>DEFAULT_9db58_00007</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0190126</td></tr>\n<tr><td>DEFAULT_9db58_00008</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0223487</td></tr>\n<tr><td>DEFAULT_9db58_00009</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0579518</td></tr>\n</tbody>\n</table><br>Number of errored trials: 10<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                         </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_9db58_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00000_0_H1=256,H2=32,H3=256,H4=2048,H5=32,H6=128,lr=0.013958_2021-05-16_17-50-47/error.txt </td></tr>\n<tr><td>DEFAULT_9db58_00001</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00001_1_H1=256,H2=32,H3=1024,H4=128,H5=64,H6=256,lr=0.086376_2021-05-16_17-50-47/error.txt </td></tr>\n<tr><td>DEFAULT_9db58_00002</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00002_2_H1=256,H2=64,H3=128,H4=512,H5=64,H6=256,lr=0.028561_2021-05-16_17-50-47/error.txt  </td></tr>\n<tr><td>DEFAULT_9db58_00003</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00003_3_H1=128,H2=32,H3=1024,H4=1024,H5=32,H6=32,lr=0.021354_2021-05-16_17-50-47/error.txt </td></tr>\n<tr><td>DEFAULT_9db58_00004</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00004_4_H1=256,H2=32,H3=128,H4=2048,H5=64,H6=32,lr=0.022154_2021-05-16_17-50-47/error.txt  </td></tr>\n<tr><td>DEFAULT_9db58_00005</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00005_5_H1=128,H2=32,H3=256,H4=256,H5=64,H6=64,lr=0.047891_2021-05-16_17-50-51/error.txt   </td></tr>\n<tr><td>DEFAULT_9db58_00006</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00006_6_H1=256,H2=32,H3=128,H4=1024,H5=64,H6=128,lr=0.077703_2021-05-16_17-50-55/error.txt </td></tr>\n<tr><td>DEFAULT_9db58_00007</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00007_7_H1=256,H2=32,H3=2048,H4=128,H5=64,H6=256,lr=0.019013_2021-05-16_17-50-55/error.txt </td></tr>\n<tr><td>DEFAULT_9db58_00008</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00008_8_H1=128,H2=32,H3=2048,H4=2048,H5=32,H6=256,lr=0.022349_2021-05-16_17-50-55/error.txt</td></tr>\n<tr><td>DEFAULT_9db58_00009</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-50-47/DEFAULT_9db58_00009_9_H1=256,H2=32,H3=1024,H4=128,H5=32,H6=32,lr=0.057952_2021-05-16_17-50-57/error.txt  </td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"error","ename":"TuneError","evalue":"('Trials did not complete', [DEFAULT_9db58_00000, DEFAULT_9db58_00001, DEFAULT_9db58_00002, DEFAULT_9db58_00003, DEFAULT_9db58_00004, DEFAULT_9db58_00005, DEFAULT_9db58_00006, DEFAULT_9db58_00007, DEFAULT_9db58_00008, DEFAULT_9db58_00009])","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m~/Masters Research Project/Code/crop-yield/src/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m        \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m        \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [DEFAULT_9db58_00000, DEFAULT_9db58_00001, DEFAULT_9db58_00002, DEFAULT_9db58_00003, DEFAULT_9db58_00004, DEFAULT_9db58_00005, DEFAULT_9db58_00006, DEFAULT_9db58_00007, DEFAULT_9db58_00008, DEFAULT_9db58_00009])"]}],"source":["# Evaluation:\n","def test_accuracy(net, device='cpu'):\n","       net.eval()\n","       with torch.no_grad():\n","              predicted = net(transformed_test_x.float().reshape(1,22))\n","              return get_accuracy(net, predicted.reshape(101,1), transformed_test_y.reshape(101,1), 0.50)\n","       \n","# Define hyperparameters\n","config = {\n","       \"H1\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H2\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H3\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H4\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H5\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H6\":tune.sample_from(lambda _: 2**np.random.randint(5,9)),\n","       \"lr\":tune.loguniform(1e-2, 1e-1)\n","}\n","\n","# Scheduler. Randomly try out combination of hyperparameters\n","scheduler = ASHAScheduler(\n","       metric=\"loss\",\n","       mode=\"min\",\n","       max_t=10,\n","       grace_period=1,\n","       reduction_factor=2\n",")\n","\n","reporter=CLIReporter(metric_columns=[\"loss\",\"accuracy\", \"training_iteration\"])\n","\n","result = tune.run(\n","       partial(train_crop_yield),\n","       config=config,\n","       num_samples=10,\n","       scheduler=scheduler\n",")\n","\n","best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","\n","print(\"Best trial config: {}\".format(best_trial.config))\n","print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n","print(\"Best trial fnal validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n","\n","best_trained_model = NeuralNet(22, best_trial.config[\"H1\"], best_trial.config[\"H2\"], best_trial.config[\"H3\"],best_trial.config[\"H4\"],best_trial.config[\"H5\"],best_trial.config[\"H6\"],101)\n","\n","device=\"cpu\"\n","best_trained_model.to(device)\n","\n","# best_checkpoint_dir = best_trial.checkpoint.value\n","# print(\" \\n\",type(best_trial.evaluated_params))\n","# model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n","# best_trained_model.load_state_dict(model_state)\n","\n","num_correct, test_acc = test_accuracy(best_trained_model, device)\n","print(\"Best trial test set accuracy: {}\".format(test_acc))\n",""]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["1.8.1\n1.2.4\n"]}],"source":["\n","\n","import torch\n","print(torch.__version__)\n","import pandas as pd\n","print(pd.__version__)\n","from torch.utils.data import DataLoader,ConcatDataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n",""]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["\n","# ## Read pesticides and fertilizer products data\n",""]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["def load_data():\n","       pesticides_frame = pd.read_csv(\"Inputs_Pesticides_Use_E_All_Data_NOFLAG.csv\", engine='python')\n","       pesticides_frame.head\n","  \n","       # Read fertilizer data in the form of products\n","       fertilizers_frame = pd.read_csv(\"Inputs_FertilizersProduct_E_All_Data_NOFLAG.csv\", engine='python')\n","       fertilizers_frame.head\n","       \n","       # Read the crop yield data\n","       yield_frame = pd.read_csv(\"Production_Crops_E_All_Data_NOFLAG.csv\",engine='python')\n","       yield_frame.head\n","\n","       # %<br>\n","       # Get the fertilizer usage in terms of agriculture\n","\n","       # In[4]:     \n","       print(\"Fertilizer products\",fertilizers_frame.columns)\n","       print(\"\\n\")\n","       print(\"Input pesticides\", pesticides_frame.columns)\n","       \n","       # Filter out the data for Australia\n","       is_Country_Australia_Products = fertilizers_frame['Area']==\"Australia\"\n","       Australia_Fertilizers_Products = fertilizers_frame[is_Country_Australia_Products]\n","       # In[5]:\n","       is_Country_Australia_Pesticides = pesticides_frame['Area']==\"Australia\"\n","       Australia_Pesticides = pesticides_frame[is_Country_Australia_Pesticides]\n","\n","       # Get the fertilizer usage for agricultural use\n","       # In[6]:\n","       is_Agricultural = Australia_Fertilizers_Products['Element']==\"Agricultural Use\"\n","       Australia_Fertilizers_Products_Agricultural = Australia_Fertilizers_Products[is_Agricultural]\n","       print(\"Fertilizers \\n\")\n","       print(Australia_Fertilizers_Products_Agricultural.columns)\n","       print(Australia_Fertilizers_Products_Agricultural['Item'])\n","       print(\"\\n\")\n","       print(\"Pesticides \\n\")\n","       print(Australia_Pesticides.columns)\n","       print(Australia_Pesticides['Item'])\n","       \n","       # %<br>\n","       # 7 years for training - 2007 to 2013<br>\n","       # 3 years fro validation - 2014 to 2016<br>\n","       # 2017 for testing\n","       # In[7]:\n","       X_ten_years_fertlizers = Australia_Fertilizers_Products_Agricultural.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = Australia_Fertilizers_Products_Agricultural['Y2017']\n","       print(\"2017 \\n\",fertilizer_2017)\n","\n","       # In[8]:\n","       X_ten_years_pesticides = Australia_Pesticides.drop(['Area Code', 'Area', 'Item Code', 'Element Code', 'Element',\n","              'Unit', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_pesticides.columns)\n","       pesticides_2017 = Australia_Pesticides['Y2017']\n","       print(\"2017 \\n\",pesticides_2017)\n","\n","       # %<br>\n","       # Drop Ammonia anyhydrous and Ammonia nitrate from 2007 to 2013. It has not been used in these years.\n","       # In[9]:\n","       X_ten_years_fertlizers = X_ten_years_fertlizers.drop([602, 607], axis=0)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = fertilizer_2017.drop([602, 607], axis=0)\n","       print(fertilizer_2017)\n","       \n","       # %<br>\n","       # 2009 to 2013\n","       # ## Handle missing values from 2009 to 2013\n","       # In[10]:\n","       interpolate_2009_to_2016 = X_ten_years_fertlizers.interpolate()\n","       interpolate_2009_to_2016\n","       interpolate_2017 = fertilizer_2017.interpolate()\n","       print(\"interpolated 2017 fertilizers \\n\",interpolate_2017)\n","       # TODO: Don't just interpolate. Use the means of the previous 2  years\n","       # interpolate_pesticides_2017 = pesticides_2017.interpolate()\n","       # print(\"interpolated 2017 pesticides \\n\",interpolate_pesticides_2017)\n","\n","       # %<br>\n","       #  Interpolate missing fertlizers data<br>\n","       #  Replace na with mean values of 2004,2005,2009,2010\n","       # ##  Replace 2007 nan values with the mean of 2004 and 2005\n","\n","       # In[11]:\n","\n","       interpolate_2004_2005 = Australia_Fertilizers_Products_Agricultural[['Y2004','Y2005']].copy().interpolate()\n","       index_names = interpolate_2004_2005.index\n","       index_names\n","       interpolate_2004_2005=interpolate_2004_2005.drop([602,607], axis=0)\n","       interpolate_2004_2005['mean'] = interpolate_2004_2005.mean(axis=1)\n","       print(\"Mean of 2004 and 2005 ==== \\n\")\n","       print(interpolate_2004_2005)\n","       pesticides_2015_2016 = Australia_Pesticides[['Y2015','Y2016']]\n","       pesticides_2015_2016['mean'] = pesticides_2015_2016.mean(axis=1)\n","       print(\"Mean of 2015 and 2016 === \\n\")\n","       print(pesticides_2015_2016)\n","\n","       # Use this for 2007\n","\n","       # In[ ]:\n","       # ##  Replace 2008 nan values with the mean of 2009 and 2010\n","\n","       # In[12]:\n","       interpolate_2009_2010 = Australia_Fertilizers_Products_Agricultural[['Y2009','Y2010']].copy().interpolate()\n","       index_names = interpolate_2009_2010.index\n","       index_names\n","\n","       # In[13]:\n","       interpolate_2009_2010['mean'] = interpolate_2009_2010.mean(axis=1)\n","       print(\"Mean of 2009 and 2010 ==== \\n\")\n","       # Use this for 2008\n","       interpolate_2009_2010=interpolate_2009_2010.drop([602,607], axis=0)\n","       print(interpolate_2009_2010)\n","\n","\n","       # ## Populate 2007 nan values with computed mean\n","       # In[14]:\n","       # Fertilizers\n","       i=0\n","       for i in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:i,1:2] = interpolate_2004_2005['mean'].iloc[0:i]\n","\n","       #Pesticides\n","       j=0\n","       for j in range(1,7):\n","              pesticides_2017.iloc[0:j,] = pesticides_2015_2016['mean'].iloc[0:j]\n","\n","       # ## Populate 2008 nan values with computed mean\n","       # In[15]:\n","       j=0\n","       for j in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:j,2:3] = interpolate_2009_2010['mean'].iloc[0:j]\n","                \n","       interpolate_2009_to_2016.iloc[15, 1] = 1437658.50\n","       print(interpolate_2009_to_2016.iloc[15, 1])\n","       interpolate_2009_to_2016.iloc[15, 2] = 171351.00\n","\n","       # In[16]:\n","       X_ten_years_fertilizers_interpolated = interpolate_2009_to_2016\n","       X_ten_years_fertilizers_interpolated\n","       fertilizer_2017_interpolated = interpolate_2017\n","       print(fertilizer_2017_interpolated)\n","\n","       # #### TODO: Combine pesticdes and fertilizers data\n","       # In[17]:\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_fertilizers_interpolated)\n","\n","       pesticides_without_Item = X_ten_years_pesticides.copy().drop(['Item'], axis=1)\n","       pesticides_2017_wo_item = pesticides_2017\n","       fertilizers_without_Item = X_ten_years_fertilizers_interpolated.copy().drop(['Item'], axis=1)\n","       fertilizer_2017_wo_item = fertilizer_2017_interpolated\n","       # torch.tensor(X_ten_years_pesticides.values.astype(np.float64))\n","       print(X_ten_years_pesticides.values)\n","       print(X_ten_years_fertilizers_interpolated.values)\n","       print(pesticides_2017_wo_item.values)\n","       print(fertilizer_2017_wo_item.values)\n","\n","       print(pesticides_without_Item)\n","       pesticides_tensor = torch.from_numpy(pesticides_without_Item.values)\n","       print(\"\\t Pesticides tensor\", pesticides_tensor)\n","       pesticides_2017_tensor = torch.from_numpy(pesticides_2017_wo_item.values)\n","\n","       print(fertilizers_without_Item)\n","       fertilizers_tensor = torch.from_numpy(fertilizers_without_Item.values)\n","       fertilizer_2017_tensor = torch.from_numpy(fertilizer_2017_wo_item.values)\n","       print(\"\\t Fertilizers tensor\", fertilizers_tensor)\n","\n","       # In[18]:\n","       print(\"Train : \\n\",pesticides_tensor.shape)\n","       print(\"Train: \\n\",fertilizers_tensor.shape)\n","       print(\"Test: \\n\",pesticides_2017_tensor.shape)\n","       print(\"Test: \\n\",fertilizer_2017_tensor.shape)\n","\n","       # TODO: Normalize inputs before concatenation\n","       pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n","       print(pesticides_and_fertilizers.shape)\n","       print(\"\\n\")\n","       print(\"Training X data for 10 years: \",pesticides_and_fertilizers)\n","       print(\"\\n\")\n","       pesticides_and_fertilizers_2017 = torch.cat((pesticides_2017_tensor, fertilizer_2017_tensor), dim=0)\n","       print(pesticides_and_fertilizers_2017.shape)\n","       print(\"\\n\")\n","       print(\"Testing X data for 1 year: \",pesticides_and_fertilizers_2017)\n","       print(\"\\n\")\n","\n","       # Training + Validation data : Input\n","       X_train_ten = pesticides_and_fertilizers;\n","       X_test = pesticides_and_fertilizers_2017\n","\n","       # #### TODO: Create output vector\n","\n","       # In[19]:\n","       print(yield_frame.columns)\n","       #%%\n","       yield_frame.head\n","       #%%\n","       Australia = yield_frame['Area']==\"Australia\"\n","       Production = yield_frame['Element']==\"Production\"\n","       Australia_yield = yield_frame[Australia]\n","       Australia_yield_production = Australia_yield[Production]\n","       print(yield_frame[Australia])\n","       #%%\n","       # Get the yield data for the year 2017 only\n","       Australia_yield_2017 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015', \n","              'Y2016', 'Y2018'], axis=1)\n","       print(\"\\n\")\n","       print(\"Test Y data: \",Australia_yield_2017)\n","       print(\"\\n\")\n","       print(Australia_yield_2017.columns)\n","       print(Australia_yield_2017['Item'].unique())\n","       print(\"Number of unique crops in 2017: \",len(Australia_yield_2017['Item'].unique()))\n","\n","       Australia_yield_2007_to_2016 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017','Y2018'], axis=1)\n","       print(Australia_yield_2007_to_2016.columns)\n","       print(Australia_yield_2007_to_2016['Item'].unique())\n","       print(\"\\n\")\n","       print(\"Train Y data: \",Australia_yield_2007_to_2016)\n","       print(\"\\n\")\n","       print(\"Number of unique crops from 2007 to 2016\",len(Australia_yield_2007_to_2016['Item'].unique()))\n","\n","       # In[20]:\n","       yield_without_Item = Australia_yield_2007_to_2016.copy().drop(['Item'],axis=1)\n","       yield_without_Item_2017 = Australia_yield_2017.copy().drop(['Item'],axis=1)\n","\n","       print(yield_without_Item)\n","       yield_tensor = torch.from_numpy(yield_without_Item.values)\n","       print(\"Shape: \", yield_tensor.shape)\n","       print(\"\\t Yield tensor\", yield_tensor)\n","       print(yield_without_Item_2017)\n","       yield_tensor_2017 = torch.from_numpy(yield_without_Item_2017.values)\n","       print(\"Shape: \", yield_tensor_2017.shape)\n","       print(\"\\t Yield tensor\", yield_tensor_2017)\n","\n","       # Training + Validation: Output\n","       Y_train_ten = yield_tensor\n","       # Test Y\n","       Y_test = yield_tensor_2017\n","\n","       print(\"Train X: \\n\",X_train_ten.shape)\n","       print(\"Train Y: \\n\",Y_train_ten.shape)\n","       print(\"Test X: \\n\",X_test.shape)\n","       print(\"Test Y: \\n\",Y_test.shape)\n","       \n","       return [X_train_ten, Y_train_ten, X_test, Y_test]"]},{"cell_type":"code","execution_count":47,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["04, 5.0922e+04, 6.3416e+04],\n","        [7.2630e+03, 9.6109e+03, 8.5209e+03, 9.4395e+03, 1.0662e+04, 1.0899e+04,\n","         1.0113e+04, 1.1160e+04, 1.1399e+04, 1.4196e+04],\n","        [2.2404e+04, 2.9647e+04, 2.6284e+04, 2.9118e+04, 3.2890e+04, 3.3619e+04,\n","         3.1195e+04, 3.4426e+04, 3.5161e+04, 4.3789e+04],\n","        [2.3249e+03, 3.0765e+03, 2.7276e+03, 3.0216e+03, 3.4131e+03, 3.4887e+03,\n","         3.2371e+03, 3.5725e+03, 3.6487e+03, 4.5441e+03],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [3.0365e+05, 1.6086e+05,        nan, 1.6086e+05, 4.7329e+04, 1.5680e+05,\n","         1.3463e+05, 1.3892e+05, 1.9200e+05, 1.7258e+05],\n","        [2.0392e+04, 2.9583e+05, 7.4000e+01, 5.9159e+05, 2.6952e+05, 6.4043e+05,\n","         1.4296e+05, 3.7954e+05, 4.9362e+05, 4.8665e+05],\n","        [7.3190e+04, 6.1536e+05, 2.0840e+05, 1.0223e+06, 4.9172e+05, 1.1241e+06,\n","         1.5129e+05, 6.2017e+05, 7.9523e+05, 8.0072e+05],\n","        [1.2599e+05, 7.7861e+05, 4.1672e+05, 1.1405e+06, 7.1392e+05, 9.8775e+05,\n","         1.5962e+05, 8.6079e+05, 1.0969e+06, 1.1148e+06],\n","        [1.7879e+05, 6.4986e+05, 4.1037e+04, 1.2587e+06, 1.1699e+05, 8.5143e+05,\n","         1.6795e+05, 6.1150e+05, 7.0043e+05, 6.6312e+05],\n","        [2.3158e+05, 3.2902e+05, 2.1622e+04, 6.3643e+05, 7.9886e+04, 7.1511e+05,\n","         1.7628e+05, 3.6221e+05, 3.0400e+05, 2.1145e+05],\n","        [2.8438e+05, 8.1850e+03, 2.2080e+03, 1.4162e+04, 4.2783e+04, 5.7880e+05,\n","         1.8461e+05, 3.5574e+05, 3.5850e+05, 2.3647e+05],\n","        [3.3718e+05, 1.0243e+04, 1.1690e+03, 1.9317e+04, 2.1696e+04, 4.4248e+05,\n","         1.9294e+05, 3.4928e+05, 4.1300e+05, 2.6149e+05],\n","        [3.8998e+05, 1.2301e+04, 1.3000e+02, 2.4472e+04, 6.0900e+02, 3.0616e+05,\n","         2.0127e+05, 2.9040e+05, 3.2642e+05, 2.2314e+05],\n","        [4.4278e+05, 1.6292e+04, 2.9580e+03, 2.9627e+04, 2.2152e+05, 1.6984e+05,\n","         2.0960e+05, 2.3152e+05, 2.3984e+05, 1.8479e+05],\n","        [2.3410e+05, 2.0284e+04, 5.7860e+03, 3.4782e+04, 1.2869e+04, 3.3525e+04,\n","         3.1054e+05, 4.0976e+04, 5.9000e+04, 4.8019e+04],\n","        [2.5426e+04, 9.8828e+04, 2.1154e+04, 1.7650e+05, 5.0525e+04, 1.2845e+05,\n","         4.1147e+05, 3.3659e+05, 4.0550e+05, 3.9277e+05],\n","        [2.8115e+05, 5.2029e+05, 7.8618e+04, 9.6196e+05, 8.9395e+04, 8.9976e+05,\n","         5.1240e+05, 6.3220e+05, 7.5200e+05, 7.3752e+05],\n","        [1.1858e+06, 7.0965e+05, 4.0956e+05, 1.0097e+06, 7.8751e+05, 1.0286e+06,\n","         1.2400e+05, 1.6100e+05, 1.9800e+05, 1.4450e+05],\n","        [1.4377e+06, 8.9901e+05, 7.4050e+05, 1.0575e+06, 1.4856e+06, 1.1575e+06,\n","         1.0925e+06, 1.7532e+06, 1.7720e+06, 1.5482e+06],\n","        [1.4377e+06, 1.7135e+05, 1.1072e+05, 2.3198e+05, 1.4856e+06, 2.7388e+05,\n","         1.4371e+05, 1.2240e+05, 1.0940e+05, 1.4470e+05]], dtype=torch.float64)\n","\n","\n","torch.Size([22])\n","\n","\n","Testing X data for 1 year:  tensor([5.7169e+04, 1.2797e+04, 3.9475e+04, 4.0964e+03, 8.0076e+02, 8.0076e+02,\n","        1.4642e+05, 4.1860e+05, 6.9077e+05, 9.6295e+05, 6.2543e+05, 2.8791e+05,\n","        3.2327e+05, 3.5863e+05, 2.4948e+05, 1.4034e+05, 2.5094e+04, 3.6271e+05,\n","        7.0032e+05, 1.7607e+05, 1.6783e+06, 1.2868e+05], dtype=torch.float64)\n","\n","\n","Index(['Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element',\n","       'Unit', 'Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967',\n","       'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', 'Y1974', 'Y1975',\n","       'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983',\n","       'Y1984', 'Y1985', 'Y1986', 'Y1987', 'Y1988', 'Y1989', 'Y1990', 'Y1991',\n","       'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999',\n","       'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007',\n","       'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015',\n","       'Y2016', 'Y2017', 'Y2018'],\n","      dtype='object')\n","/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:96: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # ## Handle missing values from 2009 to 2013\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_single_block(indexer, value, name)\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value, self.name)\n","/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:203: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n","      Area Code       Area  Item Code                              Item  \\\n","1317         10  Australia        221               Almonds, with shell   \n","1318         10  Australia        221               Almonds, with shell   \n","1319         10  Australia        221               Almonds, with shell   \n","1320         10  Australia        711  Anise, badian, fennel, coriander   \n","1321         10  Australia        711  Anise, badian, fennel, coriander   \n","...         ...        ...        ...                               ...   \n","1606         10  Australia       1729                   Treenuts, Total   \n","1607         10  Australia       1729                   Treenuts, Total   \n","1608         10  Australia       1735                Vegetables Primary   \n","1609         10  Australia       1735                Vegetables Primary   \n","1610         10  Australia       1735                Vegetables Primary   \n","\n","      Element Code         Element    Unit     Y1961     Y1962     Y1963  ...  \\\n","1317          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1318          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","1319          5510      Production  tonnes       NaN       NaN       NaN  ...   \n","1320          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1321          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","...            ...             ...     ...       ...       ...       ...  ...   \n","1606          5419           Yield   hg/ha   17925.0   15042.0   15856.0  ...   \n","1607          5510      Production  tonnes     190.0     179.0     176.0  ...   \n","1608          5312  Area harvested      ha   63571.0   63397.0   65291.0  ...   \n","1609          5419           Yield   hg/ha  100846.0  103588.0  101085.0  ...   \n","1610          5510      Production  tonnes  641089.0  656717.0  659995.0  ...   \n","\n","          Y2009      Y2010      Y2011      Y2012      Y2013      Y2014  \\\n","1317    27981.0    29340.0    30390.0    28472.0    28586.0    28967.0   \n","1318     6775.0    30675.0    11377.0    10925.0    19863.0    19325.0   \n","1319    18957.0    90000.0    34576.0    31105.0    56779.0    55978.0   \n","1320      710.0      684.0      914.0     1015.0     1040.0     1000.0   \n","1321    13901.0    11853.0    11370.0    11488.0    11550.0    11570.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1606    12376.0    24996.0    12309.0    11661.0    17257.0    17692.0   \n","1607    60694.0   130580.0    70926.0    65324.0    92450.0    94930.0   \n","1608    66234.0    69695.0    74100.0    71471.0    66887.0    69262.0   \n","1609   270772.0   259637.0   224916.0   253910.0   277885.0   238561.0   \n","1610  1793425.0  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0   \n","\n","          Y2015      Y2016      Y2017      Y2018  \n","1317    31115.0    37000.0    38000.0    36940.0  \n","1318    20354.0    19703.0    19835.0    18917.0  \n","1319    63331.0    72902.0    75373.0    69880.0  \n","1320     1011.0     1108.0     1130.0     1074.0  \n","1321    11600.0    11673.0    11721.0    11769.0  \n","...         ...        ...        ...        ...  \n","1606    16017.0    17399.0    17430.0    16445.0  \n","1607   106374.0   118977.0   125683.0   118104.0  \n","1608    66917.0    68090.0    71918.0    70457.0  \n","1609   264193.0   257780.0   245961.0   252789.0  \n","1610  1767902.0  1755227.0  1768915.0  1781067.0  \n","\n","[294 rows x 65 columns]\n","\n","\n","Test Y data:                                    Item      Y2017\n","1319               Almonds, with shell    75373.0\n","1322  Anise, badian, fennel, coriander     1324.0\n","1325                            Apples   313730.0\n","1328                          Apricots     5351.0\n","1331                         Asparagus     7472.0\n","...                                ...        ...\n","1598          Oilcrops, Oil Equivalent  1888936.0\n","1601                     Pulses, Total  4129481.0\n","1604           Roots and Tubers, Total  1176669.0\n","1607                   Treenuts, Total   125683.0\n","1610                Vegetables Primary  1768915.0\n","\n","[101 rows x 2 columns]\n","\n","\n","Index(['Item', 'Y2017'], dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","Number of unique crops in 2017:  101\n","Index(['Item', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013',\n","       'Y2014', 'Y2015', 'Y2016'],\n","      dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","\n","\n","Train Y data:                                    Item      Y2007      Y2008      Y2009  \\\n","1319               Almonds, with shell    18024.0    65000.0    18957.0   \n","1322  Anise, badian, fennel, coriander       51.0     2091.0      987.0   \n","1325                            Apples   377980.0   265481.0   295134.0   \n","1328                          Apricots    17327.0    17000.0    13673.0   \n","1331                         Asparagus     5609.0     9779.0     6981.0   \n","...                                ...        ...        ...        ...   \n","1598          Oilcrops, Oil Equivalent   311986.0   553601.0   836928.0   \n","1601                     Pulses, Total  1168635.0  1681825.0  1817218.0   \n","1604           Roots and Tubers, Total  1261119.0  1447602.0  1220994.0   \n","1607                   Treenuts, Total    60924.0   103690.0    60694.0   \n","1610                Vegetables Primary  1746974.0  1703422.0  1793425.0   \n","\n","          Y2010      Y2011      Y2012      Y2013      Y2014      Y2015  \\\n","1319    90000.0    34576.0    31105.0    56779.0    55978.0    63331.0   \n","1322      810.0     1039.0     1166.0     1201.0     1157.0     1173.0   \n","1325   264401.0   299778.0   289064.0   288878.0   266771.0   295196.0   \n","1328    13175.0    13283.0    12186.0    11551.0     9238.0     9674.0   \n","1331     8835.0    10276.0     9589.0     8396.0     8375.0     8288.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   869470.0  1152688.0  1647064.0  1869619.0  1709984.0  1503124.0   \n","1601  2143709.0  2551800.0  2633541.0  2224400.0  2247300.0  1989200.0   \n","1604  1327922.0  1185085.0  1350721.0  1341112.0  1246546.0  1215195.0   \n","1607   130580.0    70926.0    65324.0    92450.0    94930.0   106374.0   \n","1610  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0  1767902.0   \n","\n","          Y2016  \n","1319    72902.0  \n","1322     1293.0  \n","1325   308298.0  \n","1328     8700.0  \n","1331     7737.0  \n","...         ...  \n","1598  1239283.0  \n","1601  2421811.0  \n","1604  1200748.0  \n","1607   118977.0  \n","1610  1755227.0  \n","\n","[101 rows x 11 columns]\n","\n","\n","Number of unique crops from 2007 to 2016 101\n","          Y2007      Y2008      Y2009      Y2010      Y2011      Y2012  \\\n","1319    18024.0    65000.0    18957.0    90000.0    34576.0    31105.0   \n","1322       51.0     2091.0      987.0      810.0     1039.0     1166.0   \n","1325   377980.0   265481.0   295134.0   264401.0   299778.0   289064.0   \n","1328    17327.0    17000.0    13673.0    13175.0    13283.0    12186.0   \n","1331     5609.0     9779.0     6981.0     8835.0    10276.0     9589.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   311986.0   553601.0   836928.0   869470.0  1152688.0  1647064.0   \n","1601  1168635.0  1681825.0  1817218.0  2143709.0  2551800.0  2633541.0   \n","1604  1261119.0  1447602.0  1220994.0  1327922.0  1185085.0  1350721.0   \n","1607    60924.0   103690.0    60694.0   130580.0    70926.0    65324.0   \n","1610  1746974.0  1703422.0  1793425.0  1809529.0  1666625.0  1814715.0   \n","\n","          Y2013      Y2014      Y2015      Y2016  \n","1319    56779.0    55978.0    63331.0    72902.0  \n","1322     1201.0     1157.0     1173.0     1293.0  \n","1325   288878.0   266771.0   295196.0   308298.0  \n","1328    11551.0     9238.0     9674.0     8700.0  \n","1331     8396.0     8375.0     8288.0     7737.0  \n","...         ...        ...        ...        ...  \n","1598  1869619.0  1709984.0  1503124.0  1239283.0  \n","1601  2224400.0  2247300.0  1989200.0  2421811.0  \n","1604  1341112.0  1246546.0  1215195.0  1200748.0  \n","1607    92450.0    94930.0   106374.0   118977.0  \n","1610  1858687.0  1652335.0  1767902.0  1755227.0  \n","\n","[101 rows x 10 columns]\n","Shape:  torch.Size([101, 10])\n","\t Yield tensor tensor([[1.8024e+04, 6.5000e+04, 1.8957e+04,  ..., 5.5978e+04, 6.3331e+04,\n","         7.2902e+04],\n","        [5.1000e+01, 2.0910e+03, 9.8700e+02,  ..., 1.1570e+03, 1.1730e+03,\n","         1.2930e+03],\n","        [3.7798e+05, 2.6548e+05, 2.9513e+05,  ..., 2.6677e+05, 2.9520e+05,\n","         3.0830e+05],\n","        ...,\n","        [1.2611e+06, 1.4476e+06, 1.2210e+06,  ..., 1.2465e+06, 1.2152e+06,\n","         1.2007e+06],\n","        [6.0924e+04, 1.0369e+05, 6.0694e+04,  ..., 9.4930e+04, 1.0637e+05,\n","         1.1898e+05],\n","        [1.7470e+06, 1.7034e+06, 1.7934e+06,  ..., 1.6523e+06, 1.7679e+06,\n","         1.7552e+06]], dtype=torch.float64)\n","          Y2017\n","1319    75373.0\n","1322     1324.0\n","1325   313730.0\n","1328     5351.0\n","1331     7472.0\n","...         ...\n","1598  1888936.0\n","1601  4129481.0\n","1604  1176669.0\n","1607   125683.0\n","1610  1768915.0\n","\n","[101 rows x 1 columns]\n","Shape:  torch.Size([101, 1])\n","\t Yield tensor tensor([[7.5373e+04],\n","        [1.3240e+03],\n","        [3.1373e+05],\n","        [5.3510e+03],\n","        [7.4720e+03],\n","        [5.6501e+04],\n","        [4.1297e+05],\n","        [1.3506e+07],\n","        [3.5396e+04],\n","        [4.1373e+04],\n","        [4.4250e+03],\n","        [4.7570e+03],\n","        [3.7604e+05],\n","        [9.9102e+04],\n","        [4.8680e+03],\n","        [2.8382e+05],\n","        [1.4066e+05],\n","        [1.1532e+04],\n","        [2.0040e+06],\n","        [3.8579e+04],\n","        [7.7449e+05],\n","        [1.2600e+06],\n","        [       nan],\n","        [1.7609e+04],\n","        [5.7900e+02],\n","        [       nan],\n","        [7.8000e+01],\n","        [       nan],\n","        [1.5270e+03],\n","        [       nan],\n","        [7.5400e+03],\n","        [6.9880e+03],\n","        [1.8244e+06],\n","        [1.6529e+04],\n","        [7.4900e+02],\n","        [2.3740e+03],\n","        [3.7227e+04],\n","        [2.1860e+05],\n","        [1.4626e+05],\n","        [6.0000e+03],\n","        [1.0314e+06],\n","        [4.3619e+05],\n","        [1.0266e+05],\n","        [4.3748e+04],\n","        [2.6353e+05],\n","        [3.5855e+04],\n","        [4.6326e+04],\n","        [       nan],\n","        [4.6165e+04],\n","        [2.2655e+06],\n","        [1.2257e+05],\n","        [2.6324e+05],\n","        [3.3232e+05],\n","        [6.3850e+03],\n","        [8.2659e+04],\n","        [9.6741e+04],\n","        [4.1519e+05],\n","        [2.0574e+04],\n","        [7.1500e+02],\n","        [8.5922e+04],\n","        [1.4950e+03],\n","        [1.7561e+04],\n","        [1.1052e+06],\n","        [3.6880e+04],\n","        [1.0150e+05],\n","        [       nan],\n","        [4.3132e+06],\n","        [6.9700e+02],\n","        [8.0730e+05],\n","        [3.0000e+04],\n","        [4.9570e+03],\n","        [2.1510e+06],\n","        [9.9400e+05],\n","        [3.1000e+04],\n","        [1.0719e+04],\n","        [4.5251e+04],\n","        [3.6561e+07],\n","        [1.7000e+04],\n","        [7.1475e+04],\n","        [1.2727e+05],\n","        [2.3540e+03],\n","        [3.7158e+05],\n","        [1.4995e+05],\n","        [7.7444e+04],\n","        [1.1955e+04],\n","        [2.6500e+03],\n","        [1.8088e+05],\n","        [3.1819e+07],\n","        [4.9780e+07],\n","        [5.0048e+07],\n","        [5.0533e+05],\n","        [1.7422e+07],\n","        [7.7449e+05],\n","        [3.9737e+06],\n","        [5.7713e+06],\n","        [3.2762e+06],\n","        [1.8889e+06],\n","        [4.1295e+06],\n","        [1.1767e+06],\n","        [1.2568e+05],\n","        [1.7689e+06]], dtype=torch.float64)\n","Train X: \n"," torch.Size([22, 10])\n","Train Y: \n"," torch.Size([101, 10])\n","Test X: \n"," torch.Size([22])\n","Test Y: \n"," torch.Size([101, 1])\n"]}],"source":["\n","training_input = load_data()[0]\n","training_ouput = load_data()[1]\n","test_input = load_data()[2]\n","test_output = load_data()[3]\n","\n","x = training_input\n","y = training_ouput"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["\n","# Transforms\n","\n","# Training data\n","x_tensor = torch.Tensor(x.float())\n","x_data = (x - x.mean())/(x.max() - x.min())\n","test = torch.Tensor([[0,4,5]])\n","x_tensor_normalized = x.normal_()\n","y_data = (y.normal_())\n","\n","transformed_x = torch.reshape(x_tensor_normalized,(1,220))\n","transformed_y = y_data\n","y = torch.reshape(y, (1,1010))\n",""]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# Test data\n","transformed_test_x = torch.Tensor(test_input.float())\n","transformed_test_x = transformed_test_x.normal_()\n","transformed_test_y = torch.Tensor(test_output.float())\n","transformed_test_y = transformed_test_y.normal_()"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["# Define model\n","class NeuralNet(nn.Module):\n","       def __init__(self, D_in, H1, H2, H3, H4, H5, H6, D_out):\n","              super(NeuralNet, self).__init__()\n","              self.linear1 = nn.Linear(D_in, H1)\n","              self.linear2 = nn.Linear(H1, H2)\n","              self.linear3 = nn.Linear(H2, H3)\n","              self.linear4 = nn.Linear(H3, H4)\n","              self.linear5 = nn.Linear(H4, H5)\n","              self.linear6 = nn.Linear(H5, H6)\n","              self.linear7 = nn.Linear(H6, D_out)\n","              self.relu = nn.ReLU()\n","       def forward(self, x):\n","              y_pred = torch.tanh(self.linear1(x))\n","              y_pred = self.linear2(y_pred)\n","              y_pred = torch.tanh(self.linear3(y_pred))\n","              y_pred = self.linear4(y_pred)\n","              y_pred = torch.tanh(self.linear5(y_pred))\n","              y_pred = self.linear6(y_pred)\n","              y_pred = torch.sigmoid(self.linear7(y_pred))\n","              return y_pred\n",""]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["\n","\n","from functools import partial\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","from random import randint\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n",""]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["\n","\n","def get_accuracy(model, predicted, actual, threshold_percentage):\n","    num_items = len(actual)\n","    X = predicted.view(num_items)\n","    Y = actual.view(num_items)\n","    num_correct = torch.sum(torch.abs(X-Y)<torch.abs(threshold_percentage*Y))\n","    accuracy = (num_correct.item()*100.0/num_items)\n","    return (num_correct, accuracy)\n","    \n","\n","def random_split_training(trainset_x, trainset_y):\n","       # TODO:\n","       # Select 2 random numbers between 1 and 10\n","       # Use them to index into x and y.\n","       # That will give us the training and validation set\n","       # Rest of the indices are thus our training set\n","       index1 = randint(0,9)\n","       index2 = randint(0,9)\n","       indices=[]\n","       i=0\n","       for i in range(0,10):\n","              if(i not in [index1,index2]):\n","                 indices.append(i)    \n","       \n","       indices = torch.tensor(indices)\n","       val_indices = torch.tensor([index1, index2])\n","       t_subset_x = torch.index_select(trainset_x, 1, indices)\n","       t_subset_y = torch.index_select(trainset_y, 1, indices)\n","       v_subset_x = torch.index_select(trainset_x, 1,val_indices)\n","       v_subset_y = torch.index_select(trainset_y, 1, val_indices)\n","       return (t_subset_x, t_subset_y, v_subset_x, v_subset_y)\n","# \n",""]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# Function to train and validate data. Save the best model based on validation loss\n","# \n","# params:\n","# config: hyperparameter search space\n","# \n","def train_crop_yield(config):\n","       net = NeuralNet(176, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],8)\n","       criterion = torch.nn.MSELoss()\n","       optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","       # train_set = torch.cat((transformed_x, transformed_y), dim=0)\n","       # test_set = torch.cat((test_input, test_output), dim=0)\n","       \n","       x_by_years = transformed_x.reshape(22,10)\n","       y_by_years = transformed_y.reshape(101,10)\n","       \n","       train_subset_x, train_subset_y, val_subset_x, val_subset_y = random_split_training(x_by_years, y_by_years)\n","       \n","       for epoch in range(500):\n","              running_loss = 0.0\n","              epoch_steps = 0\n","              # Zero the accumulated gradients\n","              optimizer.zero_grad()\n","              # forward + backward + optimize\n","              output = net(train_subset_x.float().reshape(1,176))\n","              loss = criterion(output, train_subset_y.float())\n","              loss.backward()\n","              optimizer.step()\n","              # print statistics\n","              running_loss += loss.item()\n","              epoch_steps += 1\n","              if epoch%5==4:\n","                     print(\"[%d] loss: %.3f\"%(epoch+1, running_loss/epoch_steps))\n","                     running_loss=0.0\n","       \n","       # Validation loss\n","       net = NeuralNet(44, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],2)\n","       val_loss = 0.0\n","       val_steps = 0\n","       total = 0\n","       correct = 0\n","       with torch.no_grad():\n","              val_output = net(val_subset_x.float().reshape(1,44))\n","              total = val_subset_y.size(0)\n","              correct = torch.sum(torch.abs(val_output-val_subset_y)<torch.abs(0.10*val_subset_y))\n","              loss = criterion(val_output, val_subset_y.float())\n","              val_loss += loss.cpu().numpy()\n","              torch.save((net.state_dict(), optimizer.state_dict()), './trained_net.dat')\n","              tune.report(loss=(val_loss), accuracy=(correct/total))\n","       print(\"\\n Finished Training \\n\")\n","              "]},{"cell_type":"code","execution_count":54,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stderr","text":["2021-05-16 17:52:02,275\tWARNING experiment.py:294 -- No name detected on trainable. Using DEFAULT.\n","2021-05-16 17:52:02,291\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.9/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02<br>Number of trials: 10/10 (9 PENDING, 1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_ca897_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0323816</td></tr>\n<tr><td>DEFAULT_ca897_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0140777</td></tr>\n<tr><td>DEFAULT_ca897_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0812958</td></tr>\n<tr><td>DEFAULT_ca897_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0614726</td></tr>\n<tr><td>DEFAULT_ca897_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0456498</td></tr>\n<tr><td>DEFAULT_ca897_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0869788</td></tr>\n<tr><td>DEFAULT_ca897_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0214249</td></tr>\n<tr><td>DEFAULT_ca897_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0435258</td></tr>\n<tr><td>DEFAULT_ca897_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0206187</td></tr>\n<tr><td>DEFAULT_ca897_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0287099</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=80091)\u001b[0m 2021-05-16 17:52:04,952\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80091)\u001b[0m \n","2021-05-16 17:52:05,172\tERROR trial_runner.py:732 -- Trial DEFAULT_ca897_00000: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80091, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80091, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_ca897_00000:\n","  {}\n","  \n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m 2021-05-16 17:52:05,392\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80089)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m 2021-05-16 17:52:05,455\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80090)\u001b[0m \n","2021-05-16 17:52:05,557\tERROR trial_runner.py:732 -- Trial DEFAULT_ca897_00001: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80090, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80090, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:52:05,590\tERROR trial_runner.py:732 -- Trial DEFAULT_ca897_00002: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80089, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80089, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_ca897_00001:\n","  {}\n","  \n","Result for DEFAULT_ca897_00002:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02<br>Number of trials: 10/10 (3 ERROR, 3 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_ca897_00003</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0614726</td></tr>\n<tr><td>DEFAULT_ca897_00004</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0456498</td></tr>\n<tr><td>DEFAULT_ca897_00005</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0869788</td></tr>\n<tr><td>DEFAULT_ca897_00006</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0214249</td></tr>\n<tr><td>DEFAULT_ca897_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0435258</td></tr>\n<tr><td>DEFAULT_ca897_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0206187</td></tr>\n<tr><td>DEFAULT_ca897_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0287099</td></tr>\n<tr><td>DEFAULT_ca897_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0323816</td></tr>\n<tr><td>DEFAULT_ca897_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0140777</td></tr>\n<tr><td>DEFAULT_ca897_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0812958</td></tr>\n</tbody>\n</table><br>Number of errored trials: 3<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                         </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_ca897_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00000_0_H1=256,H2=64,H3=128,H4=512,H5=64,H6=256,lr=0.032382_2021-05-16_17-52-02/error.txt  </td></tr>\n<tr><td>DEFAULT_ca897_00001</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00001_1_H1=128,H2=32,H3=2048,H4=2048,H5=32,H6=256,lr=0.014078_2021-05-16_17-52-02/error.txt</td></tr>\n<tr><td>DEFAULT_ca897_00002</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00002_2_H1=128,H2=32,H3=512,H4=2048,H5=64,H6=256,lr=0.081296_2021-05-16_17-52-02/error.txt </td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80111, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80111, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_ca897_00006:\n","  {}\n","  \n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m 2021-05-16 17:52:11,995\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80114)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m 2021-05-16 17:52:11,999\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80117)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m 2021-05-16 17:52:12,024\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80118)\u001b[0m \n","2021-05-16 17:52:12,199\tERROR trial_runner.py:732 -- Trial DEFAULT_ca897_00005: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80114, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80114, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:52:12,236\tERROR trial_runner.py:732 -- Trial DEFAULT_ca897_00004: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80117, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80117, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:52:12,268\tERROR trial_runner.py:732 -- Trial DEFAULT_ca897_00003: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80118, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80118, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_ca897_00005:\n","  {}\n","  \n","Result for DEFAULT_ca897_00004:\n","  {}\n","  \n","Result for DEFAULT_ca897_00003:\n","  {}\n","  \n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m 2021-05-16 17:52:13,870\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80123)\u001b[0m \n","2021-05-16 17:52:14,073\tERROR trial_runner.py:732 -- Trial DEFAULT_ca897_00007: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80123, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80123, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_ca897_00007:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02<br>Number of trials: 10/10 (8 ERROR, 2 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_ca897_00008</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0206187</td></tr>\n<tr><td>DEFAULT_ca897_00009</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0287099</td></tr>\n<tr><td>DEFAULT_ca897_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0323816</td></tr>\n<tr><td>DEFAULT_ca897_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0140777</td></tr>\n<tr><td>DEFAULT_ca897_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0812958</td></tr>\n<tr><td>DEFAULT_ca897_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0614726</td></tr>\n<tr><td>DEFAULT_ca897_00004</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0456498</td></tr>\n<tr><td>DEFAULT_ca897_00005</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0869788</td></tr>\n<tr><td>DEFAULT_ca897_00006</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0214249</td></tr>\n<tr><td>DEFAULT_ca897_00007</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0435258</td></tr>\n</tbody>\n</table><br>Number of errored trials: 8<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                         </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_ca897_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00000_0_H1=256,H2=64,H3=128,H4=512,H5=64,H6=256,lr=0.032382_2021-05-16_17-52-02/error.txt  </td></tr>\n<tr><td>DEFAULT_ca897_00001</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00001_1_H1=128,H2=32,H3=2048,H4=2048,H5=32,H6=256,lr=0.014078_2021-05-16_17-52-02/error.txt</td></tr>\n<tr><td>DEFAULT_ca897_00002</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00002_2_H1=128,H2=32,H3=512,H4=2048,H5=64,H6=256,lr=0.081296_2021-05-16_17-52-02/error.txt </td></tr>\n<tr><td>DEFAULT_ca897_00003</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00003_3_H1=128,H2=64,H3=256,H4=512,H5=32,H6=128,lr=0.061473_2021-05-16_17-52-02/error.txt  </td></tr>\n<tr><td>DEFAULT_ca897_00004</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00004_4_H1=256,H2=64,H3=512,H4=256,H5=64,H6=256,lr=0.04565_2021-05-16_17-52-02/error.txt   </td></tr>\n<tr><td>DEFAULT_ca897_00005</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00005_5_H1=256,H2=32,H3=256,H4=1024,H5=32,H6=256,lr=0.086979_2021-05-16_17-52-05/error.txt </td></tr>\n<tr><td>DEFAULT_ca897_00006</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00006_6_H1=128,H2=64,H3=128,H4=512,H5=32,H6=128,lr=0.021425_2021-05-16_17-52-05/error.txt  </td></tr>\n<tr><td>DEFAULT_ca897_00007</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00007_7_H1=256,H2=32,H3=1024,H4=256,H5=32,H6=64,lr=0.043526_2021-05-16_17-52-05/error.txt  </td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=80126)\u001b[0m 2021-05-16 17:52:14,916\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80126)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m 2021-05-16 17:52:14,927\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m   File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","\u001b[2m\u001b[36m(pid=80127)\u001b[0m \n","2021-05-16 17:52:15,119\tERROR trial_runner.py:732 -- Trial DEFAULT_ca897_00008: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80126, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80126, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","2021-05-16 17:52:15,134\tERROR trial_runner.py:732 -- Trial DEFAULT_ca897_00009: Error processing event.\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80127, ip=10.201.135.226)\n","  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n","    result = self.train()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n","    result = self.step()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=80127, ip=10.201.135.226)\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/opt/anaconda3/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-53-d9d213d1f510>\", line 24, in train_crop_yield\n","RuntimeError: shape '[1, 176]' is invalid for input of size 198\n","Result for DEFAULT_ca897_00008:\n","  {}\n","  \n","Result for DEFAULT_ca897_00009:\n","  {}\n","  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02<br>Number of trials: 10/10 (10 ERROR)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_ca897_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0323816</td></tr>\n<tr><td>DEFAULT_ca897_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0140777</td></tr>\n<tr><td>DEFAULT_ca897_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0812958</td></tr>\n<tr><td>DEFAULT_ca897_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0614726</td></tr>\n<tr><td>DEFAULT_ca897_00004</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0456498</td></tr>\n<tr><td>DEFAULT_ca897_00005</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0869788</td></tr>\n<tr><td>DEFAULT_ca897_00006</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0214249</td></tr>\n<tr><td>DEFAULT_ca897_00007</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0435258</td></tr>\n<tr><td>DEFAULT_ca897_00008</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0206187</td></tr>\n<tr><td>DEFAULT_ca897_00009</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0287099</td></tr>\n</tbody>\n</table><br>Number of errored trials: 10<br><table>\n<thead>\n<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                         </th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_ca897_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00000_0_H1=256,H2=64,H3=128,H4=512,H5=64,H6=256,lr=0.032382_2021-05-16_17-52-02/error.txt  </td></tr>\n<tr><td>DEFAULT_ca897_00001</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00001_1_H1=128,H2=32,H3=2048,H4=2048,H5=32,H6=256,lr=0.014078_2021-05-16_17-52-02/error.txt</td></tr>\n<tr><td>DEFAULT_ca897_00002</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00002_2_H1=128,H2=32,H3=512,H4=2048,H5=64,H6=256,lr=0.081296_2021-05-16_17-52-02/error.txt </td></tr>\n<tr><td>DEFAULT_ca897_00003</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00003_3_H1=128,H2=64,H3=256,H4=512,H5=32,H6=128,lr=0.061473_2021-05-16_17-52-02/error.txt  </td></tr>\n<tr><td>DEFAULT_ca897_00004</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00004_4_H1=256,H2=64,H3=512,H4=256,H5=64,H6=256,lr=0.04565_2021-05-16_17-52-02/error.txt   </td></tr>\n<tr><td>DEFAULT_ca897_00005</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00005_5_H1=256,H2=32,H3=256,H4=1024,H5=32,H6=256,lr=0.086979_2021-05-16_17-52-05/error.txt </td></tr>\n<tr><td>DEFAULT_ca897_00006</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00006_6_H1=128,H2=64,H3=128,H4=512,H5=32,H6=128,lr=0.021425_2021-05-16_17-52-05/error.txt  </td></tr>\n<tr><td>DEFAULT_ca897_00007</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00007_7_H1=256,H2=32,H3=1024,H4=256,H5=32,H6=64,lr=0.043526_2021-05-16_17-52-05/error.txt  </td></tr>\n<tr><td>DEFAULT_ca897_00008</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00008_8_H1=128,H2=32,H3=256,H4=1024,H5=32,H6=256,lr=0.020619_2021-05-16_17-52-09/error.txt </td></tr>\n<tr><td>DEFAULT_ca897_00009</td><td style=\"text-align: right;\">           1</td><td>/Users/appleapple/ray_results/DEFAULT_2021-05-16_17-52-02/DEFAULT_ca897_00009_9_H1=256,H2=32,H3=1024,H4=512,H5=64,H6=256,lr=0.02871_2021-05-16_17-52-12/error.txt  </td></tr>\n</tbody>\n</table><br>"},"metadata":{}},{"output_type":"error","ename":"TuneError","evalue":"('Trials did not complete', [DEFAULT_ca897_00000, DEFAULT_ca897_00001, DEFAULT_ca897_00002, DEFAULT_ca897_00003, DEFAULT_ca897_00004, DEFAULT_ca897_00005, DEFAULT_ca897_00006, DEFAULT_ca897_00007, DEFAULT_ca897_00008, DEFAULT_ca897_00009])","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m~/Masters Research Project/Code/crop-yield/src/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m        \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m        \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [DEFAULT_ca897_00000, DEFAULT_ca897_00001, DEFAULT_ca897_00002, DEFAULT_ca897_00003, DEFAULT_ca897_00004, DEFAULT_ca897_00005, DEFAULT_ca897_00006, DEFAULT_ca897_00007, DEFAULT_ca897_00008, DEFAULT_ca897_00009])"]}],"source":["# Evaluation:\n","def test_accuracy(net, device='cpu'):\n","       net.eval()\n","       with torch.no_grad():\n","              predicted = net(transformed_test_x.float().reshape(1,22))\n","              return get_accuracy(net, predicted.reshape(101,1), transformed_test_y.reshape(101,1), 0.50)\n","       \n","# Define hyperparameters\n","config = {\n","       \"H1\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H2\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H3\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H4\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H5\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H6\":tune.sample_from(lambda _: 2**np.random.randint(5,9)),\n","       \"lr\":tune.loguniform(1e-2, 1e-1)\n","}\n","\n","# Scheduler. Randomly try out combination of hyperparameters\n","scheduler = ASHAScheduler(\n","       metric=\"loss\",\n","       mode=\"min\",\n","       max_t=10,\n","       grace_period=1,\n","       reduction_factor=2\n",")\n","\n","reporter=CLIReporter(metric_columns=[\"loss\",\"accuracy\", \"training_iteration\"])\n","\n","result = tune.run(\n","       partial(train_crop_yield),\n","       config=config,\n","       num_samples=10,\n","       scheduler=scheduler\n",")\n","\n","best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","\n","print(\"Best trial config: {}\".format(best_trial.config))\n","print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n","print(\"Best trial fnal validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n","\n","best_trained_model = NeuralNet(22, best_trial.config[\"H1\"], best_trial.config[\"H2\"], best_trial.config[\"H3\"],best_trial.config[\"H4\"],best_trial.config[\"H5\"],best_trial.config[\"H6\"],101)\n","\n","device=\"cpu\"\n","best_trained_model.to(device)\n","\n","# best_checkpoint_dir = best_trial.checkpoint.value\n","# print(\" \\n\",type(best_trial.evaluated_params))\n","# model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n","# best_trained_model.load_state_dict(model_state)\n","\n","num_correct, test_acc = test_accuracy(best_trained_model, device)\n","print(\"Best trial test set accuracy: {}\".format(test_acc))\n",""]},{"cell_type":"code","execution_count":55,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["04, 5.0922e+04, 6.3416e+04],\n","        [7.2630e+03, 9.6109e+03, 8.5209e+03, 9.4395e+03, 1.0662e+04, 1.0899e+04,\n","         1.0113e+04, 1.1160e+04, 1.1399e+04, 1.4196e+04],\n","        [2.2404e+04, 2.9647e+04, 2.6284e+04, 2.9118e+04, 3.2890e+04, 3.3619e+04,\n","         3.1195e+04, 3.4426e+04, 3.5161e+04, 4.3789e+04],\n","        [2.3249e+03, 3.0765e+03, 2.7276e+03, 3.0216e+03, 3.4131e+03, 3.4887e+03,\n","         3.2371e+03, 3.5725e+03, 3.6487e+03, 4.5441e+03],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [3.0365e+05, 1.6086e+05,        nan, 1.6086e+05, 4.7329e+04, 1.5680e+05,\n","         1.3463e+05, 1.3892e+05, 1.9200e+05, 1.7258e+05],\n","        [2.0392e+04, 2.9583e+05, 7.4000e+01, 5.9159e+05, 2.6952e+05, 6.4043e+05,\n","         1.4296e+05, 3.7954e+05, 4.9362e+05, 4.8665e+05],\n","        [7.3190e+04, 6.1536e+05, 2.0840e+05, 1.0223e+06, 4.9172e+05, 1.1241e+06,\n","         1.5129e+05, 6.2017e+05, 7.9523e+05, 8.0072e+05],\n","        [1.2599e+05, 7.7861e+05, 4.1672e+05, 1.1405e+06, 7.1392e+05, 9.8775e+05,\n","         1.5962e+05, 8.6079e+05, 1.0969e+06, 1.1148e+06],\n","        [1.7879e+05, 6.4986e+05, 4.1037e+04, 1.2587e+06, 1.1699e+05, 8.5143e+05,\n","         1.6795e+05, 6.1150e+05, 7.0043e+05, 6.6312e+05],\n","        [2.3158e+05, 3.2902e+05, 2.1622e+04, 6.3643e+05, 7.9886e+04, 7.1511e+05,\n","         1.7628e+05, 3.6221e+05, 3.0400e+05, 2.1145e+05],\n","        [2.8438e+05, 8.1850e+03, 2.2080e+03, 1.4162e+04, 4.2783e+04, 5.7880e+05,\n","         1.8461e+05, 3.5574e+05, 3.5850e+05, 2.3647e+05],\n","        [3.3718e+05, 1.0243e+04, 1.1690e+03, 1.9317e+04, 2.1696e+04, 4.4248e+05,\n","         1.9294e+05, 3.4928e+05, 4.1300e+05, 2.6149e+05],\n","        [3.8998e+05, 1.2301e+04, 1.3000e+02, 2.4472e+04, 6.0900e+02, 3.0616e+05,\n","         2.0127e+05, 2.9040e+05, 3.2642e+05, 2.2314e+05],\n","        [4.4278e+05, 1.6292e+04, 2.9580e+03, 2.9627e+04, 2.2152e+05, 1.6984e+05,\n","         2.0960e+05, 2.3152e+05, 2.3984e+05, 1.8479e+05],\n","        [2.3410e+05, 2.0284e+04, 5.7860e+03, 3.4782e+04, 1.2869e+04, 3.3525e+04,\n","         3.1054e+05, 4.0976e+04, 5.9000e+04, 4.8019e+04],\n","        [2.5426e+04, 9.8828e+04, 2.1154e+04, 1.7650e+05, 5.0525e+04, 1.2845e+05,\n","         4.1147e+05, 3.3659e+05, 4.0550e+05, 3.9277e+05],\n","        [2.8115e+05, 5.2029e+05, 7.8618e+04, 9.6196e+05, 8.9395e+04, 8.9976e+05,\n","         5.1240e+05, 6.3220e+05, 7.5200e+05, 7.3752e+05],\n","        [1.1858e+06, 7.0965e+05, 4.0956e+05, 1.0097e+06, 7.8751e+05, 1.0286e+06,\n","         1.2400e+05, 1.6100e+05, 1.9800e+05, 1.4450e+05],\n","        [1.4377e+06, 8.9901e+05, 7.4050e+05, 1.0575e+06, 1.4856e+06, 1.1575e+06,\n","         1.0925e+06, 1.7532e+06, 1.7720e+06, 1.5482e+06],\n","        [1.4377e+06, 1.7135e+05, 1.1072e+05, 2.3198e+05, 1.4856e+06, 2.7388e+05,\n","         1.4371e+05, 1.2240e+05, 1.0940e+05, 1.4470e+05]], dtype=torch.float64)\n","\n","\n","torch.Size([22])\n","\n","\n","Testing X data for 1 year:  tensor([5.7169e+04, 1.2797e+04, 3.9475e+04, 4.0964e+03, 8.0076e+02, 8.0076e+02,\n","        1.4642e+05, 4.1860e+05, 6.9077e+05, 9.6295e+05, 6.2543e+05, 2.8791e+05,\n","        3.2327e+05, 3.5863e+05, 2.4948e+05, 1.4034e+05, 2.5094e+04, 3.6271e+05,\n","        7.0032e+05, 1.7607e+05, 1.6783e+06, 1.2868e+05], dtype=torch.float64)\n","\n","\n","Index(['Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element',\n","       'Unit', 'Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967',\n","       'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', 'Y1974', 'Y1975',\n","       'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983',\n","       'Y1984', 'Y1985', 'Y1986', 'Y1987', 'Y1988', 'Y1989', 'Y1990', 'Y1991',\n","       'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999',\n","       'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007',\n","       'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015',\n","       'Y2016', 'Y2017', 'Y2018'],\n","      dtype='object')\n","      Area Code       Area  Item Code                              Item  \\\n","1317         10  Australia        221               Almonds, with shell   \n","1318         10  Australia        221               Almonds, with shell   \n","1319         10  Australia        221               Almonds, with shell   \n","1320         10  Australia        711  Anise, badian, fennel, coriander   \n","1321         10  Australia        711  Anise, badian, fennel, coriander   \n","...         ...        ...        ...                               ...   \n","1606         10  Australia       1729                   Treenuts, Total   \n","1607         10  Australia       1729                   Treenuts, Total   \n","1608         10  Australia       1735                Vegetables Primary   \n","1609         10  Australia       1735                Vegetables Primary   \n","1610         10  Australia       1735                Vegetables Primary   \n","\n","      Element Code         Element    Unit     Y1961     Y1962     Y1963  ...  \\\n","1317          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1318          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","1319          5510      Production  tonnes       NaN       NaN       NaN  ...   \n","1320          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1321          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","...            ...             ...     ...       ...       ...       ...  ...   \n","1606          5419           Yield   hg/ha   17925.0   15042.0   15856.0  ...   \n","1607          5510      Production  tonnes     190.0     179.0     176.0  ...   \n","1608          5312  Area harvested      ha   63571.0   63397.0   65291.0  ...   \n","1609          5419           Yield   hg/ha  100846.0  103588.0  101085.0  ...   \n","1610          5510      Production  tonnes  641089.0  656717.0  659995.0  ...   \n","\n","          Y2009      Y2010      Y2011      Y2012      Y2013      Y2014  \\\n","1317    27981.0    29340.0    30390.0    28472.0    28586.0    28967.0   \n","1318     6775.0    30675.0    11377.0    10925.0    19863.0    19325.0   \n","1319    18957.0    90000.0    34576.0    31105.0    56779.0    55978.0   \n","1320      710.0      684.0      914.0     1015.0     1040.0     1000.0   \n","1321    13901.0    11853.0    11370.0    11488.0    11550.0    11570.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1606    12376.0    24996.0    12309.0    11661.0    17257.0    17692.0   \n","1607    60694.0   130580.0    70926.0    65324.0    92450.0    94930.0   \n","1608    66234.0    69695.0    74100.0    71471.0    66887.0    69262.0   \n","1609   270772.0   259637.0   224916.0   253910.0   277885.0   238561.0   \n","1610  1793425.0  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0   \n","\n","          Y2015      Y2016      Y2017      Y2018  \n","1317    31115.0    37000.0    38000.0    36940.0  \n","1318    20354.0    19703.0    19835.0    18917.0  \n","1319    63331.0    72902.0    75373.0    69880.0  \n","1320     1011.0     1108.0     1130.0     1074.0  \n","1321    11600.0    11673.0    11721.0    11769.0  \n","...         ...        ...        ...        ...  \n","1606    16017.0    17399.0    17430.0    16445.0  \n","1607   106374.0   118977.0   125683.0   118104.0  \n","1608    66917.0    68090.0    71918.0    70457.0  \n","1609   264193.0   257780.0   245961.0   252789.0  \n","1610  1767902.0  1755227.0  1768915.0  1781067.0  \n","\n","[294 rows x 65 columns]\n","\n","\n","Test Y data:                                    Item      Y2017\n","1319               Almonds, with shell    75373.0\n","1322  Anise, badian, fennel, coriander     1324.0\n","1325                            Apples   313730.0\n","1328                          Apricots     5351.0\n","1331                         Asparagus     7472.0\n","...                                ...        ...\n","1598          Oilcrops, Oil Equivalent  1888936.0\n","1601                     Pulses, Total  4129481.0\n","1604           Roots and Tubers, Total  1176669.0\n","1607                   Treenuts, Total   125683.0\n","1610                Vegetables Primary  1768915.0\n","\n","[101 rows x 2 columns]\n","\n","\n","Index(['Item', 'Y2017'], dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","Number of unique crops in 2017:  101\n","Index(['Item', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013',\n","       'Y2014', 'Y2015', 'Y2016'],\n","      dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:96: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # ## Handle missing values from 2009 to 2013\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_single_block(indexer, value, name)\n","/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value, self.name)\n","/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:203: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n","\n","\n","\n","Train Y data:                                    Item      Y2007      Y2008      Y2009  \\\n","1319               Almonds, with shell    18024.0    65000.0    18957.0   \n","1322  Anise, badian, fennel, coriander       51.0     2091.0      987.0   \n","1325                            Apples   377980.0   265481.0   295134.0   \n","1328                          Apricots    17327.0    17000.0    13673.0   \n","1331                         Asparagus     5609.0     9779.0     6981.0   \n","...                                ...        ...        ...        ...   \n","1598          Oilcrops, Oil Equivalent   311986.0   553601.0   836928.0   \n","1601                     Pulses, Total  1168635.0  1681825.0  1817218.0   \n","1604           Roots and Tubers, Total  1261119.0  1447602.0  1220994.0   \n","1607                   Treenuts, Total    60924.0   103690.0    60694.0   \n","1610                Vegetables Primary  1746974.0  1703422.0  1793425.0   \n","\n","          Y2010      Y2011      Y2012      Y2013      Y2014      Y2015  \\\n","1319    90000.0    34576.0    31105.0    56779.0    55978.0    63331.0   \n","1322      810.0     1039.0     1166.0     1201.0     1157.0     1173.0   \n","1325   264401.0   299778.0   289064.0   288878.0   266771.0   295196.0   \n","1328    13175.0    13283.0    12186.0    11551.0     9238.0     9674.0   \n","1331     8835.0    10276.0     9589.0     8396.0     8375.0     8288.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   869470.0  1152688.0  1647064.0  1869619.0  1709984.0  1503124.0   \n","1601  2143709.0  2551800.0  2633541.0  2224400.0  2247300.0  1989200.0   \n","1604  1327922.0  1185085.0  1350721.0  1341112.0  1246546.0  1215195.0   \n","1607   130580.0    70926.0    65324.0    92450.0    94930.0   106374.0   \n","1610  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0  1767902.0   \n","\n","          Y2016  \n","1319    72902.0  \n","1322     1293.0  \n","1325   308298.0  \n","1328     8700.0  \n","1331     7737.0  \n","...         ...  \n","1598  1239283.0  \n","1601  2421811.0  \n","1604  1200748.0  \n","1607   118977.0  \n","1610  1755227.0  \n","\n","[101 rows x 11 columns]\n","\n","\n","Number of unique crops from 2007 to 2016 101\n","          Y2007      Y2008      Y2009      Y2010      Y2011      Y2012  \\\n","1319    18024.0    65000.0    18957.0    90000.0    34576.0    31105.0   \n","1322       51.0     2091.0      987.0      810.0     1039.0     1166.0   \n","1325   377980.0   265481.0   295134.0   264401.0   299778.0   289064.0   \n","1328    17327.0    17000.0    13673.0    13175.0    13283.0    12186.0   \n","1331     5609.0     9779.0     6981.0     8835.0    10276.0     9589.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   311986.0   553601.0   836928.0   869470.0  1152688.0  1647064.0   \n","1601  1168635.0  1681825.0  1817218.0  2143709.0  2551800.0  2633541.0   \n","1604  1261119.0  1447602.0  1220994.0  1327922.0  1185085.0  1350721.0   \n","1607    60924.0   103690.0    60694.0   130580.0    70926.0    65324.0   \n","1610  1746974.0  1703422.0  1793425.0  1809529.0  1666625.0  1814715.0   \n","\n","          Y2013      Y2014      Y2015      Y2016  \n","1319    56779.0    55978.0    63331.0    72902.0  \n","1322     1201.0     1157.0     1173.0     1293.0  \n","1325   288878.0   266771.0   295196.0   308298.0  \n","1328    11551.0     9238.0     9674.0     8700.0  \n","1331     8396.0     8375.0     8288.0     7737.0  \n","...         ...        ...        ...        ...  \n","1598  1869619.0  1709984.0  1503124.0  1239283.0  \n","1601  2224400.0  2247300.0  1989200.0  2421811.0  \n","1604  1341112.0  1246546.0  1215195.0  1200748.0  \n","1607    92450.0    94930.0   106374.0   118977.0  \n","1610  1858687.0  1652335.0  1767902.0  1755227.0  \n","\n","[101 rows x 10 columns]\n","Shape:  torch.Size([101, 10])\n","\t Yield tensor tensor([[1.8024e+04, 6.5000e+04, 1.8957e+04,  ..., 5.5978e+04, 6.3331e+04,\n","         7.2902e+04],\n","        [5.1000e+01, 2.0910e+03, 9.8700e+02,  ..., 1.1570e+03, 1.1730e+03,\n","         1.2930e+03],\n","        [3.7798e+05, 2.6548e+05, 2.9513e+05,  ..., 2.6677e+05, 2.9520e+05,\n","         3.0830e+05],\n","        ...,\n","        [1.2611e+06, 1.4476e+06, 1.2210e+06,  ..., 1.2465e+06, 1.2152e+06,\n","         1.2007e+06],\n","        [6.0924e+04, 1.0369e+05, 6.0694e+04,  ..., 9.4930e+04, 1.0637e+05,\n","         1.1898e+05],\n","        [1.7470e+06, 1.7034e+06, 1.7934e+06,  ..., 1.6523e+06, 1.7679e+06,\n","         1.7552e+06]], dtype=torch.float64)\n","          Y2017\n","1319    75373.0\n","1322     1324.0\n","1325   313730.0\n","1328     5351.0\n","1331     7472.0\n","...         ...\n","1598  1888936.0\n","1601  4129481.0\n","1604  1176669.0\n","1607   125683.0\n","1610  1768915.0\n","\n","[101 rows x 1 columns]\n","Shape:  torch.Size([101, 1])\n","\t Yield tensor tensor([[7.5373e+04],\n","        [1.3240e+03],\n","        [3.1373e+05],\n","        [5.3510e+03],\n","        [7.4720e+03],\n","        [5.6501e+04],\n","        [4.1297e+05],\n","        [1.3506e+07],\n","        [3.5396e+04],\n","        [4.1373e+04],\n","        [4.4250e+03],\n","        [4.7570e+03],\n","        [3.7604e+05],\n","        [9.9102e+04],\n","        [4.8680e+03],\n","        [2.8382e+05],\n","        [1.4066e+05],\n","        [1.1532e+04],\n","        [2.0040e+06],\n","        [3.8579e+04],\n","        [7.7449e+05],\n","        [1.2600e+06],\n","        [       nan],\n","        [1.7609e+04],\n","        [5.7900e+02],\n","        [       nan],\n","        [7.8000e+01],\n","        [       nan],\n","        [1.5270e+03],\n","        [       nan],\n","        [7.5400e+03],\n","        [6.9880e+03],\n","        [1.8244e+06],\n","        [1.6529e+04],\n","        [7.4900e+02],\n","        [2.3740e+03],\n","        [3.7227e+04],\n","        [2.1860e+05],\n","        [1.4626e+05],\n","        [6.0000e+03],\n","        [1.0314e+06],\n","        [4.3619e+05],\n","        [1.0266e+05],\n","        [4.3748e+04],\n","        [2.6353e+05],\n","        [3.5855e+04],\n","        [4.6326e+04],\n","        [       nan],\n","        [4.6165e+04],\n","        [2.2655e+06],\n","        [1.2257e+05],\n","        [2.6324e+05],\n","        [3.3232e+05],\n","        [6.3850e+03],\n","        [8.2659e+04],\n","        [9.6741e+04],\n","        [4.1519e+05],\n","        [2.0574e+04],\n","        [7.1500e+02],\n","        [8.5922e+04],\n","        [1.4950e+03],\n","        [1.7561e+04],\n","        [1.1052e+06],\n","        [3.6880e+04],\n","        [1.0150e+05],\n","        [       nan],\n","        [4.3132e+06],\n","        [6.9700e+02],\n","        [8.0730e+05],\n","        [3.0000e+04],\n","        [4.9570e+03],\n","        [2.1510e+06],\n","        [9.9400e+05],\n","        [3.1000e+04],\n","        [1.0719e+04],\n","        [4.5251e+04],\n","        [3.6561e+07],\n","        [1.7000e+04],\n","        [7.1475e+04],\n","        [1.2727e+05],\n","        [2.3540e+03],\n","        [3.7158e+05],\n","        [1.4995e+05],\n","        [7.7444e+04],\n","        [1.1955e+04],\n","        [2.6500e+03],\n","        [1.8088e+05],\n","        [3.1819e+07],\n","        [4.9780e+07],\n","        [5.0048e+07],\n","        [5.0533e+05],\n","        [1.7422e+07],\n","        [7.7449e+05],\n","        [3.9737e+06],\n","        [5.7713e+06],\n","        [3.2762e+06],\n","        [1.8889e+06],\n","        [4.1295e+06],\n","        [1.1767e+06],\n","        [1.2568e+05],\n","        [1.7689e+06]], dtype=torch.float64)\n","Train X: \n"," torch.Size([22, 10])\n","Train Y: \n"," torch.Size([101, 10])\n","Test X: \n"," torch.Size([22])\n","Test Y: \n"," torch.Size([101, 1])\n"]}],"source":["\n","training_input = load_data()[0]\n","training_ouput = load_data()[1]\n","test_input = load_data()[2]\n","test_output = load_data()[3]\n","\n","x = training_input\n","y = training_ouput"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["\n","# Transforms\n","\n","# Training data\n","x_tensor = torch.Tensor(x.float())\n","x_data = (x - x.mean())/(x.max() - x.min())\n","test = torch.Tensor([[0,4,5]])\n","x_tensor_normalized = x.normal_()\n","y_data = (y.normal_())\n","\n","transformed_x = torch.reshape(x_tensor_normalized,(1,220))\n","transformed_y = y_data\n","y = torch.reshape(y, (1,1010))\n",""]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# Test data\n","transformed_test_x = torch.Tensor(test_input.float())\n","transformed_test_x = transformed_test_x.normal_()\n","transformed_test_y = torch.Tensor(test_output.float())\n","transformed_test_y = transformed_test_y.normal_()"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["# Define model\n","class NeuralNet(nn.Module):\n","       def __init__(self, D_in, H1, H2, H3, H4, H5, H6, D_out):\n","              super(NeuralNet, self).__init__()\n","              self.linear1 = nn.Linear(D_in, H1)\n","              self.linear2 = nn.Linear(H1, H2)\n","              self.linear3 = nn.Linear(H2, H3)\n","              self.linear4 = nn.Linear(H3, H4)\n","              self.linear5 = nn.Linear(H4, H5)\n","              self.linear6 = nn.Linear(H5, H6)\n","              self.linear7 = nn.Linear(H6, D_out)\n","              self.relu = nn.ReLU()\n","       def forward(self, x):\n","              y_pred = torch.tanh(self.linear1(x))\n","              y_pred = self.linear2(y_pred)\n","              y_pred = torch.tanh(self.linear3(y_pred))\n","              y_pred = self.linear4(y_pred)\n","              y_pred = torch.tanh(self.linear5(y_pred))\n","              y_pred = self.linear6(y_pred)\n","              y_pred = torch.sigmoid(self.linear7(y_pred))\n","              return y_pred\n",""]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["\n","\n","from functools import partial\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","from random import randint\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n",""]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["\n","\n","def get_accuracy(model, predicted, actual, threshold_percentage):\n","    num_items = len(actual)\n","    X = predicted.view(num_items)\n","    Y = actual.view(num_items)\n","    num_correct = torch.sum(torch.abs(X-Y)<torch.abs(threshold_percentage*Y))\n","    accuracy = (num_correct.item()*100.0/num_items)\n","    return (num_correct, accuracy)\n","    \n","\n","def random_split_training(trainset_x, trainset_y):\n","       # TODO:\n","       # Select 2 random numbers between 1 and 10\n","       # Use them to index into x and y.\n","       # That will give us the training and validation set\n","       # Rest of the indices are thus our training set\n","       index1 = randint(0,9)\n","       index2 = randint(0,9)\n","       indices=[]\n","       i=0\n","       for i in range(0,10):\n","              if(i not in [index1,index2]):\n","                 indices.append(i)    \n","       \n","       indices = torch.tensor(indices)\n","       val_indices = torch.tensor([index1, index2])\n","       t_subset_x = torch.index_select(trainset_x, 1, indices)\n","       t_subset_y = torch.index_select(trainset_y, 1, indices)\n","       v_subset_x = torch.index_select(trainset_x, 1,val_indices)\n","       v_subset_y = torch.index_select(trainset_y, 1, val_indices)\n","       return (t_subset_x, t_subset_y, v_subset_x, v_subset_y)\n","# \n",""]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["# Function to train and validate data. Save the best model based on validation loss\n","# \n","# params:\n","# config: hyperparameter search space\n","# \n","def train_crop_yield(config):\n","       net = NeuralNet(176, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],8)\n","       criterion = torch.nn.MSELoss()\n","       optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","       # train_set = torch.cat((transformed_x, transformed_y), dim=0)\n","       # test_set = torch.cat((test_input, test_output), dim=0)\n","       \n","       x_by_years = transformed_x.reshape(22,10)\n","       y_by_years = transformed_y.reshape(101,10)\n","       \n","       train_subset_x, train_subset_y, val_subset_x, val_subset_y = random_split_training(x_by_years, y_by_years)\n","       \n","       for epoch in range(500):\n","              running_loss = 0.0\n","              epoch_steps = 0\n","              # Zero the accumulated gradients\n","              optimizer.zero_grad()\n","              # forward + backward + optimize\n","              output = net(train_subset_x.float().reshape(1,176))\n","              loss = criterion(output, train_subset_y.float())\n","              loss.backward()\n","              optimizer.step()\n","              # print statistics\n","              running_loss += loss.item()\n","              epoch_steps += 1\n","              if epoch%5==4:\n","                     print(\"[%d] loss: %.3f\"%(epoch+1, running_loss/epoch_steps))\n","                     running_loss=0.0\n","       \n","       # Validation loss\n","       net = NeuralNet(44, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],2)\n","       val_loss = 0.0\n","       val_steps = 0\n","       total = 0\n","       correct = 0\n","       with torch.no_grad():\n","              val_output = net(val_subset_x.float().reshape(1,44))\n","              total = val_subset_y.size(0)\n","              correct = torch.sum(torch.abs(val_output-val_subset_y)<torch.abs(0.10*val_subset_y))\n","              loss = criterion(val_output, val_subset_y.float())\n","              val_loss += loss.cpu().numpy()\n","              torch.save((net.state_dict(), optimizer.state_dict()), './trained_net.dat')\n","              tune.report(loss=(val_loss), accuracy=(correct/total))\n","       print(\"\\n Finished Training \\n\")\n","              "]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["2021-05-16 17:53:49,708\tWARNING experiment.py:294 -- No name detected on trainable. Using DEFAULT.\n","2021-05-16 17:53:49,725\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-53-49<br>Number of trials: 10/10 (10 PENDING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_0a9bd_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.095975 </td></tr>\n<tr><td>DEFAULT_0a9bd_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0165373</td></tr>\n<tr><td>DEFAULT_0a9bd_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0154145</td></tr>\n<tr><td>DEFAULT_0a9bd_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0793133</td></tr>\n<tr><td>DEFAULT_0a9bd_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0474008</td></tr>\n<tr><td>DEFAULT_0a9bd_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0229811</td></tr>\n<tr><td>DEFAULT_0a9bd_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0648928</td></tr>\n<tr><td>DEFAULT_0a9bd_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0108553</td></tr>\n<tr><td>DEFAULT_0a9bd_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.013946 </td></tr>\n<tr><td>DEFAULT_0a9bd_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0474737</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-53-49<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_0a9bd_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.095975 </td></tr>\n<tr><td>DEFAULT_0a9bd_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0165373</td></tr>\n<tr><td>DEFAULT_0a9bd_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0154145</td></tr>\n<tr><td>DEFAULT_0a9bd_00003</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0793133</td></tr>\n<tr><td>DEFAULT_0a9bd_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0474008</td></tr>\n<tr><td>DEFAULT_0a9bd_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0229811</td></tr>\n<tr><td>DEFAULT_0a9bd_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0648928</td></tr>\n<tr><td>DEFAULT_0a9bd_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0108553</td></tr>\n<tr><td>DEFAULT_0a9bd_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.013946 </td></tr>\n<tr><td>DEFAULT_0a9bd_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0474737</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=80156)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [5] loss: 1.250\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [10] loss: 1.242\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [5] loss: 1.263\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [10] loss: 1.251\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [15] loss: 1.230\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [20] loss: 1.216\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [25] loss: 1.200\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [15] loss: 1.235\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [5] loss: 1.218\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [10] loss: 1.108\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [15] loss: 1.018\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [20] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [25] loss: 1.013\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [30] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [5] loss: 1.227\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [10] loss: 1.143\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [15] loss: 1.049\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [20] loss: 1.015\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [25] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [30] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [30] loss: 1.180\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [35] loss: 1.156\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [40] loss: 1.126\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [20] loss: 1.214\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [25] loss: 1.187\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [35] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [35] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [45] loss: 1.093\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [40] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [50] loss: 1.062\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [30] loss: 1.153\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [40] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [45] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [55] loss: 1.039\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [60] loss: 1.025\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [35] loss: 1.113\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [45] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [50] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [40] loss: 1.075\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [50] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [65] loss: 1.018\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [55] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [55] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [70] loss: 1.014\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [45] loss: 1.047\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [60] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [50] loss: 1.030\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [75] loss: 1.013\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [60] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [80] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [85] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [55] loss: 1.022\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [65] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [70] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [65] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [70] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [60] loss: 1.017\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [90] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [95] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [75] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [75] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [65] loss: 1.015\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [80] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [80] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [85] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [100] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [105] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [110] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [70] loss: 1.013\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [75] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [80] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [90] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [85] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [115] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [90] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [120] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [85] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [95] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [100] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [95] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [100] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [125] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [130] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [135] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [140] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [90] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [95] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [105] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [110] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [105] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [110] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [115] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [100] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [105] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [115] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [145] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [110] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [120] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [150] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [120] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [155] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [125] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [125] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [160] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [130] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [130] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [115] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [135] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [135] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [165] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [170] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [175] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [180] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [120] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [125] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [140] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [145] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [140] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [145] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [185] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [130] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [150] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [150] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [155] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [190] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [135] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [155] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [195] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [140] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [160] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [165] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [160] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [200] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [205] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [145] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [170] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [165] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [170] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [210] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [150] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [175] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [175] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [215] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [155] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [220] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [225] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [230] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [160] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [180] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [185] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [180] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [185] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [165] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [190] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [190] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [235] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [240] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [170] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [195] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [195] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [175] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [200] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [200] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [245] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [180] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [205] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [250] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [255] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [185] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [205] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [210] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [210] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [260] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [265] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [270] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [190] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [215] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [220] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [225] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [215] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [195] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [275] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [220] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [200] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [230] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [280] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [225] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [285] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [290] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [205] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [210] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [235] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [240] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [245] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [230] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [235] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [295] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [215] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [300] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [240] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [245] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [220] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [250] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [305] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [255] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [250] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [310] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [315] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [225] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [230] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [260] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [255] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [235] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [265] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [320] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [325] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [330] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [240] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [270] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [260] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [265] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [275] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [270] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [335] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [340] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [245] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [250] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [280] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [285] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [290] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [275] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [280] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [345] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [350] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [255] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [355] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [285] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [360] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [260] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [265] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [295] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [300] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [290] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [365] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [370] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [295] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [270] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [305] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [310] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [300] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [375] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [275] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [305] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [380] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [385] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [390] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [280] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [285] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [315] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [320] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [310] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [315] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [395] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [325] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [320] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [400] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [290] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [405] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [330] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [335] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [325] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [295] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [330] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [410] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [300] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [340] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [335] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [415] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [420] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [305] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [345] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [340] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [425] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [310] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [350] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [355] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [345] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [430] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [350] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [435] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [315] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [320] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [360] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [355] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [440] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [445] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [365] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [360] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [450] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [455] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [325] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [330] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [335] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [370] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [375] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [365] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [370] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [460] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [380] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [465] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [340] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [385] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [375] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [470] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [475] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [345] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [390] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [395] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [380] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [400] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [385] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [480] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [485] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [350] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [390] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [355] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [395] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [490] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [495] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [360] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [405] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [410] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [415] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [400] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m [500] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [365] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [370] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [405] loss: 1.007\n","Result for DEFAULT_0a9bd_00002:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-54-02\n","  done: false\n","  experiment_id: 58a2cabf560f418ca9d9c3b36496222b\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.5205473899841309\n","  node_ip: 10.201.135.226\n","  pid: 80155\n","  time_since_restore: 3.999647855758667\n","  time_this_iter_s: 3.999647855758667\n","  time_total_s: 3.999647855758667\n","  timestamp: 1621153442\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00002\n","  \n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [375] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [420] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [425] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [410] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80155)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.5205473899841309<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-53-49<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_0a9bd_00000</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.095975 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00001</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0165373</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00002</td><td>RUNNING </td><td>10.201.135.226:80155</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0154145</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.99965</td><td style=\"text-align: right;\">1.52055</td></tr>\n<tr><td>DEFAULT_0a9bd_00003</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0793133</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00004</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0474008</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00005</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0229811</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00006</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0648928</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00007</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0108553</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00008</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.013946 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00009</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0474737</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=80153)\u001b[0m [415] loss: 1.007\n","Result for DEFAULT_0a9bd_00002:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-54-02\n","  done: true\n","  experiment_id: 58a2cabf560f418ca9d9c3b36496222b\n","  experiment_tag: 2_H1=256,H2=32,H3=128,H4=128,H5=64,H6=64,lr=0.015414\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.5205473899841309\n","  node_ip: 10.201.135.226\n","  pid: 80155\n","  time_since_restore: 3.999647855758667\n","  time_this_iter_s: 3.999647855758667\n","  time_total_s: 3.999647855758667\n","  timestamp: 1621153442\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00002\n","  \n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [380] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [385] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [430] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [435] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [420] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [425] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [390] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [440] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [445] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [430] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [395] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [435] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [450] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [440] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [400] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [455] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [405] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [410] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [460] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [445] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [450] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [415] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [465] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [455] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [470] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [420] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [475] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [460] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [465] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [480] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [425] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [430] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [485] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [470] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [490] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [475] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [435] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [440] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [495] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [480] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [485] loss: 1.007\n","Result for DEFAULT_0a9bd_00000:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_17-54-03\n","  done: true\n","  experiment_id: 70478afb719143f3a9ba522adba36b58\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.5666459798812866\n","  node_ip: 10.201.135.226\n","  pid: 80156\n","  time_since_restore: 5.057883262634277\n","  time_this_iter_s: 5.057883262634277\n","  time_total_s: 5.057883262634277\n","  timestamp: 1621153443\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00000\n","  \n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [445] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m [500] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [490] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [450] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80156)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [455] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [495] loss: 1.007Result for DEFAULT_0a9bd_00003:\n","\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m [500] loss: 1.007\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-54-04\n","  done: false\n","  experiment_id: ce3a77690d2c40a88542597deb76ed4c\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.504126787185669\n","  node_ip: 10.201.135.226\n","  pid: 80153\n","  time_since_restore: 5.3170411586761475\n","  time_this_iter_s: 5.3170411586761475\n","  time_total_s: 5.3170411586761475\n","  timestamp: 1621153444\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00003\n","  \n","Result for DEFAULT_0a9bd_00003:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-54-04\n","  done: true\n","  experiment_id: ce3a77690d2c40a88542597deb76ed4c\n","  experiment_tag: 3_H1=128,H2=32,H3=128,H4=1024,H5=32,H6=32,lr=0.079313\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.504126787185669\n","  node_ip: 10.201.135.226\n","  pid: 80153\n","  time_since_restore: 5.3170411586761475\n","  time_this_iter_s: 5.3170411586761475\n","  time_total_s: 5.3170411586761475\n","  timestamp: 1621153444\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00003\n","  \n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [460] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [465] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80153)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [470] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [475] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [480] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [485] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [490] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [495] loss: 1.008\n","Result for DEFAULT_0a9bd_00001:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-54-04\n","  done: true\n","  experiment_id: d9603ab0725a47a187ba6d23f8baa9a8\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.545193076133728\n","  node_ip: 10.201.135.226\n","  pid: 80154\n","  time_since_restore: 5.888888835906982\n","  time_this_iter_s: 5.888888835906982\n","  time_total_s: 5.888888835906982\n","  timestamp: 1621153444\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00001\n","  \n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m [500] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80154)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [5] loss: 1.248\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [10] loss: 1.239\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [15] loss: 1.227\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [20] loss: 1.211\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [5] loss: 1.246\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [10] loss: 1.202\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [15] loss: 1.138\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [20] loss: 1.054\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [25] loss: 1.193\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [30] loss: 1.171\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [25] loss: 1.015\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [30] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [5] loss: 1.240\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [35] loss: 1.143\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [40] loss: 1.111\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [35] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [40] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [45] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [45] loss: 1.078\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [50] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [50] loss: 1.050\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [55] loss: 1.031\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [55] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [60] loss: 1.021\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [65] loss: 1.016\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [70] loss: 1.014\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [60] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [65] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [75] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [70] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [75] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [10] loss: 1.216\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [5] loss: 1.205\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [80] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [80] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [85] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [85] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [90] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [90] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [95] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [95] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [15] loss: 1.178\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [100] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [105] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [100] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [105] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [110] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [110] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [115] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [115] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [120] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [120] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [125] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [125] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [130] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [20] loss: 1.124\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [10] loss: 1.096\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [130] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [135] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [135] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [140] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [140] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [145] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [145] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [150] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [150] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [155] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [155] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [160] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [25] loss: 1.071\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [165] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [160] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [170] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [165] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [175] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [170] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [175] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [180] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [185] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [180] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [185] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [30] loss: 1.038\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [15] loss: 1.018\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [190] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [195] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [190] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [200] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [205] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [195] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [200] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [210] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [215] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [205] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [220] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [210] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [35] loss: 1.023\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [225] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [215] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [230] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [220] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [235] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [225] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [240] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [230] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [235] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [245] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [40] loss: 1.016\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [20] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [250] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [240] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [255] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [245] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [250] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [260] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [265] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [270] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [255] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [260] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [275] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [280] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [45] loss: 1.014\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [285] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [265] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [290] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [270] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [275] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [280] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [295] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [300] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [305] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [25] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [285] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [290] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [310] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [315] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [50] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [295] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [300] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [320] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [325] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [330] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [335] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [305] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [310] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [340] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [55] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [315] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [345] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [320] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [325] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [350] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [30] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [330] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [355] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [335] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [360] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [340] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [365] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [345] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [350] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [370] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [375] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [60] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [355] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [380] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [360] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [365] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [385] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [390] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [395] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [370] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [375] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [400] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [405] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [410] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [65] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [35] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [380] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [385] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [390] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [415] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [395] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [420] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [425] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [430] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [400] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [70] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [405] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [435] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [440] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [410] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [445] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [450] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [415] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [420] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [455] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [425] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [460] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [40] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [430] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [465] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [75] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [470] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [435] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [475] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [440] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [445] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [450] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [480] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [485] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [455] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [490] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [460] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [495] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m [500] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [80] loss: 1.009\n","Result for DEFAULT_0a9bd_00006:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_17-54-17\n","  done: true\n","  experiment_id: fe1b50be25e24ab692ddfb38535111fe\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.5466160774230957\n","  node_ip: 10.201.135.226\n","  pid: 80189\n","  time_since_restore: 4.869223117828369\n","  time_this_iter_s: 4.869223117828369\n","  time_total_s: 4.869223117828369\n","  timestamp: 1621153457\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00006\n","  \n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [465] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [45] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [470] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [475] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [480] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [485] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80189)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.9/8.0 GiB<br>Using AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.545193076133728<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-53-49<br>Number of trials: 10/10 (2 PENDING, 3 RUNNING, 5 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_0a9bd_00004</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0474008</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00005</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0229811</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00007</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0108553</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00008</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.013946 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00009</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0474737</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.095975 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.05788</td><td style=\"text-align: right;\">1.56665</td></tr>\n<tr><td>DEFAULT_0a9bd_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0165373</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.88889</td><td style=\"text-align: right;\">1.54519</td></tr>\n<tr><td>DEFAULT_0a9bd_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0154145</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.99965</td><td style=\"text-align: right;\">1.52055</td></tr>\n<tr><td>DEFAULT_0a9bd_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0793133</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.31704</td><td style=\"text-align: right;\">1.50413</td></tr>\n<tr><td>DEFAULT_0a9bd_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0648928</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.86922</td><td style=\"text-align: right;\">1.54662</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=80190)\u001b[0m [85] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [490] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [495] loss: 1.008\n","Result for DEFAULT_0a9bd_00007:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_17-54-18\n","  done: true\n","  experiment_id: e276de4eb65c4815bd381bcafbae45b4\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.552183747291565\n","  node_ip: 10.201.135.226\n","  pid: 80185\n","  time_since_restore: 5.28221321105957\n","  time_this_iter_s: 5.28221321105957\n","  time_total_s: 5.28221321105957\n","  timestamp: 1621153458\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00007\n","  \n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m [500] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80185)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [90] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [50] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [95] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [55] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [100] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [105] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [60] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [110] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [115] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [65] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [120] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [125] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [70] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [130] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [135] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [75] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [140] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [145] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [80] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [150] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [85] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [155] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [160] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [90] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [165] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [170] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [95] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [175] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [100] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [180] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [185] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [105] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [190] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [195] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [110] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [200] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [205] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [115] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [210] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [120] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [215] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [220] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [125] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [225] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [130] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [230] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [235] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [135] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [240] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [245] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [140] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [250] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [255] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [145] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [5] loss: 1.242\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [10] loss: 1.229\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [5] loss: 1.221\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [15] loss: 1.209\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [10] loss: 1.164\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [20] loss: 1.182\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [15] loss: 1.082\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [25] loss: 1.145\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [20] loss: 1.028\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [260] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [30] loss: 1.105\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [25] loss: 1.014\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [35] loss: 1.071\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [150] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [40] loss: 1.047\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [45] loss: 1.032\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [30] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [265] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [50] loss: 1.024\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [35] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [55] loss: 1.019\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [40] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [270] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [60] loss: 1.016\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [65] loss: 1.014\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [45] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [70] loss: 1.013\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [50] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [155] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [75] loss: 1.013\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [55] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [275] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [80] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [60] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [85] loss: 1.012\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [90] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [65] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [95] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [70] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [100] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [280] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [105] loss: 1.011\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [75] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [160] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [110] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [80] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [115] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [85] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [120] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [285] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [90] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [125] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [130] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [95] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [135] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [140] loss: 1.010\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [100] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [290] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [165] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [145] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [105] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [150] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [110] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [295] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [155] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [115] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [160] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [165] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [120] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [170] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [170] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [175] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [125] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [300] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [180] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [185] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [130] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [190] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [135] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [195] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [140] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [305] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [200] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [205] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [145] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [175] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [210] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [150] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [310] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [215] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [155] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [220] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [160] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [225] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [230] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [165] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [315] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [180] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [235] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [170] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [240] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [245] loss: 1.009\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [250] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [175] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [320] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [255] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [260] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [180] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [265] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [185] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [270] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [190] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [325] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [185] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [275] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [195] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [280] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [285] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [200] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [290] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [205] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [330] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [295] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [210] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [300] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [305] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [215] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [220] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [190] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [310] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [335] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [315] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [225] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [320] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [230] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [325] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [235] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [340] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [330] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [335] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [240] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [195] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [245] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [340] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [345] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [345] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [350] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [250] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [355] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [255] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [360] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [260] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [365] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [350] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [200] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [370] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [265] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [375] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [270] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [380] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [385] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [275] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [355] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [390] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [280] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [395] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [285] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [400] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [360] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [205] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [405] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [290] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [410] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [295] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [415] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [420] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [300] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [425] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [305] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [365] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [430] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [310] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [210] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [435] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [440] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [315] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [370] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [445] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [320] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [450] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [325] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [455] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [375] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [460] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [330] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [465] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [335] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [215] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [380] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [470] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [475] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [340] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [480] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [345] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [485] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [490] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [350] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [385] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [220] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [495] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [355] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m [500] loss: 1.008\n","Result for DEFAULT_0a9bd_00008:\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [360] loss: 1.007\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-54-35\n","  done: false\n","  experiment_id: 863253d2ec8247a18163747d53e8ae6e\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.5105074644088745\n","  node_ip: 10.201.135.226\n","  pid: 80214\n","  time_since_restore: 7.840059041976929\n","  time_this_iter_s: 7.840059041976929\n","  time_total_s: 7.840059041976929\n","  timestamp: 1621153475\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00008\n","  \n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.545193076133728<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-53-49<br>Number of trials: 10/10 (4 RUNNING, 6 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_0a9bd_00004</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0474008</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00005</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0229811</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00008</td><td>RUNNING   </td><td>10.201.135.226:80214</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.013946 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.84006</td><td style=\"text-align: right;\">1.51051</td></tr>\n<tr><td>DEFAULT_0a9bd_00009</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0474737</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.095975 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.05788</td><td style=\"text-align: right;\">1.56665</td></tr>\n<tr><td>DEFAULT_0a9bd_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0165373</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.88889</td><td style=\"text-align: right;\">1.54519</td></tr>\n<tr><td>DEFAULT_0a9bd_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0154145</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.99965</td><td style=\"text-align: right;\">1.52055</td></tr>\n<tr><td>DEFAULT_0a9bd_00003</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0793133</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.31704</td><td style=\"text-align: right;\">1.50413</td></tr>\n<tr><td>DEFAULT_0a9bd_00006</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0648928</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.86922</td><td style=\"text-align: right;\">1.54662</td></tr>\n<tr><td>DEFAULT_0a9bd_00007</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0108553</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.28221</td><td style=\"text-align: right;\">1.55218</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_0a9bd_00008:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-54-35\n","  done: true\n","  experiment_id: 863253d2ec8247a18163747d53e8ae6e\n","  experiment_tag: 8_H1=256,H2=32,H3=2048,H4=128,H5=32,H6=32,lr=0.013946\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.5105074644088745\n","  node_ip: 10.201.135.226\n","  pid: 80214\n","  time_since_restore: 7.840059041976929\n","  time_this_iter_s: 7.840059041976929\n","  time_total_s: 7.840059041976929\n","  timestamp: 1621153475\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00008\n","  \n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [390] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=80214)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [365] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [370] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [375] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [395] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [225] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [380] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [385] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [390] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [400] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [395] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [400] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [405] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [230] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [405] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [410] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [410] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [415] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [420] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [235] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [425] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [415] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [430] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [435] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [420] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [440] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [240] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [445] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [450] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [425] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [455] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [460] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [465] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [430] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [245] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [470] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [475] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [435] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [480] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [485] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [250] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [490] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [440] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [495] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m [500] loss: 1.007\n","Result for DEFAULT_0a9bd_00009:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_17-54-38\n","  done: true\n","  experiment_id: 136d07922c0841d592f1a6d63e1dad15\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.5459301471710205\n","  node_ip: 10.201.135.226\n","  pid: 80212\n","  time_since_restore: 10.732824087142944\n","  time_this_iter_s: 10.732824087142944\n","  time_total_s: 10.732824087142944\n","  timestamp: 1621153478\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00009\n","  \n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [445] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [255] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80212)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [450] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [455] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [260] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [460] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [465] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [265] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [470] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [475] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [270] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [480] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [275] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [485] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [490] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [280] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [495] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m [500] loss: 1.008\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [285] loss: 1.007\n","Result for DEFAULT_0a9bd_00005:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-54-41\n","  done: false\n","  experiment_id: 9d6d8c6dd1d146008251010b5f4a84c6\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.4954121112823486\n","  node_ip: 10.201.135.226\n","  pid: 80190\n","  time_since_restore: 28.369023084640503\n","  time_this_iter_s: 28.369023084640503\n","  time_total_s: 28.369023084640503\n","  timestamp: 1621153481\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00005\n","  \n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [290] loss: 1.007"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.8/8.0 GiB<br>Using AsyncHyperBand: num_stopped=5\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.545193076133728<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-53-49<br>Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_0a9bd_00004</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0474008</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_0a9bd_00005</td><td>RUNNING   </td><td>10.201.135.226:80190</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0229811</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        28.369  </td><td style=\"text-align: right;\">1.49541</td></tr>\n<tr><td>DEFAULT_0a9bd_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.095975 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.05788</td><td style=\"text-align: right;\">1.56665</td></tr>\n<tr><td>DEFAULT_0a9bd_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0165373</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.88889</td><td style=\"text-align: right;\">1.54519</td></tr>\n<tr><td>DEFAULT_0a9bd_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0154145</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.99965</td><td style=\"text-align: right;\">1.52055</td></tr>\n<tr><td>DEFAULT_0a9bd_00003</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0793133</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.31704</td><td style=\"text-align: right;\">1.50413</td></tr>\n<tr><td>DEFAULT_0a9bd_00006</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0648928</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.86922</td><td style=\"text-align: right;\">1.54662</td></tr>\n<tr><td>DEFAULT_0a9bd_00007</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0108553</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.28221</td><td style=\"text-align: right;\">1.55218</td></tr>\n<tr><td>DEFAULT_0a9bd_00008</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.013946 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.84006</td><td style=\"text-align: right;\">1.51051</td></tr>\n<tr><td>DEFAULT_0a9bd_00009</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0474737</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.7328 </td><td style=\"text-align: right;\">1.54593</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=80190)\u001b[0m \n","Result for DEFAULT_0a9bd_00005:\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_17-54-41\n","  done: true\n","  experiment_id: 9d6d8c6dd1d146008251010b5f4a84c6\n","  experiment_tag: 5_H1=128,H2=64,H3=2048,H4=1024,H5=32,H6=32,lr=0.022981\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.4954121112823486\n","  node_ip: 10.201.135.226\n","  pid: 80190\n","  time_since_restore: 28.369023084640503\n","  time_this_iter_s: 28.369023084640503\n","  time_total_s: 28.369023084640503\n","  timestamp: 1621153481\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00005\n","  \n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [295] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [300] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [305] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [310] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [315] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [320] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [325] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [330] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [335] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [340] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [345] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [350] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [355] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [360] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [365] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [370] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [375] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [380] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [385] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [390] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [395] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [400] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [405] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [410] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [415] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [420] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [425] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [430] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [435] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [440] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [445] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [450] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [455] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [460] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [465] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [470] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [475] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [480] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [485] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [490] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [495] loss: 1.007\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m [500] loss: 1.007\n","Result for DEFAULT_0a9bd_00004:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_17-54-49\n","  done: true\n","  experiment_id: 33a36d1e201041f2a8f70e8de4f69600\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.5461505651474\n","  node_ip: 10.201.135.226\n","  pid: 80188\n","  time_since_restore: 36.79802107810974\n","  time_this_iter_s: 36.79802107810974\n","  time_total_s: 36.79802107810974\n","  timestamp: 1621153489\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 0a9bd_00004\n","  \n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80188)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.8/8.0 GiB<br>Using AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.5455616116523743<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-53-49<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_0a9bd_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.095975 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.05788</td><td style=\"text-align: right;\">1.56665</td></tr>\n<tr><td>DEFAULT_0a9bd_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0165373</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.88889</td><td style=\"text-align: right;\">1.54519</td></tr>\n<tr><td>DEFAULT_0a9bd_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0154145</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.99965</td><td style=\"text-align: right;\">1.52055</td></tr>\n<tr><td>DEFAULT_0a9bd_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0793133</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.31704</td><td style=\"text-align: right;\">1.50413</td></tr>\n<tr><td>DEFAULT_0a9bd_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0474008</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        36.798  </td><td style=\"text-align: right;\">1.54615</td></tr>\n<tr><td>DEFAULT_0a9bd_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0229811</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        28.369  </td><td style=\"text-align: right;\">1.49541</td></tr>\n<tr><td>DEFAULT_0a9bd_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0648928</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.86922</td><td style=\"text-align: right;\">1.54662</td></tr>\n<tr><td>DEFAULT_0a9bd_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0108553</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.28221</td><td style=\"text-align: right;\">1.55218</td></tr>\n<tr><td>DEFAULT_0a9bd_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.013946 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.84006</td><td style=\"text-align: right;\">1.51051</td></tr>\n<tr><td>DEFAULT_0a9bd_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0474737</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.7328 </td><td style=\"text-align: right;\">1.54593</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.8/8.0 GiB<br>Using AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.5455616116523743<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-53-49<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_0a9bd_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.095975 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.05788</td><td style=\"text-align: right;\">1.56665</td></tr>\n<tr><td>DEFAULT_0a9bd_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0165373</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.88889</td><td style=\"text-align: right;\">1.54519</td></tr>\n<tr><td>DEFAULT_0a9bd_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0154145</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.99965</td><td style=\"text-align: right;\">1.52055</td></tr>\n<tr><td>DEFAULT_0a9bd_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0793133</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.31704</td><td style=\"text-align: right;\">1.50413</td></tr>\n<tr><td>DEFAULT_0a9bd_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0474008</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        36.798  </td><td style=\"text-align: right;\">1.54615</td></tr>\n<tr><td>DEFAULT_0a9bd_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0229811</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        28.369  </td><td style=\"text-align: right;\">1.49541</td></tr>\n<tr><td>DEFAULT_0a9bd_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0648928</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.86922</td><td style=\"text-align: right;\">1.54662</td></tr>\n<tr><td>DEFAULT_0a9bd_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0108553</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.28221</td><td style=\"text-align: right;\">1.55218</td></tr>\n<tr><td>DEFAULT_0a9bd_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.013946 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.84006</td><td style=\"text-align: right;\">1.51051</td></tr>\n<tr><td>DEFAULT_0a9bd_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0474737</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.7328 </td><td style=\"text-align: right;\">1.54593</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2021-05-16 17:54:49,821\tINFO tune.py:549 -- Total run time: 60.15 seconds (59.86 seconds for the tuning loop).\n","Best trial config: {'H1': 128, 'H2': 64, 'H3': 2048, 'H4': 1024, 'H5': 32, 'H6': 32, 'lr': 0.02298110113208653}\n","Best trial final validation loss: 1.4954121112823486\n","Best trial fnal validation accuracy: 0.029702970758080482\n","Best trial test set accuracy: 20.792079207920793\n"]}],"source":["# Evaluation:\n","def test_accuracy(net, device='cpu'):\n","       net.eval()\n","       with torch.no_grad():\n","              predicted = net(transformed_test_x.float().reshape(1,22))\n","              return get_accuracy(net, predicted.reshape(101,1), transformed_test_y.reshape(101,1), 0.50)\n","       \n","# Define hyperparameters\n","config = {\n","       \"H1\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H2\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H3\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H4\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H5\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H6\":tune.sample_from(lambda _: 2**np.random.randint(5,9)),\n","       \"lr\":tune.loguniform(1e-2, 1e-1)\n","}\n","\n","# Scheduler. Randomly try out combination of hyperparameters\n","scheduler = ASHAScheduler(\n","       metric=\"loss\",\n","       mode=\"min\",\n","       max_t=10,\n","       grace_period=1,\n","       reduction_factor=2\n",")\n","\n","reporter=CLIReporter(metric_columns=[\"loss\",\"accuracy\", \"training_iteration\"])\n","\n","result = tune.run(\n","       partial(train_crop_yield),\n","       config=config,\n","       num_samples=10,\n","       scheduler=scheduler\n",")\n","\n","best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","\n","print(\"Best trial config: {}\".format(best_trial.config))\n","print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n","print(\"Best trial fnal validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n","\n","best_trained_model = NeuralNet(22, best_trial.config[\"H1\"], best_trial.config[\"H2\"], best_trial.config[\"H3\"],best_trial.config[\"H4\"],best_trial.config[\"H5\"],best_trial.config[\"H6\"],101)\n","\n","device=\"cpu\"\n","best_trained_model.to(device)\n","\n","# best_checkpoint_dir = best_trial.checkpoint.value\n","# print(\" \\n\",type(best_trial.evaluated_params))\n","# model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n","# best_trained_model.load_state_dict(model_state)\n","\n","num_correct, test_acc = test_accuracy(best_trained_model, device)\n","print(\"Best trial test set accuracy: {}\".format(test_acc))\n",""]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["1.8.1\n1.2.4\n"]}],"source":["\n","\n","import torch\n","print(torch.__version__)\n","import pandas as pd\n","print(pd.__version__)\n","from torch.utils.data import DataLoader,ConcatDataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n",""]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["\n","# ## Read pesticides and fertilizer products data\n",""]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["def load_data():\n","       pesticides_frame = pd.read_csv(\"Inputs_Pesticides_Use_E_All_Data_NOFLAG.csv\", engine='python')\n","       pesticides_frame.head\n","  \n","       # Read fertilizer data in the form of products\n","       fertilizers_frame = pd.read_csv(\"Inputs_FertilizersProduct_E_All_Data_NOFLAG.csv\", engine='python')\n","       fertilizers_frame.head\n","       \n","       # Read the crop yield data\n","       yield_frame = pd.read_csv(\"Production_Crops_E_All_Data_NOFLAG.csv\",engine='python')\n","       yield_frame.head\n","\n","       # %<br>\n","       # Get the fertilizer usage in terms of agriculture\n","\n","       # In[4]:     \n","       print(\"Fertilizer products\",fertilizers_frame.columns)\n","       print(\"\\n\")\n","       print(\"Input pesticides\", pesticides_frame.columns)\n","       \n","       # Filter out the data for Australia\n","       is_Country_Australia_Products = fertilizers_frame['Area']==\"Australia\"\n","       Australia_Fertilizers_Products = fertilizers_frame[is_Country_Australia_Products]\n","       # In[5]:\n","       is_Country_Australia_Pesticides = pesticides_frame['Area']==\"Australia\"\n","       Australia_Pesticides = pesticides_frame[is_Country_Australia_Pesticides]\n","\n","       # Get the fertilizer usage for agricultural use\n","       # In[6]:\n","       is_Agricultural = Australia_Fertilizers_Products['Element']==\"Agricultural Use\"\n","       Australia_Fertilizers_Products_Agricultural = Australia_Fertilizers_Products[is_Agricultural]\n","       print(\"Fertilizers \\n\")\n","       print(Australia_Fertilizers_Products_Agricultural.columns)\n","       print(Australia_Fertilizers_Products_Agricultural['Item'])\n","       print(\"\\n\")\n","       print(\"Pesticides \\n\")\n","       print(Australia_Pesticides.columns)\n","       print(Australia_Pesticides['Item'])\n","       \n","       # %<br>\n","       # 7 years for training - 2007 to 2013<br>\n","       # 3 years fro validation - 2014 to 2016<br>\n","       # 2017 for testing\n","       # In[7]:\n","       X_ten_years_fertlizers = Australia_Fertilizers_Products_Agricultural.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = Australia_Fertilizers_Products_Agricultural['Y2017']\n","       print(\"2017 \\n\",fertilizer_2017)\n","\n","       # In[8]:\n","       X_ten_years_pesticides = Australia_Pesticides.drop(['Area Code', 'Area', 'Item Code', 'Element Code', 'Element',\n","              'Unit', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017'], axis=1)\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_pesticides.columns)\n","       pesticides_2017 = Australia_Pesticides['Y2017']\n","       print(\"2017 \\n\",pesticides_2017)\n","\n","       # %<br>\n","       # Drop Ammonia anyhydrous and Ammonia nitrate from 2007 to 2013. It has not been used in these years.\n","       # In[9]:\n","       X_ten_years_fertlizers = X_ten_years_fertlizers.drop([602, 607], axis=0)\n","       print(X_ten_years_fertlizers)\n","       print(X_ten_years_fertlizers.columns)\n","       fertilizer_2017 = fertilizer_2017.drop([602, 607], axis=0)\n","       print(fertilizer_2017)\n","       \n","       # %<br>\n","       # 2009 to 2013\n","       # ## Handle missing values from 2009 to 2013\n","       # In[10]:\n","       interpolate_2009_to_2016 = X_ten_years_fertlizers.interpolate()\n","       interpolate_2009_to_2016\n","       interpolate_2017 = fertilizer_2017.interpolate()\n","       print(\"interpolated 2017 fertilizers \\n\",interpolate_2017)\n","       # TODO: Don't just interpolate. Use the means of the previous 2  years\n","       # interpolate_pesticides_2017 = pesticides_2017.interpolate()\n","       # print(\"interpolated 2017 pesticides \\n\",interpolate_pesticides_2017)\n","\n","       # %<br>\n","       #  Interpolate missing fertlizers data<br>\n","       #  Replace na with mean values of 2004,2005,2009,2010\n","       # ##  Replace 2007 nan values with the mean of 2004 and 2005\n","\n","       # In[11]:\n","\n","       interpolate_2004_2005 = Australia_Fertilizers_Products_Agricultural[['Y2004','Y2005']].copy().interpolate()\n","       index_names = interpolate_2004_2005.index\n","       index_names\n","       interpolate_2004_2005=interpolate_2004_2005.drop([602,607], axis=0)\n","       interpolate_2004_2005['mean'] = interpolate_2004_2005.mean(axis=1)\n","       print(\"Mean of 2004 and 2005 ==== \\n\")\n","       print(interpolate_2004_2005)\n","       pesticides_2015_2016 = Australia_Pesticides[['Y2015','Y2016']]\n","       pesticides_2015_2016['mean'] = pesticides_2015_2016.mean(axis=1)\n","       print(\"Mean of 2015 and 2016 === \\n\")\n","       print(pesticides_2015_2016)\n","\n","       # Use this for 2007\n","\n","       # In[ ]:\n","       # ##  Replace 2008 nan values with the mean of 2009 and 2010\n","\n","       # In[12]:\n","       interpolate_2009_2010 = Australia_Fertilizers_Products_Agricultural[['Y2009','Y2010']].copy().interpolate()\n","       index_names = interpolate_2009_2010.index\n","       index_names\n","\n","       # In[13]:\n","       interpolate_2009_2010['mean'] = interpolate_2009_2010.mean(axis=1)\n","       print(\"Mean of 2009 and 2010 ==== \\n\")\n","       # Use this for 2008\n","       interpolate_2009_2010=interpolate_2009_2010.drop([602,607], axis=0)\n","       print(interpolate_2009_2010)\n","\n","\n","       # ## Populate 2007 nan values with computed mean\n","       # In[14]:\n","       # Fertilizers\n","       i=0\n","       for i in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:i,1:2] = interpolate_2004_2005['mean'].iloc[0:i]\n","\n","       #Pesticides\n","       j=0\n","       for j in range(1,7):\n","              pesticides_2017.iloc[0:j,] = pesticides_2015_2016['mean'].iloc[0:j]\n","\n","       # ## Populate 2008 nan values with computed mean\n","       # In[15]:\n","       j=0\n","       for j in range(1,16):\n","              interpolate_2009_to_2016.iloc[0:j,2:3] = interpolate_2009_2010['mean'].iloc[0:j]\n","                \n","       interpolate_2009_to_2016.iloc[15, 1] = 1437658.50\n","       print(interpolate_2009_to_2016.iloc[15, 1])\n","       interpolate_2009_to_2016.iloc[15, 2] = 171351.00\n","\n","       # In[16]:\n","       X_ten_years_fertilizers_interpolated = interpolate_2009_to_2016\n","       X_ten_years_fertilizers_interpolated\n","       fertilizer_2017_interpolated = interpolate_2017\n","       print(fertilizer_2017_interpolated)\n","\n","       # #### TODO: Combine pesticdes and fertilizers data\n","       # In[17]:\n","       print(X_ten_years_pesticides)\n","       print(X_ten_years_fertilizers_interpolated)\n","\n","       pesticides_without_Item = X_ten_years_pesticides.copy().drop(['Item'], axis=1)\n","       pesticides_2017_wo_item = pesticides_2017\n","       fertilizers_without_Item = X_ten_years_fertilizers_interpolated.copy().drop(['Item'], axis=1)\n","       fertilizer_2017_wo_item = fertilizer_2017_interpolated\n","       # torch.tensor(X_ten_years_pesticides.values.astype(np.float64))\n","       print(X_ten_years_pesticides.values)\n","       print(X_ten_years_fertilizers_interpolated.values)\n","       print(pesticides_2017_wo_item.values)\n","       print(fertilizer_2017_wo_item.values)\n","\n","       print(pesticides_without_Item)\n","       pesticides_tensor = torch.from_numpy(pesticides_without_Item.values)\n","       print(\"\\t Pesticides tensor\", pesticides_tensor)\n","       pesticides_2017_tensor = torch.from_numpy(pesticides_2017_wo_item.values)\n","\n","       print(fertilizers_without_Item)\n","       fertilizers_tensor = torch.from_numpy(fertilizers_without_Item.values)\n","       fertilizer_2017_tensor = torch.from_numpy(fertilizer_2017_wo_item.values)\n","       print(\"\\t Fertilizers tensor\", fertilizers_tensor)\n","\n","       # In[18]:\n","       print(\"Train : \\n\",pesticides_tensor.shape)\n","       print(\"Train: \\n\",fertilizers_tensor.shape)\n","       print(\"Test: \\n\",pesticides_2017_tensor.shape)\n","       print(\"Test: \\n\",fertilizer_2017_tensor.shape)\n","\n","       # TODO: Normalize inputs before concatenation\n","       pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n","       print(pesticides_and_fertilizers.shape)\n","       print(\"\\n\")\n","       print(\"Training X data for 10 years: \",pesticides_and_fertilizers)\n","       print(\"\\n\")\n","       pesticides_and_fertilizers_2017 = torch.cat((pesticides_2017_tensor, fertilizer_2017_tensor), dim=0)\n","       print(pesticides_and_fertilizers_2017.shape)\n","       print(\"\\n\")\n","       print(\"Testing X data for 1 year: \",pesticides_and_fertilizers_2017)\n","       print(\"\\n\")\n","\n","       # Training + Validation data : Input\n","       X_train_ten = pesticides_and_fertilizers;\n","       X_test = pesticides_and_fertilizers_2017\n","\n","       # #### TODO: Create output vector\n","\n","       # In[19]:\n","       print(yield_frame.columns)\n","       #%%\n","       yield_frame.head\n","       #%%\n","       Australia = yield_frame['Area']==\"Australia\"\n","       Production = yield_frame['Element']==\"Production\"\n","       Australia_yield = yield_frame[Australia]\n","       Australia_yield_production = Australia_yield[Production]\n","       print(yield_frame[Australia])\n","       #%%\n","       # Get the yield data for the year 2017 only\n","       Australia_yield_2017 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015', \n","              'Y2016', 'Y2018'], axis=1)\n","       print(\"\\n\")\n","       print(\"Test Y data: \",Australia_yield_2017)\n","       print(\"\\n\")\n","       print(Australia_yield_2017.columns)\n","       print(Australia_yield_2017['Item'].unique())\n","       print(\"Number of unique crops in 2017: \",len(Australia_yield_2017['Item'].unique()))\n","\n","       Australia_yield_2007_to_2016 = Australia_yield_production.drop(['Area Code', 'Area', 'Item Code', 'Element Code','Element',\n","              'Unit','Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967', 'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', \n","              'Y1974', 'Y1975', 'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983', 'Y1984', 'Y1985', 'Y1986', 'Y1987',\n","              'Y1988', 'Y1989', 'Y1990', 'Y1991', 'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999', 'Y2000', 'Y2001', \n","              'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2017','Y2018'], axis=1)\n","       print(Australia_yield_2007_to_2016.columns)\n","       print(Australia_yield_2007_to_2016['Item'].unique())\n","       print(\"\\n\")\n","       print(\"Train Y data: \",Australia_yield_2007_to_2016)\n","       print(\"\\n\")\n","       print(\"Number of unique crops from 2007 to 2016\",len(Australia_yield_2007_to_2016['Item'].unique()))\n","\n","       # In[20]:\n","       yield_without_Item = Australia_yield_2007_to_2016.copy().drop(['Item'],axis=1)\n","       yield_without_Item_2017 = Australia_yield_2017.copy().drop(['Item'],axis=1)\n","\n","       print(yield_without_Item)\n","       yield_tensor = torch.from_numpy(yield_without_Item.values)\n","       print(\"Shape: \", yield_tensor.shape)\n","       print(\"\\t Yield tensor\", yield_tensor)\n","       print(yield_without_Item_2017)\n","       yield_tensor_2017 = torch.from_numpy(yield_without_Item_2017.values)\n","       print(\"Shape: \", yield_tensor_2017.shape)\n","       print(\"\\t Yield tensor\", yield_tensor_2017)\n","\n","       # Training + Validation: Output\n","       Y_train_ten = yield_tensor\n","       # Test Y\n","       Y_test = yield_tensor_2017\n","\n","       print(\"Train X: \\n\",X_train_ten.shape)\n","       print(\"Train Y: \\n\",Y_train_ten.shape)\n","       print(\"Test X: \\n\",X_test.shape)\n","       print(\"Test Y: \\n\",Y_test.shape)\n","       \n","       return [X_train_ten, Y_train_ten, X_test, Y_test]"]},{"cell_type":"code","execution_count":66,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["   [2.3410e+05, 2.0284e+04, 5.7860e+03, 3.4782e+04, 1.2869e+04, 3.3525e+04,\n","         3.1054e+05, 4.0976e+04, 5.9000e+04, 4.8019e+04],\n","        [2.5426e+04, 9.8828e+04, 2.1154e+04, 1.7650e+05, 5.0525e+04, 1.2845e+05,\n","         4.1147e+05, 3.3659e+05, 4.0550e+05, 3.9277e+05],\n","        [2.8115e+05, 5.2029e+05, 7.8618e+04, 9.6196e+05, 8.9395e+04, 8.9976e+05,\n","         5.1240e+05, 6.3220e+05, 7.5200e+05, 7.3752e+05],\n","        [1.1858e+06, 7.0965e+05, 4.0956e+05, 1.0097e+06, 7.8751e+05, 1.0286e+06,\n","         1.2400e+05, 1.6100e+05, 1.9800e+05, 1.4450e+05],\n","        [1.4377e+06, 8.9901e+05, 7.4050e+05, 1.0575e+06, 1.4856e+06, 1.1575e+06,\n","         1.0925e+06, 1.7532e+06, 1.7720e+06, 1.5482e+06],\n","        [1.4377e+06, 1.7135e+05, 1.1072e+05, 2.3198e+05, 1.4856e+06, 2.7388e+05,\n","         1.4371e+05, 1.2240e+05, 1.0940e+05, 1.4470e+05]], dtype=torch.float64)\n","Train : \n"," torch.Size([6, 10])\n","Train: \n"," torch.Size([16, 10])\n","Test: \n"," torch.Size([6])\n","Test: \n"," torch.Size([16])\n","torch.Size([22, 10])\n","\n","\n","Training X data for 10 years:  tensor([[3.2446e+04, 4.2935e+04, 3.8066e+04, 4.2169e+04, 4.7633e+04, 4.8688e+04,\n","         4.5177e+04, 4.9857e+04, 5.0922e+04, 6.3416e+04],\n","        [7.2630e+03, 9.6109e+03, 8.5209e+03, 9.4395e+03, 1.0662e+04, 1.0899e+04,\n","         1.0113e+04, 1.1160e+04, 1.1399e+04, 1.4196e+04],\n","        [2.2404e+04, 2.9647e+04, 2.6284e+04, 2.9118e+04, 3.2890e+04, 3.3619e+04,\n","         3.1195e+04, 3.4426e+04, 3.5161e+04, 4.3789e+04],\n","        [2.3249e+03, 3.0765e+03, 2.7276e+03, 3.0216e+03, 3.4131e+03, 3.4887e+03,\n","         3.2371e+03, 3.5725e+03, 3.6487e+03, 4.5441e+03],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [4.5447e+02, 6.0139e+02, 5.3318e+02, 5.9066e+02, 6.6719e+02, 6.8197e+02,\n","         6.3279e+02, 6.9835e+02, 7.1325e+02, 8.8827e+02],\n","        [3.0365e+05, 1.6086e+05,        nan, 1.6086e+05, 4.7329e+04, 1.5680e+05,\n","         1.3463e+05, 1.3892e+05, 1.9200e+05, 1.7258e+05],\n","        [2.0392e+04, 2.9583e+05, 7.4000e+01, 5.9159e+05, 2.6952e+05, 6.4043e+05,\n","         1.4296e+05, 3.7954e+05, 4.9362e+05, 4.8665e+05],\n","        [7.3190e+04, 6.1536e+05, 2.0840e+05, 1.0223e+06, 4.9172e+05, 1.1241e+06,\n","         1.5129e+05, 6.2017e+05, 7.9523e+05, 8.0072e+05],\n","        [1.2599e+05, 7.7861e+05, 4.1672e+05, 1.1405e+06, 7.1392e+05, 9.8775e+05,\n","         1.5962e+05, 8.6079e+05, 1.0969e+06, 1.1148e+06],\n","        [1.7879e+05, 6.4986e+05, 4.1037e+04, 1.2587e+06, 1.1699e+05, 8.5143e+05,\n","         1.6795e+05, 6.1150e+05, 7.0043e+05, 6.6312e+05],\n","        [2.3158e+05, 3.2902e+05, 2.1622e+04, 6.3643e+05, 7.9886e+04, 7.1511e+05,\n","         1.7628e+05, 3.6221e+05, 3.0400e+05, 2.1145e+05],\n","        [2.8438e+05, 8.1850e+03, 2.2080e+03, 1.4162e+04, 4.2783e+04, 5.7880e+05,\n","         1.8461e+05, 3.5574e+05, 3.5850e+05, 2.3647e+05],\n","        [3.3718e+05, 1.0243e+04, 1.1690e+03, 1.9317e+04, 2.1696e+04, 4.4248e+05,\n","         1.9294e+05, 3.4928e+05, 4.1300e+05, 2.6149e+05],\n","        [3.8998e+05, 1.2301e+04, 1.3000e+02, 2.4472e+04, 6.0900e+02, 3.0616e+05,\n","         2.0127e+05, 2.9040e+05, 3.2642e+05, 2.2314e+05],\n","        [4.4278e+05, 1.6292e+04, 2.9580e+03, 2.9627e+04, 2.2152e+05, 1.6984e+05,\n","         2.0960e+05, 2.3152e+05, 2.3984e+05, 1.8479e+05],\n","        [2.3410e+05, 2.0284e+04, 5.7860e+03, 3.4782e+04, 1.2869e+04, 3.3525e+04,\n","         3.1054e+05, 4.0976e+04, 5.9000e+04, 4.8019e+04],\n","        [2.5426e+04, 9.8828e+04, 2.1154e+04, 1.7650e+05, 5.0525e+04, 1.2845e+05,\n","         4.1147e+05, 3.3659e+05, 4.0550e+05, 3.9277e+05],\n","        [2.8115e+05, 5.2029e+05, 7.8618e+04, 9.6196e+05, 8.9395e+04, 8.9976e+05,\n","         5.1240e+05, 6.3220e+05, 7.5200e+05, 7.3752e+05],\n","        [1.1858e+06, 7.0965e+05, 4.0956e+05, 1.0097e+06, 7.8751e+05, 1.0286e+06,\n","         1.2400e+05, 1.6100e+05, 1.9800e+05, 1.4450e+05],\n","        [1.4377e+06, 8.9901e+05, 7.4050e+05, 1.0575e+06, 1.4856e+06, 1.1575e+06,\n","         1.0925e+06, 1.7532e+06, 1.7720e+06, 1.5482e+06],\n","        [1.4377e+06, 1.7135e+05, 1.1072e+05, 2.3198e+05, 1.4856e+06, 2.7388e+05,\n","         1.4371e+05, 1.2240e+05, 1.0940e+05, 1.4470e+05]], dtype=torch.float64)\n","\n","\n","torch.Size([22])\n","\n","\n","Testing X data for 1 year:  tensor([5.7169e+04, 1.2797e+04, 3.9475e+04, 4.0964e+03, 8.0076e+02, 8.0076e+02,\n","        1.4642e+05, 4.1860e+05, 6.9077e+05, 9.6295e+05, 6.2543e+05, 2.8791e+05,\n","        3.2327e+05, 3.5863e+05, 2.4948e+05, 1.4034e+05, 2.5094e+04, 3.6271e+05,\n","        7.0032e+05, 1.7607e+05, 1.6783e+06, 1.2868e+05], dtype=torch.float64)\n","\n","\n","Index(['Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element',\n","       'Unit', 'Y1961', 'Y1962', 'Y1963', 'Y1964', 'Y1965', 'Y1966', 'Y1967',\n","       'Y1968', 'Y1969', 'Y1970', 'Y1971', 'Y1972', 'Y1973', 'Y1974', 'Y1975',\n","       'Y1976', 'Y1977', 'Y1978', 'Y1979', 'Y1980', 'Y1981', 'Y1982', 'Y1983',\n","       'Y1984', 'Y1985', 'Y1986', 'Y1987', 'Y1988', 'Y1989', 'Y1990', 'Y1991',\n","       'Y1992', 'Y1993', 'Y1994', 'Y1995', 'Y1996', 'Y1997', 'Y1998', 'Y1999',\n","       'Y2000', 'Y2001', 'Y2002', 'Y2003', 'Y2004', 'Y2005', 'Y2006', 'Y2007',\n","       'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013', 'Y2014', 'Y2015',\n","       'Y2016', 'Y2017', 'Y2018'],\n","      dtype='object')\n","      Area Code       Area  Item Code                              Item  \\\n","1317         10  Australia        221               Almonds, with shell   \n","1318         10  Australia        221               Almonds, with shell   \n","1319         10  Australia        221               Almonds, with shell   \n","1320         10  Australia        711  Anise, badian, fennel, coriander   \n","1321         10  Australia        711  Anise, badian, fennel, coriander   \n","...         ...        ...        ...                               ...   \n","1606         10  Australia       1729                   Treenuts, Total   \n","1607         10  Australia       1729                   Treenuts, Total   \n","1608         10  Australia       1735                Vegetables Primary   \n","1609         10  Australia       1735                Vegetables Primary   \n","1610         10  Australia       1735                Vegetables Primary   \n","\n","      Element Code         Element    Unit     Y1961     Y1962     Y1963  ...  \\\n","1317          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1318          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","1319          5510      Production  tonnes       NaN       NaN       NaN  ...   \n","1320          5312  Area harvested      ha       NaN       NaN       NaN  ...   \n","1321          5419           Yield   hg/ha       NaN       NaN       NaN  ...   \n","...            ...             ...     ...       ...       ...       ...  ...   \n","1606          5419           Yield   hg/ha   17925.0   15042.0   15856.0  ...   \n","1607          5510      Production  tonnes     190.0     179.0     176.0  ...   \n","1608          5312  Area harvested      ha   63571.0   63397.0   65291.0  ...   \n","1609          5419           Yield   hg/ha  100846.0  103588.0  101085.0  ...   \n","1610          5510      Production  tonnes  641089.0  656717.0  659995.0  ...   \n","\n","          Y2009      Y2010      Y2011      Y2012      Y2013      Y2014  \\\n","1317    27981.0    29340.0    30390.0    28472.0    28586.0    28967.0   \n","1318     6775.0    30675.0    11377.0    10925.0    19863.0    19325.0   \n","1319    18957.0    90000.0    34576.0    31105.0    56779.0    55978.0   \n","1320      710.0      684.0      914.0     1015.0     1040.0     1000.0   \n","1321    13901.0    11853.0    11370.0    11488.0    11550.0    11570.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1606    12376.0    24996.0    12309.0    11661.0    17257.0    17692.0   \n","1607    60694.0   130580.0    70926.0    65324.0    92450.0    94930.0   \n","1608    66234.0    69695.0    74100.0    71471.0    66887.0    69262.0   \n","1609   270772.0   259637.0   224916.0   253910.0   277885.0   238561.0   \n","1610  1793425.0  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0   \n","\n","          Y2015      Y2016      Y2017      Y2018  \n","1317    31115.0    37000.0    38000.0    36940.0  \n","1318    20354.0    19703.0    19835.0    18917.0  \n","1319    63331.0    72902.0    75373.0    69880.0  \n","1320     1011.0     1108.0     1130.0     1074.0  \n","1321    11600.0    11673.0    11721.0    11769.0  \n","...         ...        ...        ...        ...  \n","1606    16017.0    17399.0    17430.0    16445.0  \n","1607   106374.0   118977.0   125683.0   118104.0  \n","1608    66917.0    68090.0    71918.0    70457.0  \n","1609   264193.0   257780.0   245961.0   252789.0  \n","1610  1767902.0  1755227.0  1768915.0  1781067.0  \n","\n","[294 rows x 65 columns]\n","\n","\n","Test Y data:                                    Item      Y2017\n","1319               Almonds, with shell    75373.0\n","1322  Anise, badian, fennel, coriander     1324.0\n","1325                            Apples   313730.0\n","1328                          Apricots     5351.0\n","1331                         Asparagus     7472.0\n","...                                ...        ...\n","1598          Oilcrops, Oil Equivalent  1888936.0\n","1601                     Pulses, Total  4129481.0\n","1604           Roots and Tubers, Total  1176669.0\n","1607                   Treenuts, Total   125683.0\n","1610                Vegetables Primary  1768915.0\n","\n","[101 rows x 2 columns]\n","\n","\n","Index(['Item', 'Y2017'], dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","Number of unique crops in 2017:  101\n","Index(['Item', 'Y2007', 'Y2008', 'Y2009', 'Y2010', 'Y2011', 'Y2012', 'Y2013',\n","       'Y2014', 'Y2015', 'Y2016'],\n","      dtype='object')\n","['Almonds, with shell' 'Anise, badian, fennel, coriander' 'Apples'\n"," 'Apricots' 'Asparagus' 'Avocados' 'Bananas' 'Barley' 'Beans, dry'\n"," 'Beans, green' 'Berries nes' 'Blueberries'\n"," 'Broad beans, horse beans, dry' 'Cabbages and other brassicas'\n"," 'Canary seed' 'Carrots and turnips' 'Cauliflowers and broccoli'\n"," 'Cherries' 'Chick peas' 'Chillies and peppers, green' 'Cotton lint'\n"," 'Cottonseed' 'Cow peas, dry' 'Cucumbers and gherkins' 'Currants'\n"," 'Fibre crops nes' 'Figs' 'Flax fibre and tow' 'Fruit, citrus nes'\n"," 'Fruit, stone nes' 'Fruit, tropical fresh nes'\n"," 'Grapefruit (inc. pomelos)' 'Grapes' 'Groundnuts, with shell' 'Hops'\n"," 'Kiwi fruit' 'Lemons and limes' 'Lentils' 'Lettuce and chicory' 'Linseed'\n"," 'Lupins' 'Maize' 'Maize, green' 'Mangoes, mangosteens, guavas'\n"," 'Melons, other (inc.cantaloupes)' 'Millet' 'Mushrooms and truffles'\n"," 'Mustard seed' 'Nuts nes' 'Oats' 'Olives' 'Onions, dry' 'Oranges'\n"," 'Papayas' 'Peaches and nectarines' 'Pears' 'Peas, dry' 'Peas, green'\n"," 'Persimmons' 'Pineapples' 'Pistachios' 'Plums and sloes' 'Potatoes'\n"," 'Pulses nes' 'Pumpkins, squash and gourds' 'Quinces' 'Rapeseed'\n"," 'Raspberries' 'Rice, paddy' 'Rye' 'Safflower seed' 'Seed cotton'\n"," 'Sorghum' 'Soybeans' 'Spinach' 'Strawberries' 'Sugar cane'\n"," 'Sunflower seed' 'Sweet potatoes'\n"," 'Tangerines, mandarins, clementines, satsumas' 'Tobacco, unmanufactured'\n"," 'Tomatoes' 'Triticale' 'Vegetables, fresh nes' 'Vetches'\n"," 'Walnuts, with shell' 'Watermelons' 'Wheat' 'Cereals (Rice Milled Eqv)'\n"," 'Cereals, Total' 'Citrus Fruit, Total' 'Coarse Grain, Total'\n"," 'Fibre Crops Primary' 'Fruit Primary' 'Oilcrops'\n"," 'Oilcrops, Cake Equivalent' 'Oilcrops, Oil Equivalent' 'Pulses, Total'\n"," 'Roots and Tubers, Total' 'Treenuts, Total' 'Vegetables Primary']\n","\n","\n","Train Y data:                                    Item      Y2007      Y2008      Y2009  \\\n","1319               Almonds, with shell    18024.0    65000.0    18957.0   \n","1322  Anise, badian, fennel, coriander       51.0     2091.0      987.0   \n","1325                            Apples   377980.0   265481.0   295134.0   \n","1328                          Apricots    17327.0    17000.0    13673.0   \n","1331                         Asparagus     5609.0     9779.0     6981.0   \n","...                                ...        ...        ...        ...   \n","1598          Oilcrops, Oil Equivalent   311986.0   553601.0   836928.0   \n","1601                     Pulses, Total  1168635.0  1681825.0  1817218.0   \n","1604           Roots and Tubers, Total  1261119.0  1447602.0  1220994.0   \n","1607                   Treenuts, Total    60924.0   103690.0    60694.0   \n","1610                Vegetables Primary  1746974.0  1703422.0  1793425.0   \n","\n","          Y2010      Y2011      Y2012      Y2013      Y2014      Y2015  \\\n","1319    90000.0    34576.0    31105.0    56779.0    55978.0    63331.0   \n","1322      810.0     1039.0     1166.0     1201.0     1157.0     1173.0   \n","1325   264401.0   299778.0   289064.0   288878.0   266771.0   295196.0   \n","1328    13175.0    13283.0    12186.0    11551.0     9238.0     9674.0   \n","1331     8835.0    10276.0     9589.0     8396.0     8375.0     8288.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   869470.0  1152688.0  1647064.0  1869619.0  1709984.0  1503124.0   \n","1601  2143709.0  2551800.0  2633541.0  2224400.0  2247300.0  1989200.0   \n","1604  1327922.0  1185085.0  1350721.0  1341112.0  1246546.0  1215195.0   \n","1607   130580.0    70926.0    65324.0    92450.0    94930.0   106374.0   \n","1610  1809529.0  1666625.0  1814715.0  1858687.0  1652335.0  1767902.0   \n","\n","          Y2016  \n","1319    72902.0  \n","1322     1293.0  \n","1325   308298.0  \n","1328     8700.0  \n","1331     7737.0  \n","...         ...  \n","1598  1239283.0  \n","1601  2421811.0  \n","1604  1200748.0  \n","1607   118977.0  \n","1610  1755227.0  \n","\n","[101 rows x 11 columns]\n","\n","\n","Number of unique crops from 2007 to 2016 101\n","          Y2007      Y2008      Y2009      Y2010      Y2011      Y2012  \\\n","1319    18024.0    65000.0    18957.0    90000.0    34576.0    31105.0   \n","1322       51.0     2091.0      987.0      810.0     1039.0     1166.0   \n","1325   377980.0   265481.0   295134.0   264401.0   299778.0   289064.0   \n","1328    17327.0    17000.0    13673.0    13175.0    13283.0    12186.0   \n","1331     5609.0     9779.0     6981.0     8835.0    10276.0     9589.0   \n","...         ...        ...        ...        ...        ...        ...   \n","1598   311986.0   553601.0   836928.0   869470.0  1152688.0  1647064.0   \n","1601  1168635.0  1681825.0  1817218.0  2143709.0  2551800.0  2633541.0   \n","1604  1261119.0  1447602.0  1220994.0  1327922.0  1185085.0  1350721.0   \n","1607    60924.0   103690.0    60694.0   130580.0    70926.0    65324.0   \n","1610  1746974.0  1703422.0  1793425.0  1809529.0  1666625.0  1814715.0   \n","\n","          Y2013      Y2014      Y2015      Y2016  \n","1319    56779.0    55978.0    63331.0    72902.0  \n","1322     1201.0     1157.0     1173.0     1293.0  \n","1325   288878.0   266771.0   295196.0   308298.0  \n","1328    11551.0     9238.0     9674.0     8700.0  \n","1331     8396.0     8375.0     8288.0     7737.0  \n","...         ...        ...        ...        ...  \n","1598  1869619.0  1709984.0  1503124.0  1239283.0  \n","1601  2224400.0  2247300.0  1989200.0  2421811.0  \n","1604  1341112.0  1246546.0  1215195.0  1200748.0  \n","1607    92450.0    94930.0   106374.0   118977.0  \n","1610  1858687.0  1652335.0  1767902.0  1755227.0  \n","\n","[101 rows x 10 columns]\n","Shape:  torch.Size([101, 10])\n","\t Yield tensor tensor([[1.8024e+04, 6.5000e+04, 1.8957e+04,  ..., 5.5978e+04, 6.3331e+04,\n","         7.2902e+04],\n","        [5.1000e+01, 2.0910e+03, 9.8700e+02,  ..., 1.1570e+03, 1.1730e+03,\n","         1.2930e+03],\n","        [3.7798e+05, 2.6548e+05, 2.9513e+05,  ..., 2.6677e+05, 2.9520e+05,\n","         3.0830e+05],\n","        ...,\n","        [1.2611e+06, 1.4476e+06, 1.2210e+06,  ..., 1.2465e+06, 1.2152e+06,\n","         1.2007e+06],\n","        [6.0924e+04, 1.0369e+05, 6.0694e+04,  ..., 9.4930e+04, 1.0637e+05,\n","         1.1898e+05],\n","        [1.7470e+06, 1.7034e+06, 1.7934e+06,  ..., 1.6523e+06, 1.7679e+06,\n","         1.7552e+06]], dtype=torch.float64)\n","          Y2017\n","1319    75373.0\n","1322     1324.0\n","1325   313730.0\n","1328     5351.0\n","1331     7472.0\n","...         ...\n","1598  1888936.0\n","1601  4129481.0\n","1604  1176669.0\n","1607   125683.0\n","1610  1768915.0\n","\n","[101 rows x 1 columns]\n","Shape:  torch.Size([101, 1])\n","\t Yield tensor tensor([[7.5373e+04],\n","        [1.3240e+03],\n","        [3.1373e+05],\n","        [5.3510e+03],\n","        [7.4720e+03],\n","        [5.6501e+04],\n","        [4.1297e+05],\n","        [1.3506e+07],\n","        [3.5396e+04],\n","        [4.1373e+04],\n","        [4.4250e+03],\n","        [4.7570e+03],\n","        [3.7604e+05],\n","        [9.9102e+04],\n","        [4.8680e+03],\n","        [2.8382e+05],\n","        [1.4066e+05],\n","        [1.1532e+04],\n","        [2.0040e+06],\n","        [3.8579e+04],\n","        [7.7449e+05],\n","        [1.2600e+06],\n","        [       nan],\n","        [1.7609e+04],\n","        [5.7900e+02],\n","        [       nan],\n","        [7.8000e+01],\n","        [       nan],\n","        [1.5270e+03],\n","        [       nan],\n","        [7.5400e+03],\n","        [6.9880e+03],\n","        [1.8244e+06],\n","        [1.6529e+04],\n","        [7.4900e+02],\n","        [2.3740e+03],\n","        [3.7227e+04],\n","        [2.1860e+05],\n","        [1.4626e+05],\n","        [6.0000e+03],\n","        [1.0314e+06],\n","        [4.3619e+05],\n","        [1.0266e+05],\n","        [4.3748e+04],\n","        [2.6353e+05],\n","        [3.5855e+04],\n","        [4.6326e+04],\n","        [       nan],\n","        [4.6165e+04],\n","        [2.2655e+06],\n","        [1.2257e+05],\n","        [2.6324e+05],\n","        [3.3232e+05],\n","        [6.3850e+03],\n","        [8.2659e+04],\n","        [9.6741e+04],\n","        [4.1519e+05],\n","        [2.0574e+04],\n","        [7.1500e+02],\n","        [8.5922e+04],\n","        [1.4950e+03],\n","        [1.7561e+04],\n","        [1.1052e+06],\n","        [3.6880e+04],\n","        [1.0150e+05],\n","        [       nan],\n","        [4.3132e+06],\n","        [6.9700e+02],\n","        [8.0730e+05],\n","        [3.0000e+04],\n","        [4.9570e+03],\n","        [2.1510e+06],\n","        [9.9400e+05],\n","        [3.1000e+04],\n","        [1.0719e+04],\n","        [4.5251e+04],\n","        [3.6561e+07],\n","        [1.7000e+04],\n","        [7.1475e+04],\n","        [1.2727e+05],\n","        [2.3540e+03],\n","        [3.7158e+05],\n","        [1.4995e+05],\n","        [7.7444e+04],\n","        [1.1955e+04],\n","        [2.6500e+03],\n","        [1.8088e+05],\n","        [3.1819e+07],\n","        [4.9780e+07],\n","        [5.0048e+07],\n","        [5.0533e+05],\n","        [1.7422e+07],\n","        [7.7449e+05],\n","        [3.9737e+06],\n","        [5.7713e+06],\n","        [3.2762e+06],\n","        [1.8889e+06],\n","        [4.1295e+06],\n","        [1.1767e+06],\n","        [1.2568e+05],\n","        [1.7689e+06]], dtype=torch.float64)\n","Train X: \n"," torch.Size([22, 10])\n","Train Y: \n"," torch.Size([101, 10])\n","Test X: \n"," torch.Size([22])\n","Test Y: \n"," torch.Size([101, 1])\n","/Users/appleapple/Masters Research Project/Code/crop-yield/src/main.py:203: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  pesticides_and_fertilizers = torch.cat((pesticides_tensor, fertilizers_tensor), dim=0)\n"]}],"source":["\n","training_input = load_data()[0]\n","training_ouput = load_data()[1]\n","test_input = load_data()[2]\n","test_output = load_data()[3]\n","\n","x = training_input\n","y = training_ouput"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["\n","# Transforms\n","\n","# Training data\n","x_tensor = torch.Tensor(x.float())\n","x_data = (x - x.mean())/(x.max() - x.min())\n","test = torch.Tensor([[0,4,5]])\n","x_tensor_normalized = x.normal_()\n","y_data = (y.normal_())\n","\n","transformed_x = torch.reshape(x_tensor_normalized,(1,220))\n","transformed_y = y_data\n","y = torch.reshape(y, (1,1010))\n",""]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["# Test data\n","transformed_test_x = torch.Tensor(test_input.float())\n","transformed_test_x = transformed_test_x.normal_()\n","transformed_test_y = torch.Tensor(test_output.float())\n","transformed_test_y = transformed_test_y.normal_()"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["# Define model\n","class NeuralNet(nn.Module):\n","       def __init__(self, D_in, H1, H2, H3, H4, H5, H6, D_out):\n","              super(NeuralNet, self).__init__()\n","              self.linear1 = nn.Linear(D_in, H1)\n","              self.linear2 = nn.Linear(H1, H2)\n","              self.linear3 = nn.Linear(H2, H3)\n","              self.linear4 = nn.Linear(H3, H4)\n","              self.linear5 = nn.Linear(H4, H5)\n","              self.linear6 = nn.Linear(H5, H6)\n","              self.linear7 = nn.Linear(H6, D_out)\n","              self.relu = nn.ReLU()\n","       def forward(self, x):\n","              y_pred = torch.tanh(self.linear1(x))\n","              y_pred = self.linear2(y_pred)\n","              y_pred = torch.tanh(self.linear3(y_pred))\n","              y_pred = self.linear4(y_pred)\n","              y_pred = torch.tanh(self.linear5(y_pred))\n","              y_pred = self.linear6(y_pred)\n","              y_pred = torch.sigmoid(self.linear7(y_pred))\n","              return y_pred\n",""]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["\n","\n","from functools import partial\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","from random import randint\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n",""]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["\n","\n","def get_accuracy(model, predicted, actual, threshold_percentage):\n","    num_items = len(actual)\n","    X = predicted.view(num_items)\n","    Y = actual.view(num_items)\n","    num_correct = torch.sum(torch.abs(X-Y)<torch.abs(threshold_percentage*Y))\n","    accuracy = (num_correct.item()*100.0/num_items)\n","    return (num_correct, accuracy)\n","    \n","\n","def random_split_training(trainset_x, trainset_y):\n","       # TODO:\n","       # Select 2 random numbers between 1 and 10\n","       # Use them to index into x and y.\n","       # That will give us the training and validation set\n","       # Rest of the indices are thus our training set\n","       index1 = randint(0,9)\n","       index2 = randint(0,9)\n","       indices=[]\n","       i=0\n","       for i in range(0,10):\n","              if(i not in [index1,index2]):\n","                 indices.append(i)    \n","       \n","       indices = torch.tensor(indices)\n","       val_indices = torch.tensor([index1, index2])\n","       t_subset_x = torch.index_select(trainset_x, 1, indices)\n","       t_subset_y = torch.index_select(trainset_y, 1, indices)\n","       v_subset_x = torch.index_select(trainset_x, 1,val_indices)\n","       v_subset_y = torch.index_select(trainset_y, 1, val_indices)\n","       return (t_subset_x, t_subset_y, v_subset_x, v_subset_y)\n","# \n",""]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["# Function to train and validate data. Save the best model based on validation loss\n","# \n","# params:\n","# config: hyperparameter search space\n","# \n","def train_crop_yield(config):\n","       net = NeuralNet(176, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],8)\n","       criterion = torch.nn.MSELoss()\n","       optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","       # train_set = torch.cat((transformed_x, transformed_y), dim=0)\n","       # test_set = torch.cat((test_input, test_output), dim=0)\n","       \n","       x_by_years = transformed_x.reshape(22,10)\n","       y_by_years = transformed_y.reshape(101,10)\n","       \n","       train_subset_x, train_subset_y, val_subset_x, val_subset_y = random_split_training(x_by_years, y_by_years)\n","       \n","       for epoch in range(50000):\n","              running_loss = 0.0\n","              epoch_steps = 0\n","              # Zero the accumulated gradients\n","              optimizer.zero_grad()\n","              # forward + backward + optimize\n","              output = net(train_subset_x.float().reshape(1,176))\n","              loss = criterion(output, train_subset_y.float())\n","              loss.backward()\n","              optimizer.step()\n","              # print statistics\n","              running_loss += loss.item()\n","              epoch_steps += 1\n","              if epoch%5==4:\n","                     print(\"[%d] loss: %.3f\"%(epoch+1, running_loss/epoch_steps))\n","                     running_loss=0.0\n","       \n","       # Validation loss\n","       net = NeuralNet(44, config['H1'], config['H2'], config['H3'], config[\"H4\"], config[\"H5\"], config[\"H6\"],2)\n","       val_loss = 0.0\n","       val_steps = 0\n","       total = 0\n","       correct = 0\n","       with torch.no_grad():\n","              val_output = net(val_subset_x.float().reshape(1,44))\n","              total = val_subset_y.size(0)\n","              correct = torch.sum(torch.abs(val_output-val_subset_y)<torch.abs(0.10*val_subset_y))\n","              loss = criterion(val_output, val_subset_y.float())\n","              val_loss += loss.cpu().numpy()\n","              torch.save((net.state_dict(), optimizer.state_dict()), './trained_net.dat')\n","              tune.report(loss=(val_loss), accuracy=(correct/total))\n","       print(\"\\n Finished Training \\n\")\n","              "]},{"cell_type":"code","execution_count":73,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stderr","text":["2021-05-16 17:56:41,589\tWARNING experiment.py:294 -- No name detected on trainable. Using DEFAULT.\n","2021-05-16 17:56:41,595\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.8/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (10 PENDING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td></tr>\n<tr><td>DEFAULT_7104f_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.9/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td></tr>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":[" 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [12975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [12980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [12985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [12990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [12995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [46995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [47995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m [50000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [48060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47530] loss: 0.994\n","Result for DEFAULT_7104f_00002:\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [47535] loss: 0.994\n","  accuracy: tensor(0.0297)\n","  date: 2021-05-16_18-08-37\n","  done: false\n","  experiment_id: c608fdb1d05c41d39ebfbc99aee5cdcd\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3621916770935059\n","  node_ip: 10.201.135.226\n","  pid: 80250\n","  time_since_restore: 705.856899023056\n","  time_this_iter_s: 705.856899023056\n","  time_total_s: 705.856899023056\n","  timestamp: 1621154317\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00002\n","  \n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80250)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.8/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3621916770935059<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (6 PENDING, 4 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00000</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>RUNNING </td><td>10.201.135.226:80250</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["9350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [48995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m [50000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49450] loss: 0.994\n","Result for DEFAULT_7104f_00001:\n","  accuracy: tensor(0.0693)\n","  date: 2021-05-16_18-09-05\n","  done: false\n","  experiment_id: f938d1cf13ea4df2971d857889fb2209\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3461556434631348\n","  node_ip: 10.201.135.226\n","  pid: 80249\n","  time_since_restore: 733.9077529907227\n","  time_this_iter_s: 733.9077529907227\n","  time_total_s: 733.9077529907227\n","  timestamp: 1621154345\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00001\n","  \n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80249)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.8/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3541736602783203<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (5 PENDING, 4 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00000</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>RUNNING   </td><td>10.201.135.226:80249</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_7104f_00001:\n","  accuracy: tensor(0.0693)\n","  date: 2021-05-16_18-09-05\n","  done: true\n","  experiment_id: f938d1cf13ea4df2971d857889fb2209\n","  experiment_tag: 1_H1=256,H2=64,H3=1024,H4=256,H5=64,H6=256,lr=0.052017\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3461556434631348\n","  node_ip: 10.201.135.226\n","  pid: 80249\n","  time_since_restore: 733.9077529907227\n","  time_this_iter_s: 733.9077529907227\n","  time_total_s: 733.9077529907227\n","  timestamp: 1621154345\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00001\n","  \n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m [50000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [13830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1685] loss: 0.994\n","Result for DEFAULT_7104f_00000:\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_18-09-13\n","  done: true\n","  experiment_id: 3b00dfbd373f4f3c8b5c1ba07ae476f9\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3607617616653442\n","  node_ip: 10.201.135.226\n","  pid: 80251\n","  time_since_restore: 742.2547867298126\n","  time_this_iter_s: 742.2547867298126\n","  time_total_s: 742.2547867298126\n","  timestamp: 1621154353\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00000\n","  \n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [1695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80251)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.7/8.0 GiB<br>Using AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3607617616653442<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (4 PENDING, 3 RUNNING, 3 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         742.255</td><td style=\"text-align: right;\">1.36076</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [30995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [21960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m [50000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35865] loss: 0.994\n","Result for DEFAULT_7104f_00005:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_18-16-19\n","  done: false\n","  experiment_id: 1ea938b8dbed4545a0878f6e6461b17d\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3334254026412964\n","  node_ip: 10.201.135.226\n","  pid: 80308\n","  time_since_restore: 423.2846329212189\n","  time_this_iter_s: 423.2846329212189\n","  time_total_s: 423.2846329212189\n","  timestamp: 1621154779\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00005\n","  \n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [35875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [31125] loss: 0.994\u001b[2m\u001b[36m(pid=80308)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80308)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.9/8.0 GiB<br>Using AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3534587025642395<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (3 PENDING, 4 RUNNING, 3 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>RUNNING   </td><td>10.201.135.226:80308</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         423.285</td><td style=\"text-align: right;\">1.33343</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         742.255</td><td style=\"text-align: right;\">1.36076</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[0m [49385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [41955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [41960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [24955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [41965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [41970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [41975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [24960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [41980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [41985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [41990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [24965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [41995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [24970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [24975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [24980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [24985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [24990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [24995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m [50000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16925] loss: 0.994\n","Result for DEFAULT_7104f_00006:\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80311)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n","  accuracy: tensor(0.0495)\n","  date: 2021-05-16_18-19-10\n","  done: true\n","  experiment_id: 3c9dbb17c9ae4b5b8a06370ddea9a5a5\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3604161739349365\n","  node_ip: 10.201.135.226\n","  pid: 80311\n","  time_since_restore: 586.0569043159485\n","  time_this_iter_s: 586.0569043159485\n","  time_total_s: 586.0569043159485\n","  timestamp: 1621154950\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00006\n","  \n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [25090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [42465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [16945] loss: 0.994\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3604161739349365<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (2 PENDING, 3 RUNNING, 5 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         742.255</td><td style=\"text-align: right;\">1.36076</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         423.285</td><td style=\"text-align: right;\">1.33343</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         586.057</td><td style=\"text-align: right;\">1.36042</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":[".994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m [50000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [27160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11900] loss: 0.994\n","Result for DEFAULT_7104f_00004:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_18-20-51\n","  done: true\n","  experiment_id: ee68573778044097a239d5325b571445\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3714414834976196\n","  node_ip: 10.201.135.226\n","  pid: 80295\n","  time_since_restore: 722.9105541706085\n","  time_this_iter_s: 722.9105541706085\n","  time_total_s: 722.9105541706085\n","  timestamp: 1621155051\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00004\n","  \n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [28995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [29000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [11910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80295)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.9/8.0 GiB<br>Using AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3605889678001404<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (1 PENDING, 3 RUNNING, 6 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         742.255</td><td style=\"text-align: right;\">1.36076</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         722.911</td><td style=\"text-align: right;\">1.37144</td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         423.285</td><td style=\"text-align: right;\">1.33343</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         586.057</td><td style=\"text-align: right;\">1.36042</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[0m [20080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [33995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [30775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m [50000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34405] loss: 0.994\n","Result for DEFAULT_7104f_00007:\n","  accuracy: tensor(0.0594)\n","  date: 2021-05-16_18-23-48\n","  done: true\n","  experiment_id: d02abff01c8a41c09d170c722b37344f\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3837828636169434\n","  node_ip: 10.201.135.226\n","  pid: 80327\n","  time_since_restore: 438.1532201766968\n","  time_this_iter_s: 438.1532201766968\n","  time_total_s: 438.1532201766968\n","  timestamp: 1621155228\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00007\n","  \n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [20745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [34415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80327)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.0/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3607617616653442<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         742.255</td><td style=\"text-align: right;\">1.36076</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         722.911</td><td style=\"text-align: right;\">1.37144</td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         423.285</td><td style=\"text-align: right;\">1.33343</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         586.057</td><td style=\"text-align: right;\">1.36042</td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         438.153</td><td style=\"text-align: right;\">1.38378</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [33350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m [50000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [35990] loss: 0.994\n","Result for DEFAULT_7104f_00008:\n","  accuracy: tensor(0.0693)\n","  date: 2021-05-16_18-25-28\n","  done: false\n","  experiment_id: 65dcb09b6de94a87872114f8112757af\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3456708192825317\n","  node_ip: 10.201.135.226\n","  pid: 80343\n","  time_since_restore: 367.9516291618347\n","  time_this_iter_s: 367.9516291618347\n","  time_total_s: 367.9516291618347\n","  timestamp: 1621155328\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00008\n","  \n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80343)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3605889678001404<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>RUNNING   </td><td>10.201.135.226:80343</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         367.952</td><td style=\"text-align: right;\">1.34567</td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>RUNNING   </td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         742.255</td><td style=\"text-align: right;\">1.36076</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         722.911</td><td style=\"text-align: right;\">1.37144</td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         423.285</td><td style=\"text-align: right;\">1.33343</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         586.057</td><td style=\"text-align: right;\">1.36042</td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         438.153</td><td style=\"text-align: right;\">1.38378</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["80362)\u001b[0m [48205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [48995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m [50000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [35875] loss: 0.994\n","Result for DEFAULT_7104f_00009:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_18-26-46\n","  done: true\n","  experiment_id: 5aa2e3521ce54433b891564773b81e0a\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3689370155334473\n","  node_ip: 10.201.135.226\n","  pid: 80362\n","  time_since_restore: 344.5250618457794\n","  time_this_iter_s: 344.5250618457794\n","  time_total_s: 344.5250618457794\n","  timestamp: 1621155406\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00009\n","  \n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80362)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using AsyncHyperBand: num_stopped=5\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3607617616653442<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n<tr><td>DEFAULT_7104f_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         742.255</td><td style=\"text-align: right;\">1.36076</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         722.911</td><td style=\"text-align: right;\">1.37144</td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         423.285</td><td style=\"text-align: right;\">1.33343</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         586.057</td><td style=\"text-align: right;\">1.36042</td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         438.153</td><td style=\"text-align: right;\">1.38378</td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         367.952</td><td style=\"text-align: right;\">1.34567</td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         344.525</td><td style=\"text-align: right;\">1.36894</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[47890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [47995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [48995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49000] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49005] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49010] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49015] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49020] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49025] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49030] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49035] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49040] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49045] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49050] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49055] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49060] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49065] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49070] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49075] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49080] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49085] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49090] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49095] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49100] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49105] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49110] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49115] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49120] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49125] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49130] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49135] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49140] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49145] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49150] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49155] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49160] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49165] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49170] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49175] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49180] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49185] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49190] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49195] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49200] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49205] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49210] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49215] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49220] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49225] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49230] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49235] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49240] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49245] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49250] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49255] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49260] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49265] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49270] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49275] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49280] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49285] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49290] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49295] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49300] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49305] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49310] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49315] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49320] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49325] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49330] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49335] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49340] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49345] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49350] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49355] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49360] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49365] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49370] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49375] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49380] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49385] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49390] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49395] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49400] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49405] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49410] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49415] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49420] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49425] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49430] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49435] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49440] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49445] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49450] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49455] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49460] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49465] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49470] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49475] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49480] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49485] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49490] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49495] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49500] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49505] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49510] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49515] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49520] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49525] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49530] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49535] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49540] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49545] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49550] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49555] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49560] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49565] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49570] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49575] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49580] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49585] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49590] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49595] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49600] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49605] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49610] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49615] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49620] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49625] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49630] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49635] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49640] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49645] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49650] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49655] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49660] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49665] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49670] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49675] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49680] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49685] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49690] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49695] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49700] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49705] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49710] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49715] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49720] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49725] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49730] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49735] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49740] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49745] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49750] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49755] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49760] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49765] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49770] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49775] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49780] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49785] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49790] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49795] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49800] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49805] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49810] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49815] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49820] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49825] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49830] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49835] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49840] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49845] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49850] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49855] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49860] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49865] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49870] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49875] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49880] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49885] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49890] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49895] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49900] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49905] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49910] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49915] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49920] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49925] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49930] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49935] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49940] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49945] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49950] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49955] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49960] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49965] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49970] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49975] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49980] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49985] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49990] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [49995] loss: 0.994\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m [50000] loss: 0.994\n","Result for DEFAULT_7104f_00003:\n","  accuracy: tensor(0.0396)\n","  date: 2021-05-16_18-32-14\n","  done: false\n","  experiment_id: 42821d24b84a47c49b21324748b41f56\n","  hostname: tprovpk.local\n","  iterations_since_restore: 1\n","  loss: 1.3499078750610352\n","  node_ip: 10.201.135.226\n","  pid: 80252\n","  time_since_restore: 2122.6106979846954\n","  time_this_iter_s: 2122.6106979846954\n","  time_total_s: 2122.6106979846954\n","  timestamp: 1621155734\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 7104f_00003\n","  \n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m  Finished Training \n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m \n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m /opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","\u001b[2m\u001b[36m(pid=80252)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.9/8.0 GiB<br>Using AsyncHyperBand: num_stopped=5\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3605889678001404<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00003</td><td>RUNNING   </td><td>10.201.135.226:80252</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2122.61 </td><td style=\"text-align: right;\">1.34991</td></tr>\n<tr><td>DEFAULT_7104f_00000</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         742.255</td><td style=\"text-align: right;\">1.36076</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         722.911</td><td style=\"text-align: right;\">1.37144</td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         423.285</td><td style=\"text-align: right;\">1.33343</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         586.057</td><td style=\"text-align: right;\">1.36042</td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         438.153</td><td style=\"text-align: right;\">1.38378</td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         367.952</td><td style=\"text-align: right;\">1.34567</td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>TERMINATED</td><td>                    </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         344.525</td><td style=\"text-align: right;\">1.36894</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_7104f_00003:\n  accuracy: tensor(0.0396)\n  date: 2021-05-16_18-32-14\n  done: true\n  experiment_id: 42821d24b84a47c49b21324748b41f56\n  experiment_tag: 3_H1=128,H2=64,H3=1024,H4=2048,H5=32,H6=256,lr=0.020266\n  hostname: tprovpk.local\n  iterations_since_restore: 1\n  loss: 1.3499078750610352\n  node_ip: 10.201.135.226\n  pid: 80252\n  time_since_restore: 2122.6106979846954\n  time_this_iter_s: 2122.6106979846954\n  time_total_s: 2122.6106979846954\n  timestamp: 1621155734\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: 7104f_00003\n  \n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"== Status ==<br>Memory usage on this node: 5.9/8.0 GiB<br>Using AsyncHyperBand: num_stopped=5\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.3605889678001404<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.61 GiB heap, 0.0/1.31 GiB objects<br>Result logdir: /Users/appleapple/ray_results/DEFAULT_2021-05-16_17-56-41<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  H1</th><th style=\"text-align: right;\">  H2</th><th style=\"text-align: right;\">  H3</th><th style=\"text-align: right;\">  H4</th><th style=\"text-align: right;\">  H5</th><th style=\"text-align: right;\">  H6</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n</thead>\n<tbody>\n<tr><td>DEFAULT_7104f_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0750056</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         742.255</td><td style=\"text-align: right;\">1.36076</td></tr>\n<tr><td>DEFAULT_7104f_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0520169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         733.908</td><td style=\"text-align: right;\">1.34616</td></tr>\n<tr><td>DEFAULT_7104f_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0166839</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         705.857</td><td style=\"text-align: right;\">1.36219</td></tr>\n<tr><td>DEFAULT_7104f_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0202657</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2122.61 </td><td style=\"text-align: right;\">1.34991</td></tr>\n<tr><td>DEFAULT_7104f_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.011306 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         722.911</td><td style=\"text-align: right;\">1.37144</td></tr>\n<tr><td>DEFAULT_7104f_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0936394</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         423.285</td><td style=\"text-align: right;\">1.33343</td></tr>\n<tr><td>DEFAULT_7104f_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0147075</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         586.057</td><td style=\"text-align: right;\">1.36042</td></tr>\n<tr><td>DEFAULT_7104f_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0403951</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         438.153</td><td style=\"text-align: right;\">1.38378</td></tr>\n<tr><td>DEFAULT_7104f_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0643413</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         367.952</td><td style=\"text-align: right;\">1.34567</td></tr>\n<tr><td>DEFAULT_7104f_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0780191</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         344.525</td><td style=\"text-align: right;\">1.36894</td></tr>\n</tbody>\n</table><br><br>"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2021-05-16 18:32:14,361\tINFO tune.py:549 -- Total run time: 2132.77 seconds (2132.54 seconds for the tuning loop).\n","Best trial config: {'H1': 256, 'H2': 64, 'H3': 128, 'H4': 256, 'H5': 64, 'H6': 128, 'lr': 0.09363940153806419}\n","Best trial final validation loss: 1.3334254026412964\n","Best trial fnal validation accuracy: 0.039603959769010544\n","Best trial test set accuracy: 24.752475247524753\n"]}],"source":["# Evaluation:\n","def test_accuracy(net, device='cpu'):\n","       net.eval()\n","       with torch.no_grad():\n","              predicted = net(transformed_test_x.float().reshape(1,22))\n","              return get_accuracy(net, predicted.reshape(101,1), transformed_test_y.reshape(101,1), 0.50)\n","       \n","# Define hyperparameters\n","config = {\n","       \"H1\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"H2\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H3\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H4\":tune.sample_from(lambda _: 2**np.random.randint(7,12)),\n","       \"H5\":tune.sample_from(lambda _: 2**np.random.randint(5,7)),\n","       \"H6\":tune.sample_from(lambda _: 2**np.random.randint(7,9)),\n","       \"lr\":tune.loguniform(1e-2, 1e-1)\n","}\n","\n","# Scheduler. Randomly try out combination of hyperparameters\n","scheduler = ASHAScheduler(\n","       metric=\"loss\",\n","       mode=\"min\",\n","       max_t=10,\n","       grace_period=1,\n","       reduction_factor=2\n",")\n","\n","reporter=CLIReporter(metric_columns=[\"loss\",\"accuracy\", \"training_iteration\"])\n","\n","result = tune.run(\n","       partial(train_crop_yield),\n","       config=config,\n","       num_samples=10,\n","       scheduler=scheduler\n",")\n","\n","best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","\n","print(\"Best trial config: {}\".format(best_trial.config))\n","print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n","print(\"Best trial fnal validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n","\n","best_trained_model = NeuralNet(22, best_trial.config[\"H1\"], best_trial.config[\"H2\"], best_trial.config[\"H3\"],best_trial.config[\"H4\"],best_trial.config[\"H5\"],best_trial.config[\"H6\"],101)\n","\n","device=\"cpu\"\n","best_trained_model.to(device)\n","\n","# best_checkpoint_dir = best_trial.checkpoint.value\n","# print(\" \\n\",type(best_trial.evaluated_params))\n","# model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n","# best_trained_model.load_state_dict(model_state)\n","\n","num_correct, test_acc = test_accuracy(best_trained_model, device)\n","print(\"Best trial test set accuracy: {}\".format(test_acc))\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}